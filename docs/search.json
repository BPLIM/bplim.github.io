[
  {
    "objectID": "manuals.html",
    "href": "manuals.html",
    "title": "BPLIM",
    "section": "",
    "text": "Data Manuals\n\nBBS\nCB\nCBHP\nCRC\nGE\nHCRC\nIBACH\nIREE\nSI\nSLB"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Banco de Portugal Microdata Research Laboratory",
    "section": "",
    "text": "Welcome to BPLIM"
  },
  {
    "objectID": "guides.html",
    "href": "guides.html",
    "title": "BPLIM",
    "section": "",
    "text": "Guides\n\nGuide for Researchers\nDatasets Guide\nExternal Server\nHow to use Git\nHow to use Containers\nOutput Control\nWorking with Pseudo-Data\nContainers\nReplication App\n\n\n\nWebsite\n\n\n\n\n\n\n\nInstructional Videos\n\n\n\n\n\n\n\nGuides\n\n\n\n\n\n\n\nData Manuals\n\n\n\n\n\n\n\nTools\n\n\n\n\n\n\n\nWorkshops"
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/readme.html",
    "href": "Guides/01_Guide_for_Researchers/readme.html",
    "title": "BPLIM",
    "section": "",
    "text": "Guide for Researchers\nThis folder contains the Guide for Researchers, which provides comprehensive information on all procedures related to data access at BPLIM. The guide includes detailed instructions on how to request data, outlines the approval process and access conditions, and explains the proper handling and use of datasets."
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#what-are-the-characteristics-of-bplims-datasets",
    "href": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#what-are-the-characteristics-of-bplims-datasets",
    "title": "Guide for Researchers",
    "section": "2.1 What are the characteristics of BPLIM’s datasets?",
    "text": "2.1 What are the characteristics of BPLIM’s datasets?\nAll BPLIM datasets created for research purposes are stripped of elements that allow for direct identification of companies, banks, or individuals. Whenever possible, the datasets contain unique unit identifiers common across datasets: examples of these are tina – the tax identification number anonymized for companies – and bina – the bank identification number anonymized. \nBy default BPLIM datasets are made available in Stata format. Larger datasets may be made available in parquet format. Data is stored in an efficient way that minimizes file size and follows BPLIM’s naming convention. Labels are applied to all variables and value labels to all categorical variables. Whenever possible, labels can be displayed in Portuguese and English.\nAll datasets are accompanied by a Manual that contains all relevant information regarding the data. The data manuals, the metafiles1 and citation information for the different data extractions are available on GitHub. A metafile that contains additional descriptive statistics for each dataset can be obtained once researchers are given access to the server. Please refer to the External Server Guide on how to access this information. \nBPLIM datasets may also have companion script files that calculate additional variables or harmonized variables to guarantee comparability over time and across datasets. Datasets are updated regularly based on a data extraction (“data freeze”) at a specific point in time, and a versioning system is applied to reflect any changes to the data set. Most datasets have an associated Digital Object Identifier (DOI)."
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#what-data-are-available",
    "href": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#what-data-are-available",
    "title": "Guide for Researchers",
    "section": "2.2 What data are available?",
    "text": "2.2 What data are available?\nThe complete list of datasets, including a short description and the access conditions, is available in the BPLIM Datasets Guide. On BPLIM´s website you will find a list of the latest version of the datasets available for external researchers, along with a link to the respective documentation."
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#who-can-gain-access-to-the-data",
    "href": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#who-can-gain-access-to-the-data",
    "title": "Guide for Researchers",
    "section": "3.1 Who can gain access to the data?",
    "text": "3.1 Who can gain access to the data?\nAccess is restricted to BPLIM accredited researchers who intend to utilize the data for scientific purposes. Individuals affiliated with BdP are classified as Internal Researchers and have unrestricted access to all datasets maintained by BPLIM. Those not affiliated with BdP are considered External Researchers, and their access is subject to several restrictions. BPLIM Datasets Guide summarizes the access restrictions for each dataset."
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#how-can-researchers-request-access-to-the-data",
    "href": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#how-can-researchers-request-access-to-the-data",
    "title": "Guide for Researchers",
    "section": "3.2 How can researchers request access to the data?",
    "text": "3.2 How can researchers request access to the data?\nInternal researchers are provided access to a data repository maintained by BPLIM. The repository makes available data, which can be used unreservedly for research and policy activities without any formality. However, data made available strictly for policy should not be used for research because as it may have lower quality, are not documented, and may not be reproducible. If Internal Researchers are working with external co-authors or if they need to use or link other micro datasets not available in the data repository they will need to submit a project to BPLIM.\nExternal Researchers must always submit a project. The project proposal must: (1) contain a short description of the research project; (2) identify all participants involved in the project along with their affiliations, and include a curriculum vitae (CV) for each; and (3) specify the datasets, timeframe, and variables required. All external researchers with access to the data are required to sign a confidentiality agreement. If the project consists of a master or doctoral dissertation then the supervisor(s) has to be identified and must also sign the confidentiality agreement. BPLIM staff can collaborate with the researcher(s) to identify the required datasets and, if necessary, construct a customized dataset. A copy of the required documents can be found in BPLIM’s website."
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#approval-process",
    "href": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#approval-process",
    "title": "Guide for Researchers",
    "section": "3.3 Approval process",
    "text": "3.3 Approval process\nUpon submission of all required documentation and verification that it conforms to BPLIM rules, the project will be evaluated to ensure that it addresses a legitimate research question. Compliance with the guidelines is crucial for recurring external researchers (those who have already participated in BPLIM projects). Once the project is approved, the researcher will receive an email notification with the user credentials and instructions for accessing the data. Summary information about the project and researchers will be posted on BPLIM’s website."
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#how-can-the-data-be-accessed",
    "href": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#how-can-the-data-be-accessed",
    "title": "Guide for Researchers",
    "section": "3.4 How can the data be accessed?",
    "text": "3.4 How can the data be accessed?\nWhen applying for a project, researchers must specify if they plan to access an internal account, in Pitagoras, or an external account in the External Server.\nAccounts open at Pitagoras can only be accessed at the installations of BdP (“on-site access”) either at Lisbon or Porto. Internal researchers can log into Pitagoras from their terminal using their network login credentials. External researchers will be provided with a login and password for Pitagoras and granted access to a terminal where it is technically restricted to transfer, download, copy, paste, or print any data. BPLIM projects at Pitagoras are placed in a specific folder containing all projects, with users having access only to their designated project folder.\nFor more details on accessing BPLIM projects in Pitagoras, please refer to the BPLIM Pitagoras Manual. Due to a limited number of terminals available, external researchers must book their visits well in advance.\nIf the account is on the External Server, then the data must be accessed remotely (“remote access”) using a secure connection. BPLIM uses the NoMachine software for this purpose. With this connection, it is not possible to exchange files between the external server and the local computer. For more details on using the External Server, please refer to the External Server Guide.\nIn special circumstances explained below, the external researcher may be granted indirect access to the data (“surrogate access”, also known as, “remote execution”). With surrogate access, there is no need for the external researcher to have an account, as BPLIM staff (or an internal researcher) will act as a proxy for data access. This means that BPLIM staff (or an internal researcher) will execute the scripts written by the external researcher and share the outputs after disclosure control."
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#what-type-of-anonymization-is-applied-to-bplims-datasets",
    "href": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#what-type-of-anonymization-is-applied-to-bplims-datasets",
    "title": "Guide for Researchers",
    "section": "3.5 What type of anonymization is applied to BPLIM’s datasets?",
    "text": "3.5 What type of anonymization is applied to BPLIM’s datasets?\nWhen BPLIM makes its datasets accessible to researchers, it uses several different strategies to anonymize the data. The type of anonymization depends on the specific data and the user. BPLIM uses four levels of anonymization:\n\nLevel 1 - All information that could lead to the direct identification of statistical units (firms/banks/individuals) is omitted, and unique identifiers (e.g., NIF, bank ID) undergo a 1-to-1 transformation to new identifiers that are specific to the project. Level 1 datasets will contain “_A_” in the name.\nLevel 2 - in addition to Level 1, the values of variables containing sensitive information will be replace by modified values, which are random values that exhibit some correlation with the original values. The file name of a Level 2 dataset will contain “_P_”, and the labels of the modified variables will reflect this information.\nLevel 3 - in addition to Level 2, variables may be sorted randomly and independently to break the link between the observations. Level 3 datasets will be identified with “_R_”.\nLevel 4 - a subset of the data is generated randomly (pseudo-data), respecting only the metadata and the time structure of the original data. Level 4 datasets will be identified with “_D_” in their name.\n\nDatasets of Level 2, 3, or 4 are generically designated as modified datasets.\nLevel 4 datasets are the only ones that researchers are allowed to use outside of the bank computing environment, because the values generated for this level are fictitious."
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#what-determines-the-type-of-anonymization-applied-to-the-data-set",
    "href": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#what-determines-the-type-of-anonymization-applied-to-the-data-set",
    "title": "Guide for Researchers",
    "section": "3.6 What determines the type of anonymization applied to the data set?",
    "text": "3.6 What determines the type of anonymization applied to the data set?\nBPLIM data meant to be used by Internal researchers are always anonymized at Level 1. The exception is if the Internal Researcher is accessing the data through the External Server. In that case, Internals Researchers have the same access conditions as External Researchers.\nDatasets made available to External Researchers are subject to a confidentiality classification as follows: low, medium, or high. If the data are classified as low, then the data is anonymized at Level 1. Datasets classified as medium may be anonymized at Level 2 or 3, depending on the risk of identification. For datasets classified with a high level of confidentiality, External Researchers may only have access to Level 4 data."
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#how-can-researchers-work-with-modified-data",
    "href": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#how-can-researchers-work-with-modified-data",
    "title": "Guide for Researchers",
    "section": "3.7 How can researchers work with modified data?",
    "text": "3.7 How can researchers work with modified data?\nModified datasets serve only the purpose of facilitating the creation of scripts that manipulate/analyze the data. Results of analysis performed on “modified” datasets are not valid for research purposes. However, external researchers can always request to have their scripts run on the original datasets. This rule applies whether the access is “on-site” or “remote”. Researchers working with modified data should use BPLIM’s Replication App. For instructions on how to use the Replication App, please refer to the Replication App User Guide. While not strictly enforced, use of the BPLIM’s Replication App, will ensure that the replication on the original data is implemented correctly and in a much more timely manner. We strongly encourage use of the BPLIM’s Replication App.\nResearchers working with Level 4 data (pseudo-data) in their personal computers, will receive a package along with instructions to create the pseudo-data. For instructions on how to work with pseudo-data, please refer to the BPLIM Guide to Working with Pseudo-Data."
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#how-are-projects-involving-co-authorship-between-internal-researchers-and-external-researchers-handled",
    "href": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#how-are-projects-involving-co-authorship-between-internal-researchers-and-external-researchers-handled",
    "title": "Guide for Researchers",
    "section": "3.8 How are projects involving co-authorship between Internal Researchers and External Researchers handled?",
    "text": "3.8 How are projects involving co-authorship between Internal Researchers and External Researchers handled?\nProjects where internal and external researchers have access to the data are designated “mixed projects”.\nIf the mixed project is implemented in the External Server and only data with low level of confidentiality is used, then the distinction is irrelevant as all researchers are treated as external and the data is anonymized (Level 1).\nHowever, if the data needed for the project is classified at a higher confidentiality level, external researchers can only access modified data or, in the most restrictive cases, pseudo-data, but never the original data. In such mixed projects, where the external researcher does not have access to the original, the internal researcher is responsible for ensuring that the information shared with their external co-authors complies with the confidentiality requirements associated with the data.\nIn these cases, BPLIM will open a second “parallel” account with access only for the internal researcher(s) and place all the original (anonymized) data there. It will be the responsibility of the internal researcher to execute all scripts on the original data stored in this “parallel” account.\nFurthermore, it will be their responsibility to ensure that external researchers do not have any access to confidential data. Specifically, external co-authors must not access the project account containing the original data, the internal co-author’s desktop computer, or any logs that may contain confidential information.\nIn a mixed project all interaction with BPLIM should be done via the internal researcher."
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#how-can-an-external-researcher-gain-surrogate-access-to-a-dataset",
    "href": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#how-can-an-external-researcher-gain-surrogate-access-to-a-dataset",
    "title": "Guide for Researchers",
    "section": "3.9 How can an External Researcher gain surrogate access to a dataset?",
    "text": "3.9 How can an External Researcher gain surrogate access to a dataset?\nThe BPLIM Dataset Guide lists the datasets that can be accessed in surrogate mode by an external researcher that does not have an internal co-author. In that case the external researcher needs to submit a detailed project to BPLIM. The project will be evaluated according to the relevance of the topic to the research agenda of BdP. Only projects deemed relevant will be granted surrogate access. If BPLIM decides to support the project, external researchers will be assigned a data expert at BPLIM, who will collaborate with them to prepare and run scripts on the original data. However, the coding itself remains the ultimate responsibility of the external researcher, and BPLIM will not validate or certify the scripts written by the researcher. External researchers are encouraged to work closely with BPLIM staff to ensure they achieve the intended results and are also encouraged to discuss their findings with BPLIM staff. It is highly recommended that external researchers initiate their project with a short-stay visit in BPLIM, during which time they can discuss their research with BPLIM staff and gain a thorough understanding of the data complexities. Additional visits throughout the project are also encouraged. All outputs shared with the external researcher are subject to the usual disclosure restrictions."
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#transfer-of-external-files",
    "href": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#transfer-of-external-files",
    "title": "Guide for Researchers",
    "section": "3.10 Transfer of external files",
    "text": "3.10 Transfer of external files\nInternal researchers working in Pitagoras can freely copy files to and from their accounts. Thus, they are free to place external files in their Pitagoras accounts. However, if the external data needs to be merged with BPLIM datasets using an anonymized key, then the internal researcher must be working in a BPLIM project account at Pitagoras. They will also need to fill in an application for using an external dataset (see below). BPLIM will anonymize the external datasets using the same linking key as the one used for the BPLIM datasets. Note that BPLIM identifiers (eg: tina and bina) are specific to a project and are not valid to link files exchanged between accounts.\nIf the account is shared with external researchers - a “mixed project” - the internal researcher must ensure that external researchers are allowed access to the external dataset and that they do not gain undue access to confidential data. At the request of the internal researcher, BPLIM will anonymize/modify the external datasets intended for use in “mixed projects”.\nExternal researchers may also request that external data files be placed in their accounts. BPLIM staff will assist if there is a need to merge external datasets with BPLIM datasets. External datasets typically contain aggregated data, but it may be possible to add external datasets with finer granularity. BPLIM staff will assess if the addition of the external datasets increases the risk of identification of individual observations. In such cases, additional measures will be undertaken to ensure that the confidentiality of the data is preserved once external files are merged with existing BPLIM datasets. These situations will be analyzed case by case and discussed with BPLIM staff.\nAll external datasets provided to BPLIM should be in a Stata or CSV format and an External Datasets Form must be filled in. In the form, researchers are required to explain the data provenance, provide a justification for its use, and identify the key variables that enable linking the external dataset with BPLIM’s datasets. The researcher must also certify that all researchers with access to the account are authorized to use the data. It is the responsibility of the researcher to ensure the external files can be legitimately used for that purpose."
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#statistical-software-1",
    "href": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#statistical-software-1",
    "title": "Guide for Researchers",
    "section": "4.1 Statistical Software",
    "text": "4.1 Statistical Software\nWhen researchers apply for a new project, they will need to specify the software to be used. Available options are Stata, R, Julia, and Python. BPLIM provides the researcher with a default list of external packages/ados for each software. If researchers require additional packages, they must specify the package, its source, and version.\nFor each project, BPLIM creates a container with the software and packages required for the project. If researchers require additional external packages during the project, they should send a request via email to BPLIM.\nOnce the project account is set up, researchers will have access to a Linux environment where they can use the container. Templates for writing code are also provided in the account, and researchers should strive to adhere to the conventions outlined in these templates as much as possible."
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#bplim-tools",
    "href": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#bplim-tools",
    "title": "Guide for Researchers",
    "section": "4.2 BPLIM Tools",
    "text": "4.2 BPLIM Tools\nBPLIM staff has developed several Stata packages to assist researchers in their tasks. Some of these tools are tailored for use with BPLIM datasets, while others have broader utility. To promote transparency in coding and versioning, BPLIM makes all tools available on Github. Users are welcome to suggest improvements or add their own contributions. Tools with general applicability can be installed directly from Github on any internet-connected machine."
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#can-researchers-transfer-files-from-the-server",
    "href": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#can-researchers-transfer-files-from-the-server",
    "title": "Guide for Researchers",
    "section": "5.1 Can researchers transfer files from the server?",
    "text": "5.1 Can researchers transfer files from the server?\nExternal researchers are never allowed to transfer files to/from BPLIM’s accounts. This policy also applies to internal researchers accessing the external server. However, internal researchers have the flexibility to transfer files to and from their accounts in Pitagoras, including data and output logs.\nWhen internal researchers collaborate with external co-authors, it becomes their responsibility to ensure that all files shared with external co-authors comply with BPLIM’s data security and confidentiality policies. This ensures that no sensitive or restricted information is improperly disseminated.\nFor further details, please refer to the Output Control Guide."
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#what-restrictions-apply-to-the-release-of-output-logs",
    "href": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#what-restrictions-apply-to-the-release-of-output-logs",
    "title": "Guide for Researchers",
    "section": "5.2 What restrictions apply to the release of output logs?",
    "text": "5.2 What restrictions apply to the release of output logs?\nAs a general principle, BPLIM will not verify the “correctness” of scripts used by researchers to generate logs and expressly disclaims responsibility for any errors or inaccuracies in researchers’ code. This responsibility solely rests with the researcher.\nOutput logs should never contain information that discloses individual dataset records; they should only include aggregate-level information. Therefore, listings of individual records, tables with cells derived from manipulation of three or fewer observations, statistical measures with standard errors of zero, minimum and maximum values, etc., are prohibited.\nPlain text files (including Latex and comma or tab separated values) are the preferable formats, although other formats may be accepted provided that the data content can be easily verified. Graphical outputs should be generated in “.png” format.\nIn mixed projects, the internal researcher is responsible for ensuring adherence to these principles. BPLIM will assist in this process upon request from the internal researcher. For projects involving only external researchers, BPLIM staff will verify the conformity of all outputs.\nOutput disclosure control depends on staff availability and may take longer in periods of high workload. BPLIM staff will only answer requests for output extraction sent by email and will take the necessary time to ensure that all confidentiality requirements are safeguarded. Researchers should keep their output requests to a minimum and, whenever feasible, these requests should be of final outputs.\nFor further details, please refer to the Output Control Guide."
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#what-happens-if-the-researcher-violates-the-rules",
    "href": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#what-happens-if-the-researcher-violates-the-rules",
    "title": "Guide for Researchers",
    "section": "5.3 What happens if the researcher violates the rules?",
    "text": "5.3 What happens if the researcher violates the rules?\nBPLIM assumes that all researchers operate in good faith and should endeavor to adhere to the provisions outlined in the signed “Declaration on Confidentiality and Use of Data”. In cases where a researcher engages in behavior deemed inappropriate, BPLIM reserves the right to terminate or suspend all projects involving the researcher and the institution to which he/she is affiliated."
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#replicability-of-work",
    "href": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#replicability-of-work",
    "title": "Guide for Researchers",
    "section": "6.1 Replicability of work",
    "text": "6.1 Replicability of work\nResearchers working at BPLIM have all the necessary conditions to ensure that their work is reproducible. All BPLIM datasets are versioned and can be exactly recreated based on archived extractions. Researchers have access to a Singularity container specific to their project, including the respective definition file. This means that the computing enviroment is also replicable. All external files, including external datasets and scripts, are stored in the project folder. Researchers can utilize BPLIM’s Replication App to verify the replicability of results and to generate a replication package. Ultimately, it is up to the researcher to garantee that his/her work is reproducible."
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#replicability-by-third-parties",
    "href": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#replicability-by-third-parties",
    "title": "Guide for Researchers",
    "section": "6.2 Replicability by third parties",
    "text": "6.2 Replicability by third parties\nIf requested, BPLIM will work with third-parties such as data editors, certification services, or individual researchers to provide conditions for replication of results of individual projects.\nThe replication process consists of opening an account for the “replicator” and providing him/her with access to the same conditions as the researcher(s). Projects by internal researchers may have to be replicated in “surrogate” mode.\nTo facilitate replication, researchers should unequivocally identify all datasets used in the analysis, ideally using the Replication App. All other needed files should be provided by the “replicator”.\nBPLIM will work directly with data editors or certification services (e.g. Cascad) to evaluate the best approach to implement their replication protocol. In the case of individual researchers willing to act as “replicators” they will have to go through the standard process of submitting a research project."
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#archiving",
    "href": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#archiving",
    "title": "Guide for Researchers",
    "section": "6.3 Archiving",
    "text": "6.3 Archiving\nBy default, once a project is closed BPLIM will keep a copy of all syntax files (e.g. text files with “do”, “R”, “py”, and “jl” extensions) plus all files found in the “initial_dataset” and the “tools” folders.\nA copy of the syntax files will be sent to the researcher, who should verify that the list is complete. All other files will be deleted unless the researcher explicitly requests that certain external file(s) are archived. Ideally, the researcher should use the Replication app and save the replication package created by the application.\nOnly in exceptional and well justified circumstances will BPLIM agree to archive intermediate data files created by the researchers. It is the responsibility of the researcher to ensure that all files needed for proper replication of the results are archived with the project.\nArchives are kept for ten years since the closing of the project. BPLIM may archive a project that has been inactive for more than one year."
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#citations-and-research-outputs",
    "href": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#citations-and-research-outputs",
    "title": "Guide for Researchers",
    "section": "7.1 Citations and research outputs",
    "text": "7.1 Citations and research outputs\nAll BPLIM datasets should be cited according to the information provided in the data manual. If available, the Digital Object Identifier (DOI) should be referenced.\nWhen the topic analyzed by the external researcher(s) bears special relationship to BdP’s statutory tasks, researchers are encouraged to discuss their results with BdP staff. In this case, BdP may ask to see a copy of the work, prior to any public release, and may provide suggestions regarding the research project.\nMoreover, in situations where data access is granted based on the relevance of the topic (“surrogate access”) researchers are not only encouraged to discuss their results with BdP staff but must also seek BdP approval prior to any public release of their results.\nAs soon as available, researchers are required to send to BPLIM a copy of all research outputs (working paper, conference proceedings, paper, thesis, etc.) related to the project."
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#how-can-i-contact-bplim",
    "href": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#how-can-i-contact-bplim",
    "title": "Guide for Researchers",
    "section": "8.1 How can I contact BPLIM?",
    "text": "8.1 How can I contact BPLIM?\nThe preferred way to contact BPLIM is through email: BPLIM@bportugal.pt. For projects already ongoing the subject line should always include the project reference (eg: p###_Surname). If necessary you can contact us at:\nAddress: \nBanco de Portugal \nMicrodata Research Laboratory \nRua do Almada, 71 \n4050-036 Porto \nPortugal"
  },
  {
    "objectID": "Guides/06_Output_Control/Output_Control_Guide.html",
    "href": "Guides/06_Output_Control/Output_Control_Guide.html",
    "title": "Output Control at BPLIM",
    "section": "",
    "text": "Only researchers identified in the project may request extraction of outputs. BPLIM will not check the “correctness” of the scripts used to produce the output files. The code is the sole responsibility of the researcher. However, researchers should abide by the following rules:\n\n\nOutput files should never contain information that reveals individual records. This means that listings of individual records, tables with cells whose values were obtained by manipulation of three or less observations, statistical measures with standard errors of zero, minimum and maximum values, etc., are not allowed. BPLIM may refuse disclosure of output files if it perceives that the information in the logs may, directly or indirectly, reveal confidential information.\n\n\n\n\nAll outputs files must be generated by a script file which should be easy to identify. Requests for output extraction may be refused if BPLIM cannot associate the script file with the output.\n\n\n\n\nAll aggregate statistics must report the underlying value of N. This means, for example, that all regression outputs must report the number of observations and tables must report the number of observations per cell. Depending on the type of data BPLIM may impose stricter criteria at its discretion.\n\n\n\n\nOutput files of results must be plain text files (allowed formats are “txt”, “csv”, “log” and “tex”).\n\n\n\n\nComments in output files should not include references to data values.\n\n\n\n\nThe only graphical outputs allowed are “png” files. The information depicted in the graph must be of aggregated values. In some circumstances BPLIM may request that authors report a table with the N associated with each data point depicted in the graphic.\n\n\n\n\nResearchers should keep their output requests to a minimum and, as much as possible, these should be of final outputs.\n\n\nNote that output verification depends on staff availability and may take longer in periods when the workload is higher. BPLIM staff will only answer requests for output extraction sent by email and will take as long as needed to ensure that all confidentiality requirements are safeguarded.\nTo request an extraction researchers should place the files in the “results” folder and send an email requesting extraction of the results to bplim@bportugal.pt. Please do not forget to include the project number in the subject."
  },
  {
    "objectID": "Guides/06_Output_Control/readme.html",
    "href": "Guides/06_Output_Control/readme.html",
    "title": "BPLIM",
    "section": "",
    "text": "Rules for Output Control\nThis folder contains the Guide to Output Control Rules, which outlines the guidelines and procedures that apply when external researchers request the release of their work outputs."
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#upon-access-approval",
    "href": "Guides/03_External_Server/External_Server_Guide.html#upon-access-approval",
    "title": "External Server Guide",
    "section": "1.1 Upon access approval",
    "text": "1.1 Upon access approval\nThe User will be able to connect to the external server using NoMachine client access: see Section 8.4 for details on installation and use."
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#password-policy",
    "href": "Guides/03_External_Server/External_Server_Guide.html#password-policy",
    "title": "External Server Guide",
    "section": "1.2 Password policy",
    "text": "1.2 Password policy\n\nThe first password delivered must be changed at the first login.\nAfter 60 days the password will expire: the login window will show new password\nThe passwords to be specified must meet the requirements described in Section 8.3."
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#first-steps",
    "href": "Guides/03_External_Server/External_Server_Guide.html#first-steps",
    "title": "External Server Guide",
    "section": "1.3 First steps",
    "text": "1.3 First steps\nOnce you start NoMachine, these are the first three screens you will see:\n\n\n\n\nSelect the “Kickoff Application Launcher” menu (in the lower left corner):\n\n\n\n\nThen you should:\n\n\nClick on the “Applications” button\nSelect “BPLIM” and click on your project (i.e., “pxxx_name”). At this stage, you should see a graphical environment (‘Dolphin’ application1) like this:\n\n\nYou can see the prompt command line together with ‘Dolphin’ using the keyboard shortcut ‘F4’.\n\n\nFiles with the \"sh\" extension allow you to send commands to your operating system or to enter your operating system for interactive use (for example, the file xstata17mp.sh will launch the graphical version of Stata 17). You can start the application by double-clicking the file name in ‘Dolphin’2 or by typing in the Terminal xstata17mp.sh\n\n\n\nThe directories that you have access to within the folder include:\n\n\n\n\n\n\n\n\ninitial_dataset\nData sources provided by BPLIM.\nYou have read-only access to this directory.\n\n\ninitial_dataset/modified\nModified data provided by BPLIM.\n\n\nresults\nOutput files that researchers wish to generate and extract from the server.\nYou have read-write access to this directory.\n\n\ntools\nSpecific analysis tools.\nYou have read-only access to this directory.\n\n\nwork_area\nTemporary work directory.\nYou have read-write access to this directory.\n\n\n/bplimext/doc/Manuals\nManuals and auxiliary files are available here.\n\n\n\n\nYou will have in your work_area folder templates for both Stata and R(R.sh). By default, the template file is read-only.\n\n\nTo reset and disconnect the remote desktop connection or session, you can simply log out of your remote session, as shown in the screenshot below. After you log out, close the window.3\n\n\nConfirm before exiting by clicking on the “Logout” button to close the window:4\n\n\nIn case you do not logout, your session will be left open until your next login. You may use this facility to run your programs. However, one must be aware that this option uses resources from the server, so the efficient solution to run your programs “overnight” is using the batch mode as described in Step 6 below. Furthermore, in case the server is rebooted during a maintenance procedure, your session will be automatically closed, and unsaved documents will be lost. We recommend you save at regular intervals your statistical programs."
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#keep-your-home-area-tidy",
    "href": "Guides/03_External_Server/External_Server_Guide.html#keep-your-home-area-tidy",
    "title": "External Server Guide",
    "section": "2.1 Keep your home area tidy",
    "text": "2.1 Keep your home area tidy\n\nDo not save files in your home area /home/USER_LOGIN. In case you exceed its size you will not be able to login.\nCheck regularly the size of your project on the harddrive. Open a Terminal and apply the following steps:\n\n\nMove to the project folder: cd /bplimext/projects/p000_xxx_yyy/\nList project size: du -h\nCheck size by folder and list folders with at least 1Gb: du --max-depth 1 -h | sort -n | grep G\nMove to the folder ‘work_area’: cd work_area\nRepeat the check in this folder: du --max-depth 1 -h | sort -n | grep G\nIdentify duplicated and temporary files and delete them: use the command rm\nCompress big files/folders you are not using at the moment:\n\n\nCompress folders: tar -zcvf YOUR_FOLDER.tar.gz YOUR_FOLDER\nCompress individual files: gzip YOUR_FILE"
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#using-the-terminal",
    "href": "Guides/03_External_Server/External_Server_Guide.html#using-the-terminal",
    "title": "External Server Guide",
    "section": "2.2 Using the Terminal",
    "text": "2.2 Using the Terminal\nLinux’s Terminal is a command-line interpreter. You can use the ‘shell’ for a wide range of tasks, including searching files and files’ contents, organizing your working space, and, most importantly, running your programs in batch mode.\n\nLinux’s Terminal can be accessed from5\n\nRedHat > Applications > System > Terminal\n\n\nSee Section 8.1 for a list of some of the most used commands.\nIf you use a non-English keyboard, the ‘true’ keyboard might be different from the one you see. The changes apply mostly to the symbols, not letters or numbers. For example, in case you have a Portuguese keyboard on your computer the ‘+’ is now in key ‘?’, or the ‘*’ is in SHIFT + ?. This issue is specific to the Operating System of your computer\nRemember that Linux is case-sensitive: e.g., “LS” and “ls” are treated as different commands.\nYou can use the arrow keys to scroll up and down through the commands you have entered.\nYou can use the “Tab” key to complete the command line automatically.\ne.g., type the following line to list elements within a folder in a ‘human-readable’ format, h, long list format, l, in reverse order, r, sort by modification time, t, and almost all files, A,\nls -lArth"
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#stata",
    "href": "Guides/03_External_Server/External_Server_Guide.html#stata",
    "title": "External Server Guide",
    "section": "3.1 Stata",
    "text": "3.1 Stata\nStata versions available in the server: 15, 16 and 17 (adjust the following lines to the Stata version you want to use)\n\nStata can be accessed in interactive graphical or non-graphical modes.6\n\n\nInteractive non-graphical mode\nMove to the desired folder, e.g.,\ncd /bplimext/projects/I001_jdoe/\nand type\nstata17-mp\n\n\n\nYou may add a ‘PATH’ to your system folder by typing, for example on Stata 16, the following command in the shell “vi ~/.bash_profile” and adapt the following line\nPATH=$PATH:$HOME/.local/bin:$HOME/bin:/opt/bplimext/stata17\nFor the interactive graphical mode click on the icons “xstata17mp.sh” (Stata 17) located in the ‘desktop’, depending on the desired Stata version,\n\n\n\n\nYou can use the ‘Do-file Editor’ in Stata to create your own “do-files” and “ado-files”, or you can use KWrite editor (or ‘gedit’)\nYou can open it from RedHat > Applications > Utilities > KWrite. You can also launch ‘KWrite’ from the ‘shell’ by typing ‘kwrite’\nIn case the Stata’s icon is not on your desktop, use Dolphin, move to the folder ‘/bplimext/scripts/wrappers/’, and drag and drop the file ‘xstata17-mp’ into the desktop\n\n\nNOTE: to start Stata use the shortcuts in your project’s folder.\n\nTo look for “ado-files”:\n\n“Ado-files” are text files containing the Stata program. It is advisable that one create and save his/her “ado-files” so the results can be replicated later by running the saved “ado-files” on the BPLIM’s datasets.\nStata looks for “ado-files” in several places. When it comes to personal ado-directories, they can be categorized in four ways:\n\n(SITE), the directory for “ado-files” your site might have installed;\n(PLUS), the directory for “ado-files” you might have installed;\n(PERSONAL), the directory for “ado-files” you might have written;\n(OLDPLACE), the directory where Stata users used to save their personally written ado-files.\n\nThe ado-files you have just written or those created for this project can be found in the current directory (.).\nSpecific ‘ado-files’ you may ask to be made available in the server will be placed in your folder ‘/bplimext/projects/YOURPROJECTID/tools’. You should add this folder to your Stata ‘ado-files’ folder by executing the following command within Stata,\nsysdir set PERSONAL \"/bplimext/projects/YOURPROJECTID/tools\"\nYou may also edit your ‘profile.do’ file, located in your root folder, “/home/YOURPROJECTID/”, and add key instructions you may want to be executed every time you start Stata. The above instruction is one of such cases. You can create or edit the file ‘profile.do’ using ‘Do-file Editor’ within Stata (‘vi profile.do’ or KWrite are also a possibility).\nThe sysdir command within Stata will tell you where they are on your computer:\n\n\n\nStart a 'shell' in Linux and navigate to the directory of the “do-file” file that you want to run (ex: prog1.do)\n\ncd /bplimext/projects/I001_jdoe/work_area/\n\nYou might find it easier to use ‘Dolphin’ (= File Manager) to move over your folder structure. In this case, we recommend activating the ‘shell’ (= ‘Terminal’) associated with ‘Dolphin’\n\n\nuse Dolphin/File Manager\nclick ‘F4’ to activate the shell with Dolphin. Benefit: fast transition within folders and, at the same time, the ability to run shell commands\n\n\nCreate an ASCII file named, e.g., ‘batch_prog1’\nInside the file, write just a line with the execution command you would type in the ‘shell’; e.g.,\n/bplimext/projects/I001_jdoe/stata-mp do\n/bplimext/projects/I001_jdoe/work_area/prog1.do\nYou can use, for example, the command line app ‘vi’ to create the batch file\n\n\n\nThe batch file can also be created using apps like ‘kwrite’ or Stata ‘do file editor’\n\n\nor\n\n\nYou may add the extension ‘.txt’ to the name of the batch file, as sometimes Stata doeditor does not ‘see’ the file ‘batch’, while it ‘sees’ ‘batch.txt’\nOnce the batch file is created, one runs the .do file in batch mode by typing in the ‘Terminal’:\nat now -f batch.txt\nType ‘man at’ to see a further option of the command ‘at’; e.g., one could type\nat now + 5 hours -f batch.txt\n\nor\n`at now + 4 minutes -f batch_prog1`\nto run the Stata program within 5 hours or 4 minutes from now, respectively. ‘man’ is the help function in Linux\n\nType ‘top’ in the shell/Terminal to confirm the program is running\nUnder ‘top’ type ‘i’ to hide irrelevant processes (show less output)\nTo kill a running process with ‘top’ press ‘k’, for ‘kill’, write > the process number and then type ‘9’. The process number is > identified in the first column as PID\nTo get out of the top, type ‘q’\nUseful features of the command ‘at’:\n\n\n‘atq’: use it to see programs in the batch queue (an ‘=’ sign indicates the program is running; an ‘a’ indicates it is in the queue and we see the time when it will be executed)\n‘atrm #’: remove a batch from the batch queue\none can see how the batch is running by typing\ntail --f logcrc_may21.log\n\nIt allows you to see an updated version of the last lines of the log; i.e., it updates each time Stata changes the log. A key advantage of tail is that it does not interfere with the log file. Namely, it does not write over it.\n\nAnother way to run a program in the background is by using the command ‘screen’\n\n\n‘screen’ is useful when one wants to run Stata in interactive mode and still guarantees that if the network connection goes down one does not lose the session. We can simply kill the ‘NoMachine’ session and recover it later by typing ‘screen --r’\nWe can run several instances of screen. If this is the case, after opening a new NoMachine session, we need to type in the Terminal shell ‘screen -d’ to identify the running background sessions. We can retrieve a particular session by knowing the ‘pid’ number and typing ‘screen -r 34176’"
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#r",
    "href": "Guides/03_External_Server/External_Server_Guide.html#r",
    "title": "External Server Guide",
    "section": "3.2 R",
    "text": "3.2 R\n\nAs with Stata, R can be accessed in interactive graphical or non-graphical modes.\n\n\nInteractive non-graphical mode: go to the RedHat symbol and type `R’ in the Search box\n\n\n\nAlternatively, you can open a Terminal and type\n\n\nR\n\n\nPlease make sure R is in your PATH; type $PATH in the Terminal. If this is not the case, type PATH=$PATH:/usr/bin/\n\n\nUsing RStudio.\n\n\nOpen a Terminal and type\n\n\nrstudio\n\n\nPlease make sure RStudio is in your PATH; type $PATH in the Terminal. If this is not the case, type\n\n\nPATH=$PATH:/opt/bplimext/R/usr/lib64/rstudio/bin/\n\n\nIn case you face difficulties opening/saving files in RStudio, please open a Terminal and type\n\n\n/bplimext/scripts/wrappers/R.sh\n\nIMPORTANT: do not save your workspace image in your home folder (Save workspace image? [y/n/c]). If you want to keep the workspace file save it in your project folder under work_area.\nRStudio Font Type: please make sure you are not using Font Type Courier (Menu Tools, Global Options, Appearance …)"
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#python",
    "href": "Guides/03_External_Server/External_Server_Guide.html#python",
    "title": "External Server Guide",
    "section": "3.3 Python",
    "text": "3.3 Python\n\nOpen a Terminal and type\n\n\npython3"
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#julia",
    "href": "Guides/03_External_Server/External_Server_Guide.html#julia",
    "title": "External Server Guide",
    "section": "3.4 Julia",
    "text": "3.4 Julia\n\n3.4.1 Alternative A\n\nOpen a Terminal and type (julia is located in /opt/bplimext/julia/lib/, you can add it to your PATH)\n\n\njulia\n\n\nUse Atom: open a Terminal and type\n\n\natom\n\n\n\n3.4.2 Alternative B: using a container (see the discussion in the Appendix)\n\nRequest a container with Julia for your project\n\n\nThe container will be in the folder tools inside the project folder\n\n\nAdvantages: you can build a Julia setup fine-tuned to your project, including the definition of Julia’s version and packages"
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#updates-to-the-commands-and-packages-list",
    "href": "Guides/03_External_Server/External_Server_Guide.html#updates-to-the-commands-and-packages-list",
    "title": "External Server Guide",
    "section": "3.5 Updates to the commands and packages list",
    "text": "3.5 Updates to the commands and packages list\nAdditional commands/packages or updates to the existing ones have to be requested from BPLIM’s Team."
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#build-a-container-to-fine-tune-your-statistical-packages",
    "href": "Guides/03_External_Server/External_Server_Guide.html#build-a-container-to-fine-tune-your-statistical-packages",
    "title": "External Server Guide",
    "section": "3.6 Build a container to fine-tune your statistical packages",
    "text": "3.6 Build a container to fine-tune your statistical packages\nYou can use Singularity containers in the server. To do so, please send us the definition file so we build the image and put it in your working area. You can find detailed information on Singularity containers at https://sylabs.io/. We provide some notes in the Appendix"
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#shell_commands",
    "href": "Guides/03_External_Server/External_Server_Guide.html#shell_commands",
    "title": "External Server Guide",
    "section": "8.1 Basic ‘shell’ commands on Linux",
    "text": "8.1 Basic ‘shell’ commands on Linux\n\ntop: List the procedures that are being executed on the server\n\npress ‘i’ option to omit background processes;\nclicar press ‘h’ para help on top options ; ‘h’ > option to obtain the top command help.\n\npwd: Show current working directory\ncd: Change directory\ncd /bplimext/projects/I001_jdoe/work_area/\n‘cd ~’ moves to your home folder\n\n\n\ncp: Copy file(s) to a given path\ncp prog1.do /bplimext/projects/I001_jdoe/results\n\n\n\nmv: Move file(s) or rename a file from a given path\nmv prog1.do /bplimext/projects/I001_jdoe/results\n\n\n\nrm: Delete a file\nrm /bplimext/projects/I001_jdoe/results/prog1.do\n\n\n\nmkdir: Creates a directory\nmkdir programs\n\n\n\nrmdir: Deletes a directory\nrmdir programs\n\n\n\nscreen: Switch between screen\nscreen top\n\n\n\nman: Show the manual page for the given command\nman ls\n\n\n\ndu -h: Check the information on disk usage of files and directories.\n\n\nThe “-h” option with “du” command provides results in “Human Readable Format”.\n\nEx: du /bplimext/projects/I001_jdoe/work_area/\n\ndf -h: Check disk space utilization and show the disk space > statistics in “human readable” format.\nvi: View ‘ASCII’ files; e.g., log files\nghostscript: Preview files with the extensions of .eps and .pdf\n\n\nghostscript /bplimext/projects/I001_jdoe/results/file_name.pdf\n\n\nokular: View ‘PDF’\nfind: Find files\n\n\nStructure: find /path option filename\nfind . -name \"*.do\"\nSend the ‘find’ output to a file:\nfind . -name \"\\*.do\" > find_results.txt\nLook for a particular string within the ‘find’ output:\nfind . -name \"\\*.do\" | grep \"analysis\"\nIdentify files with extension ‘.do’ that contain the word ‘graph’:\nfind . -name \"\\*.do\" -exec grep \"graph export\" '{}' \\; -print\n\n\npasswd: Change your password\nTo exit a program, type CTRL + C (‘CTRL + C’ kills a particular execution in the shell)"
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#using-the-vi-file-editor",
    "href": "Guides/03_External_Server/External_Server_Guide.html#using-the-vi-file-editor",
    "title": "External Server Guide",
    "section": "8.2 Using the ‘vi’ file editor",
    "text": "8.2 Using the ‘vi’ file editor\n\nIn the shell type ‘vi batch1.txt’\nThese are the main shortcut keys\n\n‘i’ insert text\n‘ESC’ key get out of the ‘insert’ mode\n‘x’ delete specific characters\n‘dd’ delete a full line\n‘10 dd’ delete 10 lines\n‘yy’ copy lines\n‘p’ paste lines\n‘SHIFT + G’ go to the last line\n‘gg’ goes to the first line\n‘ESC + q!’ exit ‘vi’ without writing\n‘w!’ write and replace the file\n‘ESC + q’ exit the ‘vi’ session\nCheck, for example, https://www.cs.colostate.edu/helpdocs/vi.html\n\nMuch easier solution: call ‘gedit’ file editor\nLinux commands I have to add to the manual\n‘CTRL + R’: allows me to recover a previous command\nvi .bash_history"
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#password",
    "href": "Guides/03_External_Server/External_Server_Guide.html#password",
    "title": "External Server Guide",
    "section": "8.3 External server’s password requirements",
    "text": "8.3 External server’s password requirements\n\n\n\n\n\n\n\n\nRule\nValue\nNotes \n\n\nMaximum Password Lifetime\n60 days\nAfter 60 days the password will expire and has to be changed in the next login. The password can be changed at any moment using: (1), “All Applications | Settings | System Settings – Account Details”, click “Change Password”; or, (2), in the ‘Shell’ type ‘passwd’\n\n\nMinimum Number of Character Classes\n4\nYou should include at least 4 classes of characters in the password. For example, small letters, capital letters, numbers and punctuation marks.\nThere are a total of five classes:\n\n1. Capital letters : A-Z\n2. Small letters: a-z\n3. Numbers: 1-9\n\n\n\n\n\n\n4. Punctuation marks: <space> ! % & ( ) * + . , { } [ ] ~ \" # $ ' - / \\ ^ _ ` |\n5. Characters above 127 (0x7F): marked characters (ã, á, ä, à, etc.); symbols (@, £, §, º, ª, «, », etc.)\n\nNumber of characters: by using the same character 3 or more times may imply the use of an additional class (it is highly recommended that you do not use consecutively the same character more than 2 times)\n\n\nMinimum Length of Password\n8\nThe minimum size of the password is 8 characters (it may be higher in case you repeat characters)\n\n\nPassword History\n7\nOne cannot use a password defined in the previous set of 7 passwords\n\n\nMaximum Consecutive Failures\n6\nIf the user fails 6 consecutive times the password the account will be locked for the time defined in “Lockout Time”\n\n\nFail Interval\n60 sec.\nTime interval for attempts to enter a password to be considered consecutive. If more than 60 seconds have elapsed since the last attempt, consecutive attempts are no longer considered, ie the number of failures, according to the requirement \"Maximum Consecutive Failures\" becomes one.\n\n\nLockout Time\n600 sec.\nTime (10 minutes) during which the account will be locked if the maximum number of failed attempts is reached."
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#install_nomachine",
    "href": "Guides/03_External_Server/External_Server_Guide.html#install_nomachine",
    "title": "External Server Guide",
    "section": "8.4 Download, install and configure NoMachine client",
    "text": "8.4 Download, install and configure NoMachine client\nStep 1: go to the link below and use the credentials provided by BPLIM to access the site. Note: sometimes the internet provider, e.g., a University, may block access to this particular website. Please check with your provider in case you get an error while trying to use the link.\nhttps://www.bportugal.pt/webdrive/index.php/s/irAzxZmir8KHyzD/authenticate\n\nStep 2: download the file with an extension compatible with your OS (Operation System)\n\nStep 3: install ‘NoMachine’\n\n\n\n\n\n\n\n\nStep 4: reboot your computer\n\nStep 5: NoMachine client access configuration\nStep 5.1: start ‘NoMachine’ and create a new connection\n\n\n\nStep 5.2: Choose ‘NX protocol’\n\n\n\nStep 5.3: Define the ‘Host’ as bplimexterno.bportugal.pt, ‘Port’ 4000\n\nClick ‘Use UDP communication for multimedia data’\n\nStep 5.4: Use password authentication, with or without proxy, depending on the instructions of the network administrator/user 's computer support, with the name “BPLIM-LabInvestMicrodados Banco de Portugal”.\n\nStep 5.5: Do not use a ‘proxy’\n\nStep 5.6: Define a name for the connection\n\nStep 5.7: Once the entry for bplimexterno.bportugal.pt has been created, connect:\n\n\n\n\n\nStep 5.8: Before the first effective connection, it may be necessary to accept the certificate from bplimexterno.bportugal.pt\nThe Investigator should verify that the \"fingerprint\" (verification code) is:\nSHA256 ED 1B D9 E2 C2 F8 C6 08 1A 53 5F 97 DA 71 77 D9 D2 EE 7A 5F 9C 35 87 B3 19 F4 7E A1 CB 2C 68 0B\n\n\n\n\n\nStep 5.9: Connect with the UserID (case sensitive) and password provided by Banco de Portugal:\n\nStep 5.10: After the first successful login, it is necessary to change the password, which must comply with the Password Policy defined above.\nIf the new password does not comply with the Password Policy, the original password provided by the Banco de Portugal will be re-requested. See Appendix 3 for details.\nThe NoMachine client does not tell you why the new password was not accepted – it is the responsibility of the user to verify that the new password is in compliance.\n\nStep 5.11: Upon login success, the following screens should appear\n\n\nOnce logged in and with access to a KDE session, click on the upper right corner of the KDE desktop, as shown below, to access the menu and then expand the screen as exemplified for greater ease of use.\nStep 5.12: You should see the following screen.\n\nStep 5.13: Click ‘Display’\n\nStep 5.14: Click ‘Fit to window’ and click ‘Done’"
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#frequently-asked-questions",
    "href": "Guides/03_External_Server/External_Server_Guide.html#frequently-asked-questions",
    "title": "External Server Guide",
    "section": "8.5 Frequently Asked Questions",
    "text": "8.5 Frequently Asked Questions\n\n\nMac users are not able to install NoMachine, receiving the following message\n\n\n\n\nPlease check if your Mac OSX is updated. Temporary solution: download NoMachine Enterprise Client from the official website, and run the installation file:\nhttps://www.nomachine.com/download-enterprise#NoMachine-Enterprise-Client\n\nNoMachine authentication failure\n\n\n\n\n\n\nIn some cases, it occurs due to a different keyboard layout. For example, if you have a Portuguese keyboard, but the website assumes a US keyboard, and your password contains a symbol like ‘ç’ then you will get a “wrong password” message. Please check the keyboard layout that is active when you type the password. Alternatively, change the password after the first login with NoMachine. Use linux’s command ‘passwd’.\nLogin fails, and the system shows the message: \"Could not connect to the server. Error is 138: Connection is timed out\" Please check if your network has a strict firewall; e.g., some researchers are not able to reach BPLIM’s server within their University network. Please check if in a different location, like at home, the connection works.\n\n\nUser pressed ‘Lock’ instead of ‘Log out’ and the unlock/password does not work:\n\n\nCheck if the keyboard settings are correct (e.g., PT or UK)\nClose the ‘NoMachine’ connection and start a new one. Before the last step -before the 'Login’- right-click and choose ‘Logout’. Double-click for the new connection\n\n\n“Cannot see the screen in NoMachine” (see image below)\n\n\n\n\n\nOPTION A: move your mouse on top the upper right corner of NoMachine, you should see a “folded like sheet”, left-click your mouse, go to ‘Display’, ‘Change settings’, and click in ‘Disable client side hardware decoding’\n\n\n\nOPTION B: Close the ‘NoMachine’ connection and start a new one. Before the last step -before the 'Login’- right click and choose ‘Logout’. Double-click for the new connection\n“Error: Parameter ‘agentm_display’ has bad value” (see image below)\n\n\n\n\n\nYour home folder is full (/home/USER_LOGIN): Do not save files in your home folder\nAsk BPLIM Staff to empty space in your home folder\n\n\nSession is frozen\n\n\nGo to NoMachine first screen and double-click in the following icon\n\n\n\nright-click on the icon below and choose “Terminar sessão”\n\n\n\nVisualizing tables\n\n\nIn case you want to see the pdf of tables you have exported to you can create a generic tex file, main.tex, with the following content:\n\n\n\\documentclass{article}\n\\begin{document}\n\\input{your_table.tex}\n\\end{document}\n\n\nwhere your table is ‘your_table.tex’. The tex file can be compiled in the Terminal by typing pdflatex main.tex."
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#version-control",
    "href": "Guides/03_External_Server/External_Server_Guide.html#version-control",
    "title": "External Server Guide",
    "section": "8.6 Version control",
    "text": "8.6 Version control\nThe server runs GitLab. If you need to use Git for your projects, please send your request to BPLIM (bplim@bportugal.pt).\nIn case you want to use Git you should request it from BPLIM (bplim@bportugal.pt).\n\nWikipedia:\n``Git is a distributed version-control system for tracking changes in any set of files, originally designed for coordinating work among programmers cooperating on source code during software development. Its goals include speed, data integrity, and support for distributed, non-linear workflows’’\n\nFirst steps\n\nFirst, authenticate using an ssh-key. Open a Terminal in your home folder\n\n  cd ~\nand type:\n  ssh-keygen -t rsa -C \"BPLIM git\"\n  cat ~/.ssh/id_rsa.pub\n\nSecond, after generating your SSH key, you’ll need to select the text of the key in your terminal. You can usually do this by clicking and dragging your mouse over the key text. Once the key is highlighted, right-click on the selection and choose ‘Copy’ to copy the resulting key to your clipboard.\nThird, open Firefox (go to the RedHat icon and type Firefox in the search box) and navigate to https://vxpp-bplimgit.bplim.local/\n\n\nOnce you’ve navigated to the website, use your credentials for the external server to log in.\n\nOnce logged in, navigate to your profile. In the upper-right corner of the webpage, you will find the Settings option.\n\n\nNow, on the left-side bar, click in SSH Keys\n\nand paste the contents of the clipboard in the text box on the top right corner under Key.\n\nGive a title, e.g., “BPLIM git”, and click in Add key.\n\nGo to Projects\n\n\nand create a New project, e.g., scripts_P999, where P999 is your_project_ID_number\n\nTo use git, it is necessary to modify or create the .gitconfig file in your user’s home directory. You can use KWrite (click in the RedHat icon and search for KWrite) to edit/create the file. The file should have the following format. In this file, you can adapt the name and replace ‘investa’ with your own user.\n  [cola]\n          spellcheck = false  \n  [user]\n          name = Investigador A\n          email = investa@sxpe-bplim01.bplim.local\n  [gui]\n          editor = kwrite\n\nYou can clone the project by opening a Terminal and moving to your work_area:\n\n  cd /bplimext/projects/your_project_ID/work_area/\nand typing\n  git clone git@vxpp-bplimgit.bplim.local:investa/scripts_P999.git\n\nAdd the file .gitignore available in folder tools of your project:\n\n  cd scripts_P999\n  cp /bplimext/projects/your_project_ID/tools/.gitignore .\n\nDo your first commit & push\n\n  git add *\n\n  git commit -a -m \"First\"\n\n  git push\n\nTo use the version control system effectively, please place all your scripts and code files in the folder named ‘scripts_P999’. This organization is important for maintaining a structured and efficient workflow with the version control system."
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#containers",
    "href": "Guides/03_External_Server/External_Server_Guide.html#containers",
    "title": "External Server Guide",
    "section": "8.7 Containers",
    "text": "8.7 Containers\n\n\n\n8.7.1 Build your container\n\nYou can write a script to build your container using the template definition files available at our GitHub repository\nTest your script and build the container using SylabsCloud (you can use your GitHub account to login)\nClick in ‘CREATE’\n\n\n\nIn the following step upload your ‘.def’ file or copy/paste its contents in the Text box:\n\n\n\nSylabs runs a first test on the validity of your script and releases the button ‘Build’ (click on it)\nFollow the outcome at the bottom of the screen and check for possible error messages\nOnce you succeed in building the container, you can send us the definition file with your changes\n\n\n\n8.7.2 Use the container in BPLIM’s server\n\nOpen a Terminal\nMove to your project’s folder\n\n\ncd /bplimext/projects/YOURPROJECTID/tools/containers\n\n\nStart the container by typing\n\n\nsingularity shell YOURPROJECTID.sif\n\n\nThe prompt of the Terminal will show: Singularity\nStart RStudio by typping rstudio (small caps)\n\n\n\nOnce inside RStudio you have access to the original folder structure of your project"
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#jupyter-lab",
    "href": "Guides/03_External_Server/External_Server_Guide.html#jupyter-lab",
    "title": "External Server Guide",
    "section": "8.8 Jupyter Lab",
    "text": "8.8 Jupyter Lab\nExplore Jupyter lab:\n\n“JupyterLab is a web-based interactive development environment for Jupyter notebooks, code, and data. JupyterLab is flexible: configure and arrange the user interface to support a wide range of workflows in data science, scientific computing, and machine learning. JupyterLab is extensible and modular: write plugins that add new components and integrate with existing ones.”\n\nStart Jupyter Lab by typing:\n\njupyter lab --browser=firefox\n\nSample session:"
  },
  {
    "objectID": "Guides/03_External_Server/readme.html",
    "href": "Guides/03_External_Server/readme.html",
    "title": "BPLIM",
    "section": "",
    "text": "External Server Guide\nThis folder contains the User Guide for BPLIM’s External Server, specifically designed for External Researchers. This guide provides detailed information on all procedures related to accessing and using the External Server."
  },
  {
    "objectID": "Guides/07_Working_with_Pseudo-Data/pseudo_data_guide.html#data-structure-and-links",
    "href": "Guides/07_Working_with_Pseudo-Data/pseudo_data_guide.html#data-structure-and-links",
    "title": "How to Work with Pseudo-Data",
    "section": "1.1 Data structure and links",
    "text": "1.1 Data structure and links\nTo prepare the pseudo-data BPLIM must work with the researchers to identify all original datasets to use in the project along with the linking variables. For each original dataset, BPLIM will create a metafile and a file with a sample of the id (linking) variables. These files are needed to create the pseudo-data. As mentioned in the article, BPLIM does not provide the actual pseudo-data, but instead provides a set of Stata do-files that will generate the pseudo-data."
  },
  {
    "objectID": "Guides/07_Working_with_Pseudo-Data/pseudo_data_guide.html#the-package",
    "href": "Guides/07_Working_with_Pseudo-Data/pseudo_data_guide.html#the-package",
    "title": "How to Work with Pseudo-Data",
    "section": "1.2 The package",
    "text": "1.2 The package\nBPLIM will prepare and send to the external researchers a compressed file with all the necessary files. For illustrative purposes, let’s assume that the name of the project is pxxx_BPLIM and that the researcher identified two original files to work with:“CB_2006” and “CB_2007”. The researcher will receive a zipped file, for example, pxxx_BPLIM_pseudo.zip. The researcher only has to unzip that file. This will create the following directory/file structure:\n.../package\n│\n├── ados/\n│\n├── dos/\n│   ├── generate_dummy_dos.do\n│   ├── master.do \n│   ├── CB_2006_dummy.do\n│   └── ...\n│   \n├── ids/\n│   ├── CB_2006_ID.dta\n│   ├── CB_2007_ID.dta\n│   └── ...\n│   \n├── metadata/\n│   ├── CB_2006_meta.xlsx\n│   ├── CB_2007_meta.xlsx\n│   └── ...\n│\n└── pxxx_BPLIM/\n    └── ...\nAll the files in the first four directories - ados, dos, ids, and metadata - are needed to generate the pseudo datasets. The directory pxxx_BPLIM is the project folder, where the researcher is supposed to develop his/her work. Below we will further detail the structure of the project folder."
  },
  {
    "objectID": "Guides/07_Working_with_Pseudo-Data/pseudo_data_guide.html#creating-the-pseudo-data",
    "href": "Guides/07_Working_with_Pseudo-Data/pseudo_data_guide.html#creating-the-pseudo-data",
    "title": "How to Work with Pseudo-Data",
    "section": "1.3 Creating the pseudo data",
    "text": "1.3 Creating the pseudo data\nAfter unzipping the file, the researcher should open the file master.do in Stata (located in the dos folder). Please make sure that the Stata working directory is …/package/dos (where the master.do file is located) because we use relative paths in this script to reference other necessary files. Running master.do will create the pseudo datasets and place them in the project directory pxxx_BPLIM under the folder initial_dataset. The project directory has the following structure:\n.../package/pxxx_BPLIM/\n│\n├── initial_dataset/\n│   ├── CB_D_2006.dta\n│   ├── CB_D_2007.dta\n│   └── ...\n│\n├── results/\n│   └── ...\n│   \n├── tools/\n│   └── ...\n│\n└── work_area\n    ├── profile.do\n    └── template.do\nwhere we show the pseudo-data files that were created. These files will contain “_D_” in their names to reflect the fact that they are “dummy” data. After creating the pseudo data, you may relocate the project directory wherever you wish. However, the structure of the project directory must remain the same, because it mirrors the structure that BPLIM (or an internal user) has to replicate the code using original data. You should not copy any additional data files to the folder initial_dataset."
  },
  {
    "objectID": "Guides/07_Working_with_Pseudo-Data/pseudo_data_guide.html#setting-up-the-project",
    "href": "Guides/07_Working_with_Pseudo-Data/pseudo_data_guide.html#setting-up-the-project",
    "title": "How to Work with Pseudo-Data",
    "section": "1.4 Setting up the project",
    "text": "1.4 Setting up the project\n\n1.4.1 Modifying the profile.do\nAfter creating the pseudo data, the researcher is ready to start coding. The researcher should place all scripts in the work_area directory. In this area, the researcher is free to organize the code as it pleases. However, you may have noticed that this folder already contains two files: “template.do” and “profile.do”. As the name suggests, template.do is a template that you should use to prepare your scripts. But before you can start coding you must edit profile.do. This do-file sets all the configurations (globals, paths, etc.) and must be placed in the same directory as the script file that you are executing. Stata will run this file first automatically. For example, suppose that you decide to place your project directory under the folder C:/Users/Jane/. In that case you edit profile.do and adjust the global root_path, to “C:/Users/Jane/pxxx_BPLIM”, the path to the project directory.\n*********************************************************\n*            Initialization                              \n*********************************************************\nversion 18\nclear all\nprogram drop _all\nset more off\nset rmsg on\nset matsize 10000\nset linesize 255\ncapture log close\n*********************************************************\n*               Define globals                          *\n********************************************************* \n**** Path for replication ****\n* Root path\nglobal root_path \"C:/Users/Jane/pxxx_BPLIM\"    /* changed here  */\n* Base path for replications\nglobal path_rep \"${root_path}/work_area\"\n\n**** Paths for data ****\n* Set the path for non perturbed data source\nglobal path_source \"${root_path}/initial_dataset\"\n\n**** Globals for type of modified dataset\n* Dummy \nglobal M1 \"D\"\n/*********************************************************************\nExample: pseudo data \n\nuse \"${path_source}/SLB_${M1}_YBNK_20102018_OCT20_QA1_V01.dta\"\n*********************************************************************/\n\n**** Path for project specific ado files ****\nadopath ++ \"${root_path}/tools\"\n\n* Change ados path\nsysdir set PLUS \"${root_path}/tools\"\nlocal places \"SITE PERSONAL OLDPLACE\"\nforeach p of local places {\n  capture adopath - `p'\n}\nNote that the globals path_rep and path_source are built based on root_path. Global M1 is used to reference the type of dataset being used. This means that when writing your scripts you must always use the global M1 when referring to the datasets. For example, when reading the data do not write:\nuse \"${path_source}/CB_D_2006.dta\"\nbut instead use the global in the name of the file:\nuse \"${path_source}/CB_${M1}_2006.dta\"\n\n\n1.4.2 Installing tools for your project\nAll the external user-written ados must be installed in the tools directory of your project. We recommend using adoinstall for this purpose. Fo example, to install reghdfe in the tools directory, you need to type the following in Stata:\nssc install adoinstall\nadoinstall reghdfe, to(\"C:/Users/Jane/package/pxxx_BPLIM/tools\")\nThe scripts that you prepare should only use user-written commands that are placed in the tools directory of your project."
  },
  {
    "objectID": "Guides/07_Working_with_Pseudo-Data/pseudo_data_guide.html#preparing-your-code",
    "href": "Guides/07_Working_with_Pseudo-Data/pseudo_data_guide.html#preparing-your-code",
    "title": "How to Work with Pseudo-Data",
    "section": "1.5 Preparing your code",
    "text": "1.5 Preparing your code\nIt is really important to follow the above guidelines to ensure that results can be safely replicated on the original data. If the code is well organized and follows the guidelines then BPLIM staff (or an internal co-author) can easily change the configuration file profile.do to rerun the analysis on the original data. The template file that we provide shows examples on how the researcher should code, based on the settings defined in profile.do. Below we show the contents of template.do:\n* Project      :  pxxx_BPLIM\n* Author(s)    :\n* Date         :\n* Description  :\n* Dependencies :\n* Modifications: (add date, author and change)\n\n* Run profile (usually not needed, but just to be sure)\ncapture run \"profile.do\"\n\n* Change to work path - global `path_rep` defined in profile.do\ncd \"${path_rep}\"\n\n/* You may create a `results` folder inside `path_rep` to save outputs (this \nis optional since there is already a `results` folder outside `path_rep`)\nAlways use capture when creating directories in scripts*/\ncapture mkdir results\n* You may create the structure that you want, adding sub-diectories to `results`\ncapture mkdir results/tables\ncapture mkdir results/figures\n\n/*\nWhen defining globals for paths (if you do not want to use relative paths), remember to\ninclude the global `path_rep`. This is the path where the analysis should run. See the\ntwo examples below, where we define two globals for separate results folders\n*/\nglobal results_tables \"${path_rep}/results/tables\"\nglobal results_figures \"${path_rep}/results/figures\"\n\n* Creating a log file in the work area, where \"logexample\" is the log requested for extraction\nlog using \"logexample.log\", replace\n\n*********************************************************\n*                  Open data files                      *\n*********************************************************\n/*\nPlease note the VERY IMPORTANT use of global `M1`, ${M1}, \nin the file names of the modified data. Failing to use the \nglobals when working with modified data will cause the \nREPLICATION TO FAIL.\n*/\n\n* Example on how to read a dummy data file provided by BPLIM:\nuse \"${path_source}/CB_${M1}_2006\", clear\n\n*********************************************************\n*            Start data analysis                        *\n*********************************************************\n*\n* ...\n* YOUR STATA CODE GOES HERE\n* ...\n* You may call other do-files, just be sure to use the\n* globals defined in the profile and eventually in this\n* file\n\n*********************************************************\n*            Saving the results                             *\n*********************************************************\n\n* Saving an intermediate dataset. You may save it in your\n* work area, if you wish to preserve it in order to analyze\n* it later. However, in a replication, if these datasets are\n* not needed, you can delete them. As an example, see below:\n\n* Create intermediate data directory\ncap mkdir intermediate_data\n\n* Add global for directory\nglobal path_data \"${path_rep}/intermediate_data\"\n\n* Save datasets\ncompress\nsave \"${path_data}/filename1.dta\", replace\nsave \"${path_data}/filename2.dta\", replace\n\n* Remove intermediate data after analysis is finished\nlocal files: dir \"${path_data}\" files \"*.dta\", respectcase\nforeach file of local files {\n  rm \"${path_data}/`file'\"\n}\n\n****** Results Examples ******\n* Creating a graph and saving to the results area (figures)\ngraph export \"${results_figures}/mygraph.png\", replace\n* Creating a table and saving to the results area (tables)\nesttab using \"${results_tables}/myfile.tex\", cells(\"cell1 cell2 ...\")\n\n*********************************************************\n*                  Close the log file                   *\n*********************************************************\nlog close\n\n***************** IMPORTANT *****************\n\n* BPLIM reserves the right to decline to send log files that are \n* exceptionally large. For really long scripts, remember that not\n* every line of code and its output needs to be in the log file.\n* Take the following example:\n/*\nlog using \"mylog.log\", replace\n\nquietly {\n  sysuse auto, clear\n  summarize price\n  noisily display \"Mean Price: `r(mean)'\"\n  drop if foreign == 1\n  keep price mpg rep78\n  noisily reg price mpg i.rep78\n}\n\nlog close\n*/\n\n* Only the outputs of the third and sixth lines will appear in the\n* log file. This is a good strategy to follow if your log files are \n* too cluttered with code and output, which are only intermediate steps\n* to final outputs\nNotice the first lines of the template, where we run the configuration file - capture run \"profile.do\" - and change directory to the work area folder - cd \"${path_rep}\". Note that global path_rep is defined in the configuration file. We also create new folders in our working directory and set globals in order to reference them later:\ncapture mkdir results\n* You may create the structure that you want, adding sub-diectories to `results`\ncapture mkdir results/tables\ncapture mkdir results/figures\n...\nglobal results_tables \"${path_rep}/results/tables\"\nglobal results_figures \"${path_rep}/results/figures\"\nThe remaining lines of the template contain standard examples of how to generate outputs and organize your code.\n\n1.5.1 Create a master script\nResearchers should create a master script that runs their analysis from top to bottom. This file should be based on template.do and must create a log file that proves that the code ran without errors. Still, researchers are free to organize code in the work_area as they please. For example, the following structure would be acceptable:\n.../package/pxxx_BPLIM/work_area/\n│\n├── profile.do\n├── master.do\n│\n├── 01_data_management/\n│   ├── 01_data_manipulation.do\n│   └── ...\n│\n├── 02_exploratory/\n│   ├── 01_tables.do\n│   ├── 02_figures.do\n│   └── ...\n│\n└── 03_regressions/\n    ├── 01_regressions.do\n    └── ...\nThen, within master.do, the researcher only has to run the dependencies:\n...\ncd ${path_rep}\n...\n\ndo 01_data_management/01_data_manipulation.do\n\ndo 02_exploratory/01_tables.do\ndo 02_exploratory/02_figures.do\n\ndo 03_regressions/01_regressions.do\n\nlog close"
  },
  {
    "objectID": "Guides/07_Working_with_Pseudo-Data/pseudo_data_guide.html#creating-the-replicability-package",
    "href": "Guides/07_Working_with_Pseudo-Data/pseudo_data_guide.html#creating-the-replicability-package",
    "title": "How to Work with Pseudo-Data",
    "section": "1.6 Creating the replicability package",
    "text": "1.6 Creating the replicability package\nAfter completing your analysis, you must prepare your replication package to send to BPLIM. With that package, BPLIM wil be able to replicate your results using the original data.\nPreparing the package is a simple procedure. All you need to do is run ado archive_rep, which will zip all the necessary files for the replication. It also generates a list of all the files used and creates a requirements file with all the dependencies (ados). Please note that only script files (ados, dos, etc) are copied to the archive. In this case, we would run the following in Stata:\nadopath + \"C:/Users/Jane/pxxx_BPLIM/tools\"\narchive_rep, rep(1) path(\"C:/Users/Jane/pxxx_BPLIM\")\nThe output of the command is a folder named Rep001 with the folders and files (scripts) needed for the replication. This folder, and the zip file Rep001.zip (zip file of the folder) are created in the work_area directory. The researcher only has to send the zip file to BPLIM."
  },
  {
    "objectID": "Guides/07_Working_with_Pseudo-Data/pseudo_data_guide.html#recap",
    "href": "Guides/07_Working_with_Pseudo-Data/pseudo_data_guide.html#recap",
    "title": "How to Work with Pseudo-Data",
    "section": "1.7 Recap",
    "text": "1.7 Recap\nIt is important that the researcher follows the guidelines and the proper workflow:\n\nResearcher identifies the datasets and tools for the project\nBPLIM staff prepares the package and sends it to the researcher in a zip file\nResearcher unpacks the file and creates the project structure\nResearcher runs the do-file to create the pseudo data in the project initial dataset folder\nResearcher adapts profile.do, namely the global root_path, so that it points to the correct directory in her computer\nResearcher creates a master.do file based on template.do (it can be copied) and organizes the code as she desires\nAfter the analysis is finished, the researcher runs the ado archive_rep and sends the output zip file to BPLIM. Researchers should clearly indicate the output files needed for the analysis.\nBPLIM checks that the analysis runs from top to bottom. In case that it does, the staff will run the analysis on the original data\nBPLIM checks the output files requested by the researchers and emails the results if they respect the output control rules.\n\nImportant note:\n\nIntermediate datasets must be created during the analysis. Remember that the researcher is only allowed to send scripts and logs for replications, so BPLIM only has access to datasets in the initial_dataset folder. Trying to reference intermediate data that is not created during the analysis will cause the replication to fail."
  },
  {
    "objectID": "Guides/07_Working_with_Pseudo-Data/readme.html",
    "href": "Guides/07_Working_with_Pseudo-Data/readme.html",
    "title": "BPLIM",
    "section": "",
    "text": "Working with Pseudo-Data\nThis folder contains the Guide to Working with Pseudo-Data, which provides detailed guidelines for researchers on how to work with pseudo-data prepared by BPLIM."
  },
  {
    "objectID": "Guides/02_BPLIM_Datasets_Guide/readme.html",
    "href": "Guides/02_BPLIM_Datasets_Guide/readme.html",
    "title": "BPLIM",
    "section": "",
    "text": "BPLIM Datasets Guide\nThis folder contains the BPLIM Datasets Guide, which provides a detailed overview of the datasets available at BPLIM. This guide is designed to complement the information found in the Guide for Researchers."
  },
  {
    "objectID": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v102024.html#central-balance-sheet-database-cb",
    "href": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v102024.html#central-balance-sheet-database-cb",
    "title": "BPLIM Datasets Guide",
    "section": "2.1 Central Balance Sheet Database (CB)",
    "text": "2.1 Central Balance Sheet Database (CB)\nThe Central Balance Sheet Database is constructed and made available by BdP and provides economic and financial information on non-financial corporations operating in Portugal. It contains annual data from 2006 onwards and is mostly based on information reported through Informação Empresarial Simplificada (IES, Simplified Corporate Information).\nUntil 2009, the data were reported according to the Plano Oficial de Contabilidade (POC, Official Chart of Accounts). In 2010, a new accounting system, the Sistema de Normalização Contabilística (SNC, Accounting Standards System) was put in place. Thus, all data reported after 2009 follow the SNC. This is a structural change that is reflected in the way the dataset is organized and poses some challenges in the comparability of the variables over time.\nBPLIM provides two different products: the annual firm-level set (CB) and the firm harmonized panel (CBHP). CB contains the data as reported while CBHP contains variables that were made consistent over time. Besides these two products, any variable available at IES can be added on demand.\nFor more information on this dataset, please refer to the Manuals and Auxiliary Documentation (metadata files, variable descriptions) available in the BPLIM Github Repository - CB. \n\n\n\nCentral Balance Sheet Available Products\n\n\n\nThere are two variables that, if requested by an external researcher, will be anonymized at Level 2."
  },
  {
    "objectID": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v102024.html#fast-and-exceptional-enterprise-survey---covid-19-covid-iree",
    "href": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v102024.html#fast-and-exceptional-enterprise-survey---covid-19-covid-iree",
    "title": "BPLIM Datasets Guide",
    "section": "2.2 Fast and Exceptional Enterprise Survey - COVID-19 (COVID-IREE)",
    "text": "2.2 Fast and Exceptional Enterprise Survey - COVID-19 (COVID-IREE)\nThe Fast and Exceptional Enterprise Survey - COVID-19 (COVID-IREE) was launched by Statistics Portugal (INE) and the BdP aiming to identify the main effects of the COVID-19 pandemic on key aspects of the enterprises activity, such as firm’s turnover, workforce, prices, credit conditions and the use of Government support measures. This survey was addressed to a representative sample of non-financial firms located in Portugal. The data started being collected with a weekly frequency on April 6-10, 2020 and refers to a fortnight from May 2020 onwards. Although the survey was suspended after July 2020, two new editions were implemented, in November 2020 and in the first fortnight of February 2021 given the evolution of the pandemic situation.\nFor more information on this dataset, please refer to the Manuals and Auxiliary Documentation (metadata files, variable descriptions) available in the BPLIM Github Repository - IREE. \n\n\n\nCOVID-IREE Available Products"
  },
  {
    "objectID": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v102024.html#the-incentives-systems-si",
    "href": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v102024.html#the-incentives-systems-si",
    "title": "BPLIM Datasets Guide",
    "section": "2.3 The Incentives Systems (SI)",
    "text": "2.3 The Incentives Systems (SI)\nThe Incentives Systems Database (SI) includes information for projects submitted to the incentives systems funded by the European Regional Development Fund (ERDF) within the QREN (2007-2013) framework and the European Regional Development Fund and European Social Fund (ESF) within the PT2020 (2014-2020) framework. The data include information on both approved and non-approved applications. The data made available by BPLIM corresponds to a data freeze at a certain moment. The QREN data will no longer be subject to updates and corresponds to a freeze that occurred on September 2017. The PT2020 data is updated annually.\nFor more information on this dataset, please refer to the Manuals and Auxiliary Documentation (metadata files, variable descriptions) available in the BPLIM Github Repository - SI. \n\n\n\nIncentives Systems Available Products"
  },
  {
    "objectID": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v102024.html#entreprise-groups-ge",
    "href": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v102024.html#entreprise-groups-ge",
    "title": "BPLIM Datasets Guide",
    "section": "2.4 Entreprise Groups (GE)",
    "text": "2.4 Entreprise Groups (GE)\nThe Enterprise Groups Database (GE) is constructed and made available by BdP and provides information on the participations in equity capital and voting rights of non-financial corporations operating in Portugal. This dataset contains annual data from 2014 onwards and is based on the information reported through Informação Empresarial Simplificada (IES, Simplified Corporate Information).\nFor more information on this dataset, please refer to the Manuals and Auxiliary Documentation (metadata files, variable descriptions) available in the BPLIM Github Repository - GE. \n\n\n\nEntreprise Groups Available Products"
  },
  {
    "objectID": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v102024.html#spanish-and-portuguese-companies-microdata-ibach",
    "href": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v102024.html#spanish-and-portuguese-companies-microdata-ibach",
    "title": "BPLIM Datasets Guide",
    "section": "2.5 Spanish and Portuguese Companies Microdata (iBACH)",
    "text": "2.5 Spanish and Portuguese Companies Microdata (iBACH)\nThe Spanish and Portuguese Companies Microdata (iBACH) contains economic and financial granular information on non-financial Spanish and Portuguese corporations from iBACH. This dataset derives from BACH dataset. BACH is a database of aggregated and harmonized accounting data of non-financial companies, based on national accounting standards (individual annual accounts).\nThis dataset is also made available by the Data Laboratory of the Banco de España - BELab.\nFor more information on this dataset, please refer to the Manuals and Auxiliary Documentation (metadata files, variable descriptions) available in the BPLIM Github Repository - iBACH. \n\n\n\niBACH Available Products"
  },
  {
    "objectID": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v102024.html#central-credit-register-database-crc",
    "href": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v102024.html#central-credit-register-database-crc",
    "title": "BPLIM Datasets Guide",
    "section": "2.6 Central Credit Register Database (CRC)",
    "text": "2.6 Central Credit Register Database (CRC)\nThe Central Credit Register (CRC) database reports credit supply by all credit-granting institutions in Portugal. Data is collected monthly with the objective of supporting participants in the risk assessment of credit concession. Until 2018, Banco de Portugal collected information at the credit exposure level and produced three datasets with different levels of granularity: exposure-level, firm-bank-level and firm-level data (this database is now known as the Old CRC). This database was updated annually until 2018. Old CRC underwent a major revision in September, 2018 and was replaced by a new reporting system.\nFor more information on this dataset, please refer to the Manuals and Auxiliary Documentation (metadata files, variable descriptions) available in the BPLIM Github Repository - CRC. \n\n\n\nCRC Available Products"
  },
  {
    "objectID": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v102024.html#harmonized-central-credit-register-hcrc",
    "href": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v102024.html#harmonized-central-credit-register-hcrc",
    "title": "BPLIM Datasets Guide",
    "section": "2.7 Harmonized Central Credit Register (HCRC)",
    "text": "2.7 Harmonized Central Credit Register (HCRC)\nThe Portuguese Central Credit Register (with the database prior to September 2018 known as the Old CRC) underwent a major revision in September, 2018 and was replaced by a new reporting system (known as the New CRC), which started to collect granular credit data at the instrument level.\nThe Harmonized Central Credit Register Database (HCRC) aims to build compatible series between the Old CRC and the New CRC by selecting a set of most relevant variables and adopting necessary steps to harmonize the data. The database is updated annually, covers the period from 2009 onwards and consists of information aggregated at the firm-level and bank-firm level. Data constructed at exposure level mimicking the data structure of the Old CRC is only available to internal researchers.\nFor more information on this dataset, please refer to the Manuals and Auxiliary Documentation (metadata files, variable descriptions) available in the BPLIM Github Repository - HCRC. \n\n\n\nHCRC Available Products"
  },
  {
    "objectID": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v102024.html#bank-balance-sheet-bbs",
    "href": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v102024.html#bank-balance-sheet-bbs",
    "title": "BPLIM Datasets Guide",
    "section": "2.8 Bank Balance Sheet (BBS)",
    "text": "2.8 Bank Balance Sheet (BBS)\nThe Monetary Financial Institutions (MFIs) Balance Sheet Database (BBS) reports, on a individual basis, detailed information on the assets and liabilities of the MFIs operating in Portugal. The dataset contains monthly data, from 1997 onwards and is updated annually.\nFor more information on this dataset, please refer to the Manuals and Auxiliary Documentation (metadata files, variable descriptions) available in the BPLIM Github Repository - BBS. \n\n\n\nBBS Available Products"
  },
  {
    "objectID": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v102024.html#historical-series-of-portuguese-banking-sector-slb",
    "href": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v102024.html#historical-series-of-portuguese-banking-sector-slb",
    "title": "BPLIM Datasets Guide",
    "section": "2.9 Historical Series of Portuguese Banking Sector (SLB)",
    "text": "2.9 Historical Series of Portuguese Banking Sector (SLB)\nThe Historical Series of the Portuguese Banking Sector Database (SLB) reports, on a consolidated basis, a wide range of series on bank’s financial statements (i.e., balance sheet, income statement, and solvency), loans to customers, interest rates, human resources, branch network, and payment systems. The data is collected and assembled by a working group at BdP, which was established at the end of 2017 with the objective of constructing historical series on the Portuguese banking sector.\nThe SLB database covers the period from 1990 onwards and is available at yearly frequency. For some tables (i.e., balance sheet, income statement, solvency, loans to customers, and interest rates), data is also available at quarterly frequency. This dataset is updated annually.\nFor more information on this dataset, please refer to the Manuals and Auxiliary Documentation (metadata files, variable descriptions) available in the BPLIM Github Repository - SLB. \n\n\n\nSLB Available Products"
  },
  {
    "objectID": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v102024.html#summary",
    "href": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v102024.html#summary",
    "title": "BPLIM Datasets Guide",
    "section": "2.10 Summary",
    "text": "2.10 Summary\n\n\n\nSummary of Datasets Available to External Researchers"
  },
  {
    "objectID": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v102024.html#central-credit-register-crc---households",
    "href": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v102024.html#central-credit-register-crc---households",
    "title": "BPLIM Datasets Guide",
    "section": "3.1 Central Credit Register (CRC) - Households",
    "text": "3.1 Central Credit Register (CRC) - Households\nThe Central Credit Register (CRC) database contains credit exposure data for both firms and households. Data on credit granted to households is only available for Internal Researchers, with no exceptions. This information is considered highly confidential due to its sensitive nature."
  },
  {
    "objectID": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v102024.html#quadros-de-pessoal-qp",
    "href": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v102024.html#quadros-de-pessoal-qp",
    "title": "BPLIM Datasets Guide",
    "section": "3.2 Quadros de Pessoal (QP)",
    "text": "3.2 Quadros de Pessoal (QP)\nQuadros de Pessoal (QP) dataset is available through a Protocol between BdP and the Ministry of Labour and Social Solidarity. The QP dataset includes all public and private firms with at least one employee, providing annual data on firms, establishments, and their workers.\nIt offers comprehensive details on worker characteristics and covers the period from 1982 onwards. BPLIM has data available up to 2013.\nThis dataset is only available for Internal Researchers."
  },
  {
    "objectID": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v102024.html#supermarket-daily-prices-sdp-and-matched-supermarket-daily-prices-msdp",
    "href": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v102024.html#supermarket-daily-prices-sdp-and-matched-supermarket-daily-prices-msdp",
    "title": "BPLIM Datasets Guide",
    "section": "3.3 Supermarket Daily Prices (SDP) and Matched Supermarket Daily Prices (MSDP)",
    "text": "3.3 Supermarket Daily Prices (SDP) and Matched Supermarket Daily Prices (MSDP)\nThe Supermarket Daily Prices (SDP) database provides daily data on the prices of food and beverage products sold in six supermarkets in Portugal: Auchan, Continente, Minipreço, Pingo Doce, Froiz, and Supercor. This information is webscraped from their online stores by BPLIM.\nUsing this data, BPLIM created the Matched Supermarket Daily Prices (MSDP), which offers daily data on the prices of food and beverage products sold across multiple supermarkets included in the SDP database. This enables price comparisons across different supermarkets.\nBoth the SDP and MSDP datsets cover the period from 2021 to 2023 and are updated annually for research purposes.\nThis data can only be used for research projects involving Internal Researchers."
  },
  {
    "objectID": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v102024.html#balance-of-payments-bop",
    "href": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v102024.html#balance-of-payments-bop",
    "title": "BPLIM Datasets Guide",
    "section": "4.1 Balance of Payments (BoP)",
    "text": "4.1 Balance of Payments (BoP)\nThe Portuguese Balance of Payments records the transactions that take place between residents and non-residents in Portugal over a given period of time, typically a month, quarter, or year. Transactions are classified in the balance of payments according to the nature of the underlying economic resources. There are three main categories: current account, capital account, and financial account.\nThe current account includes transactions between residents and non-residents involving produced assets (goods and services), primary income (income from the use of factors of production such as capital and labor), and secondary income (current transfers)."
  },
  {
    "objectID": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v102024.html#statistical-central-credit-register-scrc",
    "href": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v102024.html#statistical-central-credit-register-scrc",
    "title": "BPLIM Datasets Guide",
    "section": "4.2 Statistical Central Credit Register (SCRC)",
    "text": "4.2 Statistical Central Credit Register (SCRC)\nThe Portuguese Central Credit Register (with the database prior to August 2018 known as the Old CRC) underwent a major revision in September, 2018 and was replaced by a new reporting system (i.e. NEWCRC) which started to collect granular credit data on a loan-by-loan basis.\nTo facilitate and harmonize the use of credit information, the Statistical Central Credit Register (Statistical CRC) was created. This database contains credit information at the micro level, where information reported to the CRC is made available after statistical data processing processes. These processes include the correction of flaws identified in the reported information and the calculation of additional variables following harmonized algorithms.\nAs a result, the Statistical CRC guarantees the existence of a common base of granular information on credit so that analyses made from this database are consistent with each other.\nThe data will be available at the firm level, firm-bank level, and instrument level."
  },
  {
    "objectID": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v102024.html#e-fatura",
    "href": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v102024.html#e-fatura",
    "title": "BPLIM Datasets Guide",
    "section": "4.3 E-Fatura",
    "text": "4.3 E-Fatura\nEFATURA data is received monthly under a Protocol signed between BdP and INE. This database contains information regarding invoices issued by entities that have their headquarters or permanent establishment in the national territory. The information is aggregated by the tax identification number (NIF) of the issuer (only for legal persons) and the acquirer.\nINE implements some data cleaning, like imputation of missings or outliers, before sharing the data with BdP.\nThis data will only be available to Internal Researchers."
  },
  {
    "objectID": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v102024.html#social-security-ss",
    "href": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v102024.html#social-security-ss",
    "title": "BPLIM Datasets Guide",
    "section": "4.4 Social Security (SS)",
    "text": "4.4 Social Security (SS)\nSocial Security Data (SS) is received every month under a Protocol signed between BdP and Social Security Institute and may only be used by Internal Researchers.\nIt provides monthly information on the classification of the individual for contribution purposes, wage registers, Social Security transfers, and pensions since January 2010. Information on pensions is also available since January 2017.\nThis data will only be available to Internal Researchers."
  },
  {
    "objectID": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v102024.html#institute-for-employment-and-vocational-training-database-iefp",
    "href": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v102024.html#institute-for-employment-and-vocational-training-database-iefp",
    "title": "BPLIM Datasets Guide",
    "section": "4.5 Institute for Employment and Vocational Training Database (IEFP)",
    "text": "4.5 Institute for Employment and Vocational Training Database (IEFP)\nIEFP data encompasses all job applications, job offers, applications to job offers, interventions, and calls managed by the Institute for Employment and Vocational Training’s information system.\nThis dataset tracks individuals who register with IEFP for job searching purposes, detailing their interventions, calls received from job centers, and job applications submitted. It also provides insights into the outcomes of these interventions, calls, and applications.\nThis data will only be available to Internal Researchers."
  },
  {
    "objectID": "Guides/readme.html",
    "href": "Guides/readme.html",
    "title": "BPLIM",
    "section": "",
    "text": "BPLIM Guides\n\n01_Guide for Researchers\n02_BPLIM Datasets Guide\n03_External Server Guide\n04_How to use Git on External Server\n05_How to use Containers on External Server\n06_Rules for Output Control\n07_How to work with Pseudo-Data"
  },
  {
    "objectID": "Guides/04_How_to_use_Git/readme.html",
    "href": "Guides/04_How_to_use_Git/readme.html",
    "title": "BPLIM",
    "section": "",
    "text": "How to use Git on External Server\nThis folder contains the User Guide to use Git on BPLIM External Server, offering comprehensive instructions on how to effectively use Git on BPLIM’s External Server."
  },
  {
    "objectID": "Guides/04_How_to_use_Git/Guide_Git.html",
    "href": "Guides/04_How_to_use_Git/Guide_Git.html",
    "title": "Using Git",
    "section": "",
    "text": "Whenever a user wants to use Git on the external server, it is necessary to add their project to the internal GIT server. This procedure is carried out by DSI.\nPlease run the following test:\n1. Login to the external server, Config file\nTo use git, it is necessary to modify or create the .gitconfig file in your user’s home directory. You can use KWrite to edit/create the file. The file should have the following format and should be created for each user who has access to GitLab. In this file, you can adapt the name and replace ‘investa’ with your own user.\n[cola]\n      spellcheck = false\n      \n[user]\n      name = Investigador A\n      \n      email = investa@sxpe-bplim01.bplim.local\n      \n[gui]\n      editor = kwrite\n2. Authenticate by ssh-key. Open a Terminal in your home folder and type:\n- `ssh-keygen -t rsa -C “BPLIM git”`\n\n- `cat ~/.ssh/id_rsa.pub`\n3. Copy the resulting key to the clipboard\n4. Open Firefox and navigate to https://vxpp-bplimgit.bplim.local/\n\n\nConfirm that you have a secure connection and use your credentials for the external server to login.\n\n5. In your profile go to settings and on the left-side bar click in SSH Keys and paste the contents of the clipboard in the text box on the top right corner under “Key”\n\n\n\n\nGive a title, e.g., “BPLIM git”, and click in “Add key”\n\n6. Go to Projects and create a New project, e.g., myfirst\n\n\n7. Now you can clone the project\n\nOpen a Terminal in your work_area and type\ngit clone git@vxpp-bplimgit.bplim.local:investa/myfirst.git\n\nYou have now a new folder corresponding to your project:\n\n8. You should now create a .gitignore file following the instructions available at https://git-scm.com/docs/gitignore\n9. You are now ready to work with Git on your project\nYou can find here a Git tutorial\nhttps://git-scm.com/docs/gittutorial"
  },
  {
    "objectID": "Guides/05_How_to_use_Containers/readme.html",
    "href": "Guides/05_How_to_use_Containers/readme.html",
    "title": "BPLIM",
    "section": "",
    "text": "How to use Containers\nThis folder contains the User Guide to use Containers on BPLIM External Server, offering comprehensive instructions on how to effectively use Containers on BPLIM’s External Server."
  },
  {
    "objectID": "Guides/05_How_to_use_Containers/Guide_Containers.html#alternative-a-use-a-pre-defined-setup",
    "href": "Guides/05_How_to_use_Containers/Guide_Containers.html#alternative-a-use-a-pre-defined-setup",
    "title": "Using Containers",
    "section": "2.1 Alternative A: use a pre-defined setup",
    "text": "2.1 Alternative A: use a pre-defined setup\nOnce you have logged in to the server, you will see an icon that opens the container for your project. To launch the container and start the predefined application that you specified in your request, simply double-click on the icon. Once you have completed this step, it will function similarly to any other application."
  },
  {
    "objectID": "Guides/05_How_to_use_Containers/Guide_Containers.html#alternative-b-use-the-command-line",
    "href": "Guides/05_How_to_use_Containers/Guide_Containers.html#alternative-b-use-the-command-line",
    "title": "Using Containers",
    "section": "2.2 Alternative B: use the command line",
    "text": "2.2 Alternative B: use the command line\nThis solution makes use of the Terminal, giving you more freedom to interact with the container. Follow the steps below:\n\nOpen a Terminal in your project’s work_area \n(e.g., /bplimext/projects/PROJECT_ID/work_area).\nThe container is located in your project’s tools folder \n(/bplimext/projects/PROJECT_ID/tools/_container).\nRun the container using the following command:\n\nsingularity shell ../tools/iPROJECT_ID.BPLIM_Python_R_Jupyter.sif\n\nInside the container, the prompt will change to Singularity >\n\n\n\n\n\nFor example, to open a Jupyter Notebook, type:\n\njupyter notebook\nIf you want to use Stata or RStudio, type xstata-mp or rstudio, respectively.\n\nWhen you’re finished using the container, type exit to leave the Singularity image."
  },
  {
    "objectID": "Guides/05_How_to_use_Containers/Guide_Containers.html#starting-point-the-container",
    "href": "Guides/05_How_to_use_Containers/Guide_Containers.html#starting-point-the-container",
    "title": "Using Containers",
    "section": "3.1 Starting point: the container",
    "text": "3.1 Starting point: the container\nA Singularity container is a way to package and distribute software and its dependencies in a portable and isolated environment. To build a Singularity container, you’ll need to have Singularity installed on your system.\nHere are the basic steps to build a container:\n\nCreate a recipe/definition file: This is a text file that contains the instructions for building the container. The recipe file should specify the base image to use, any additional software to install, and any environment variables to set.\nBuild the container: Use the singularity build command to build the container from the recipe file. For example:\n\nsingularity build mycontainer.sif recipe.def\n\nTest the container: Use the singularity shell command to enter the container and test that it runs correctly. For example:\n\nsingularity shell mycontainer.sif\n\n(optional) Publish your container to a public or private registry, like Singularity Hub or Singularity Container Library.\n\nIn the Appendix of this manual, you will find an example of a definition file for building a container with Stata. For detailed instructions and troubleshooting, you can refer to the official documentation at https://sylabs.io/guides/3.6/user-guide/index.html. Additionally, here is a simple recipe file that creates a container based on the Ubuntu 20.04 base image and installs the nano text editor:\n        Bootstrap: library\n        From: ubuntu:20.04\n\n        %post\n            apt-get update\n            apt-get install -y nano"
  },
  {
    "objectID": "Guides/05_How_to_use_Containers/Guide_Containers.html#the-definition-file-step-by-step",
    "href": "Guides/05_How_to_use_Containers/Guide_Containers.html#the-definition-file-step-by-step",
    "title": "Using Containers",
    "section": "3.2 The definition file step by step",
    "text": "3.2 The definition file step by step\nIn a Singularity definition file, you can write a variety of commands and instructions. Some common things you might include in the file are:\n\nPackage installation: You can use package managers like apt or yum to install software and dependencies that your container needs to run. For example, you might install a specific version of Python or a library that your application depends on.\nEnvironment setup: You can use commands like ENV or export to set environment variables that your container needs to run. For example, you might set the PATH variable to include the location of a specific binary or library.\nFile copy: You can use commands like COPY or ADD to copy files from the host system into the container. For example, you might copy a script or a configuration file that your application needs to run.\nRunscript: As previously stated, the Runscript is a script that is executed when the container is run. It typically contains commands to set up the environment and launch the application or process that the container is designed to run.\nLabels: You can include information about the container, such as the container’s name, version, and author.\nHelp: You can include a brief description of the container, which is useful for users who are trying to understand what the container does.\n\nIt’s worth noting that depending on the complexity of your container and the requirements of your application, you may need to include additional commands and instructions in your definition file.\n\n3.2.1 header\nIn a Singularity definition file, the “header” refers to the first section of the file that contains metadata and instructions for building the container. The header typically includes information such as the container’s name, version, and author, as well as instructions for obtaining and installing the software that the container is designed to run. The header also can include instructions for configuring the build environment, such as setting environment variables or installing dependencies. The header is usually written in a specific format (e.g. #!Singularity) and starts at the first line of the file.\n        #!Singularity\n        Bootstrap: library\n        From: ubuntu:20.04\n\n        %help\n        This container runs Stata.\n\n        %labels\n        AUTHOR BPLIM\n        VERSION v1.0\nThis header uses the Bootstrap: library to indicate that the container should be built using the Singularity Library, and uses the From: ubuntu:20.04 to indicate that the container is based on the Ubuntu 20.04 image. This header also includes a %help section that provides a brief description of the container, and a %labels section that includes information about the container’s author and version.\nIt’s worth noting that the way headers are written is not fixed and different instructions could be included.\n\n\n3.2.2 runscript\nThe “runscript” in a Singularity definition file is a script that is executed when the container is run. It typically contains commands to set up the environment and launch the application or process that the container is designed to run. The runscript is executed by the Singularity runtime after the container is started, and it can be used to configure the container’s environment, set up the application, and launch the application or process.\nAn example:\n    %runscript\n        Rscript myscript.R\nIn case you want the container to execute a Stata .do file you can write something along the following lines:\n    %runscript\n        if [ $# -ne 1 ]; then\n            echo \"Please provide the main script\"\n            exit 1\n        fi\n        stata-mp -e do \"$1\"\n        if tail -1 \"$log\" | egrep \"^r\\([0-9]+\\);\"\n        then\n            exit 1\n        else\n            exit 0\n        fi\nTo launch Stata in graphical mode, your run script should look something like this:\n%runscript\n    xstata-mp\nIf your container is named container_name.sif, you can launch Stata inside the container using the Terminal by typing the following command:\n./container_name.sif\nIf you want to execute a particular file, you should type the following command in the Terminal:\n./container_name.sif MyDoFile.do\n\n\n3.2.3 files\nThe %files section in a Singularity definition file is used to specify files or directories that should be included in the container image when it is built. Here is an example of what you might include in the %files section:\n    %files\n    /path/to/myfile1.txt\n    /path/to/mydir1\n    /path/to/myfile2.sh\n    /path/to/mydir2\nThis would include the files myfile1.txt and myfile2.sh and the directories mydir1 and mydir2 in the container image, with the same paths.\nYou can also use wildcard to copy multiple files or directories, for example :\n    %files\n    /path/to/mydir/*\nThis would include all files and directories in the folder mydir in the container image.\nIt’s worth noting that the %files section is optional, you don’t need to include it in your definition file if you don’t need to add any additional files to your container. Also, the %files section is only useful when creating a new container, if you want to add files to an existing container you can use the singularity copy command.\n\n\n3.2.4 environment\nThe %environment section in a Singularity definition file is used to specify environment variables that should be set when the container is run. Here is an example of what you might include in the %environment section to build a container running R version 4.1.1:\n    %environment\n        export R_VERSION=4.1.1\n        export R_HOME=/usr/lib/R/$R_VERSION\n        export PATH=$PATH:$R_HOME/bin\nThis will set the environment variable R_VERSION to 4.1.1, the environment variable R_HOME to /usr/lib/R/4.1.1 and the environment variable PATH to include the R’s binary path \\$R_HOME/bin\nYou can use these environment variables in your %post or %runscript sections to install R 4.1.1 and run commands with the R version 4.1.1.\nThis will set the environment variable R_VERSION to 4.1.1, the environment variable R_HOME to /usr/lib/R/4.1.1 and the environment variable PATH to include the R’s binary path \\$R_HOME/bin\n    %post\n        apt-get update && apt-get install -y r-base=$R_VERSION\nIt’s worth noting that you can also use ENV instruction in place of export and you can use %environment section to set any environment variable you want, not only R.\n\n\n3.2.5 post\nThe %post section in a Singularity definition file is used to specify commands that should be run during the container build process, after the base image has been imported. Here is an example of what you might include in the %post section to build a container running R version 4.1.1 and the most recent version of RStudio Server:\n    %post\n        # update package lists and install R 4.1.1\n        apt-get update && apt-get install -y r-base=$R_VERSION\n        \n        # add RStudio repository and key\n        echo \"deb https://cran.rstudio.com/bin/linux/ubuntu \n        bionic-cran35/\" | tee -a /etc/apt/sources.list\n        apt-key adv --keyserver keyserver.ubuntu.com --recv-keys \n        E298A3A825C0D65DFD57CBB651716619E084DAB9\n        \n        # install RStudio Server\n        apt-get update && apt-get install -y rstudio-server\nThis will update the package lists, install R version 4.1.1 using the environment variable set before and add the RStudio repository and key to the container, then install RStudio Server on the container.\nIt’s worth noting that this example assumes that the container is based on Ubuntu 20.04 (codenamed ‘focal’), if you are using a different version or distribution you should adjust the package manager commands and repository URLs accordingly.\nAlso, the %post section is optional and you don’t need to include it in your definition file if you don’t need to run any additional command during the container building process.\nIf you also want to include Jupyter Notebook, the R kernel and R nbextensions in your container, you can add the following commands to the %post section of your Singularity definition file:\n    %post\n        # update package lists and install R 4.1.1\n        apt-get update && apt-get install -y r-base=$R_VERSION\n        \n        # add RStudio repository and key\n        echo \"deb https://cran.rstudio.com/bin/linux/ubuntu \n        bionic-cran35/\" | tee -a /etc/apt/sources.list\n        apt-key adv --keyserver keyserver.ubuntu.com --recv-keys\n        E298A3A825C0D65DFD57CBB651716619E084DAB9\n        \n        # install RStudio Server\n        apt-get update && apt-get install -y rstudio-server\n        \n        # install Jupyter\n        apt-get install -y jupyter-core\n        \n        # install R kernel for Jupyter\n        R -e \"install.packages(c('repr', 'IRdisplay', 'evaluate', \n        'crayon', 'pbdZMQ', 'devtools', 'uuid', 'digest'), \n        repos='https://cloud.r-project.org/')\"\n        R -e \"devtools::install_github('IRkernel/IRkernel')\"\n        R -e \"IRkernel::installspec(user = FALSE)\"\n        \n        \n        # install R nbextensions\n        R -e \"install.packages('devtools', repos = \n        'https://cloud.r-project.org/')\"\n        R -e \"devtools::install_github('randy3k/r-notebook')\"\n        \nThis will install Jupyter, the R kernel for Jupyter, and the R nbextensions for Jupyter Notebook.\nIt’s worth noting that these commands are installing R packages from CRAN and GitHub repositories, so you may want to check that these repositories are available on your system and that you have internet connection during the container build process.\nNote: you can install packages from binaries.\n\n\n3.2.6 Including additional packages\nTo include TinyTeX and machine learning packages in your container, you can add the following commands to the %post section of your Singularity definition file:\n    %post\n        # update package lists and install R 4.1.1\n        apt-get update && apt-get install -y r-base=$R_VERSION\n        \n        # add RStudio repository and key\n        echo \"deb https://cran.rstudio.com/bin/linux/ubuntu\n        bionic-cran35/\" | tee -a /etc/apt/sources.list\n        apt-key adv --keyserver keyserver.ubuntu.com --recv-keys\n        E298A3A825C0D65DFD57CBB651716619E084DAB9\n        \n        # install RStudio Server\n        apt-get update && apt-get install -y rstudio-server\n        \n        # install Jupyter\n        apt-get install -y jupyter-core\n        \n        # install R kernel for Jupyter\n        R -e \"install.packages(c('repr', 'IRdisplay', 'evaluate',\n        'crayon', 'pbdZMQ', 'devtools', 'uuid', 'digest'),\n        repos='https://cloud.r-project.org/')\"\n        R -e \"devtools::install_github('IRkernel/IRkernel')\"\n        R -e \"IRkernel::installspec(user = FALSE)\"\n        \n        # install R nbextensions\n        R -e \"install.packages('devtools', repos = \n        'https://cloud.r-project.org/')\"\n        R -e \"devtools::install_github('randy3k/r-notebook')\"\n        \n        # install TinyTeX\n        wget -qO- \"https://github.com/yihui/tinytex/raw/main/tools/inst\n        all-unx.sh\" | sh\n        \n        # install additional R packages\n        R -e \"install.packages(c('caret','randomForest','e1071','gbm','\n        xgboost', 'lightgbm','catboost','mlr','tidymodels','h2o','caret\n        Ensemble','pROC','ROCR',  \n        'pROC.plot','ROCR.plot','kernlab','pls','neuralnet','nnet'), \n        repos='https://cloud.r-project.org/')\"\n        \n        \nThis will install TinyTeX, which is a lightweight, cross-platform, and easy-to-maintain LaTeX distribution and also some popular machine learning packages such as caret, randomForest, e1071, gbm, xgboost, lightgbm, catboost, mlr, tidymodels, h2o, caretEnsemble, pROC, ROCR, pROC.plot, ROCR.plot, kernlab, pls, neuralnet, and nnet.\n\n\n3.2.7 Cleaning temporary files\nTo clean up temporary files during the build process of your container, you can add the following command to the %post section of your Singularity definition file:\n    %post\n        # ... other commands ...\n        # clean up temporary files\n        apt-get clean && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*\nThis command will remove all temporary files created during the installation of packages, including package lists and downloaded package files. By doing this, it will help to keep your container’s size as small as possible, and can also help to avoid potential issues with the container.\nYou can also include this set of commands at the end of your %runscript, so that when the container runs it will clean temporary files after the completion of the container’s job.\n    %runscript\n       # commands to run your container\n       ...\n       apt-get clean && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*\nThis will keep your container clean and ready for the next run.\nIn the example provided in the Appendix we also added:\n        apt-get update\n        apt-get autoremove\n        apt-get autoclean"
  },
  {
    "objectID": "Guides/05_How_to_use_Containers/Guide_Containers.html#building-the-container",
    "href": "Guides/05_How_to_use_Containers/Guide_Containers.html#building-the-container",
    "title": "Using Containers",
    "section": "3.3 Building the container",
    "text": "3.3 Building the container\ntime singularity build --fakeroot iPROJECT_ID_BPLIM.sif PROJECT_ID_BPLIM.def\nWe usually name the image with the prefix i followed by the name of the definition file. For example, iBPLIM-Stata_V1.sif, where the definition file is named BPLIM-Stata_V1.def."
  },
  {
    "objectID": "Guides/05_How_to_use_Containers/Guide_Containers.html#an-example-of-a-singularity-definition-file",
    "href": "Guides/05_How_to_use_Containers/Guide_Containers.html#an-example-of-a-singularity-definition-file",
    "title": "Using Containers",
    "section": "5.1 An example of a Singularity definition file",
    "text": "5.1 An example of a Singularity definition file\n        Bootstrap: docker\n        From: ubuntu:20.04\n        IncludeCmd: yes\n\n\n        %runscript\n            if [ $# -ne 1 ]; then\n                echo \"Please provide the main script\"\n                exit 1\n            fi\n            stata-mp -e do \"$1\"\n            if tail -1 \"$log\" | egrep \"^r\\([0-9]+\\);\"\n            then\n                exit 1\n            else\n                exit 0\n            fi\n\n\n        %files\n          /mnt/cephfs/home/exu0o9@bdp.pt/containers/\n          _SOURCES/libpng12-0_1.2.54-1ubuntu1.11ppa0eoan_amd64.deb\n          \n          /mnt/cephfs/home/exu0o9@bdp.pt/containers/\n          _SOURCES/stata17.tar.gz\n          \n          /mnt/cephfs/home/exu0o9@bdp.pt/containers/\n          _SOURCES/stata_container_just_Stata.do\n\n\n        %environment\n\n          R_VERSION=4.1.0\n          export R_VERSION\n          R_CONFIG_DIR=/etc/R/\n          export R_CONFIG_DIR\n          export LC_ALL=C\n          export PATH=$PATH\n\n           TZ=Europe/Lisbon\n           export PATH=\"/opt/stata17:$PATH\"\n\n\n        %labels\n\n          Author         :: Gustavo Iglesias and Miguel Portela - \n                            BPLIM\n                            \n          Version        :: (just) Stata -- V1.0.1\n          \n          Build_date     :: January 12, 2023\n\n\n        %post\n\n        apt update && apt-get update &&\n        DEBIAN_FRONTEND=\"noninteractive\" TZ=\"Europe/London\" apt-get\n        -y install apt-transport-https apt-utils\n        software-properties-common dirmngr curl wget xkb-data x11-apps\n        bzip2 qt5-default mesa-utils libgl1-mesa-dev libgl1-mesa-glx\n        libegl1-mesa libxrandr2 libxss1 libxcursor1 libxcomposite1\n        libasound2 libxi6 libxtst6 iproute2 swig build-essential\n        libnss3 net-tools unixodbc-dev git vim krb5-user libncurses5\n        libxml2-dev libsasl2-dev libldap2-dev libssl-dev libnlopt-dev\n        gnupg gnupg2 unixodbc gfortran nano cmake libblas3 libblas-dev\n        liblapack-dev liblapack3 aptitude xorg-dev libreadline-dev\n        libpcre3-dev liblzma-dev libbz2-dev libcurl4-openssl-dev\n        libmagick++-dev libhdf5-dev hdf5-helpers gsl-bin libgsl-dev\n        libgsl23 libgslcblas0 libgdal-dev libproj-dev libnss3\n        libzmq3-dev libgtk2.0-0\n        \n\n\n        #  Stata\n\n            tar -xvzf/mnt/cephfs/home/exu0o9@bdp.pt/containers/_SOURCES\n            /stata17.tar.gz --no-same-owner\n            \n            mv stata17 /opt/\n            \n            dpkg -i/mnt/cephfs/home/exu0o9@bdp.pt/containers/_SOURCES\n            /libpng12-0_1.2.54-1ubuntu1.11ppa0eoan_amd64.deb\n            \n            export PATH=\"/opt/stata17:$PATH\"\n\n            \n            # install ado files\n            mkdir /opt/stata17/ado/plus\n\n            stata-mp -b do\n            /mnt/cephfs/home/exu0o9@bdp.pt/containers/_SOURCES/\n            stata_container_just_Stata.do\n            \n                chmod -R ugo=rx /opt/stata17/\n\n\n        # CLEAN temporary files\n\n            apt-get update\n            apt-get autoremove\n            apt-get autoclean"
  }
]