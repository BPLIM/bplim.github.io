[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Banco de Portugal Microdata Research Laboratory GitHub repository",
    "section": "",
    "text": "Welcome to BPLIM: December 2025\nThis is BPLIM’s GitHub repository. Here, you will find our Instructional Videos, Guides, Data Manuals, and Tools. It is organized to help you navigate and search through our public resources using the search tool at the top right corner of this page. PDF versions of the Guides and Data Manuals, as well as auxiliary files, are available in each section. You can also explore and clone our tools folder, which contains all the packages developed and maintained by the BPLIM team. For Stata users, some ado files are ready to install using net install.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "Guides/07_Working_with_Pseudo-Data/pseudo_data_guide.html#data-structure-and-links",
    "href": "Guides/07_Working_with_Pseudo-Data/pseudo_data_guide.html#data-structure-and-links",
    "title": "How to Work with Pseudo-Data",
    "section": "Data structure and links",
    "text": "Data structure and links\nTo prepare the pseudo-data BPLIM must work with the researchers to identify all original datasets to use in the project along with the linking variables. For each original dataset, BPLIM will create a metafile and a file with a sample of the id (linking) variables. These files are needed to create the pseudo-data. As mentioned in the article, BPLIM does not provide the actual pseudo-data, but instead provides a set of Stata do-files that will generate the pseudo-data.",
    "crumbs": [
      "Home",
      "Guides",
      "Work with Pseudo-Data"
    ]
  },
  {
    "objectID": "Guides/07_Working_with_Pseudo-Data/pseudo_data_guide.html#the-package",
    "href": "Guides/07_Working_with_Pseudo-Data/pseudo_data_guide.html#the-package",
    "title": "How to Work with Pseudo-Data",
    "section": "The package",
    "text": "The package\nBPLIM will prepare and send to the external researchers a compressed file with all the necessary files. For illustrative purposes, let’s assume that the name of the project is pxxx_BPLIM and that the researcher identified two original files to work with:“CB_2006” and “CB_2007”. The researcher will receive a zipped file, for example, pxxx_BPLIM_pseudo.zip. The researcher only has to unzip that file. This will create the following directory/file structure:\n.../package\n│\n├── ados/\n│\n├── dos/\n│   ├── generate_dummy_dos.do\n│   ├── master.do \n│   ├── CB_2006_dummy.do\n│   └── ...\n│   \n├── ids/\n│   ├── CB_2006_ID.dta\n│   ├── CB_2007_ID.dta\n│   └── ...\n│   \n├── metadata/\n│   ├── CB_2006_meta.xlsx\n│   ├── CB_2007_meta.xlsx\n│   └── ...\n│\n└── pxxx_BPLIM/\n    └── ...\nAll the files in the first four directories - ados, dos, ids, and metadata - are needed to generate the pseudo datasets. The directory pxxx_BPLIM is the project folder, where the researcher is supposed to develop his/her work. Below we will further detail the structure of the project folder.",
    "crumbs": [
      "Home",
      "Guides",
      "Work with Pseudo-Data"
    ]
  },
  {
    "objectID": "Guides/07_Working_with_Pseudo-Data/pseudo_data_guide.html#creating-the-pseudo-data",
    "href": "Guides/07_Working_with_Pseudo-Data/pseudo_data_guide.html#creating-the-pseudo-data",
    "title": "How to Work with Pseudo-Data",
    "section": "Creating the pseudo data",
    "text": "Creating the pseudo data\nAfter unzipping the file, the researcher should open the file master.do in Stata (located in the dos folder). Please make sure that the Stata working directory is …/package/dos (where the master.do file is located) because we use relative paths in this script to reference other necessary files. Running master.do will create the pseudo datasets and place them in the project directory pxxx_BPLIM under the folder initial_dataset. The project directory has the following structure:\n.../package/pxxx_BPLIM/\n│\n├── initial_dataset/\n│   ├── CB_D_2006.dta\n│   ├── CB_D_2007.dta\n│   └── ...\n│\n├── results/\n│   └── ...\n│   \n├── tools/\n│   └── ...\n│\n└── work_area\n    ├── profile.do\n    └── template.do\nwhere we show the pseudo-data files that were created. These files will contain “_D_” in their names to reflect the fact that they are “dummy” data. After creating the pseudo data, you may relocate the project directory wherever you wish. However, the structure of the project directory must remain the same, because it mirrors the structure that BPLIM (or an internal user) has to replicate the code using original data. You should not copy any additional data files to the folder initial_dataset.",
    "crumbs": [
      "Home",
      "Guides",
      "Work with Pseudo-Data"
    ]
  },
  {
    "objectID": "Guides/07_Working_with_Pseudo-Data/pseudo_data_guide.html#setting-up-the-project",
    "href": "Guides/07_Working_with_Pseudo-Data/pseudo_data_guide.html#setting-up-the-project",
    "title": "How to Work with Pseudo-Data",
    "section": "Setting up the project",
    "text": "Setting up the project\n\nModifying the profile.do\nAfter creating the pseudo data, the researcher is ready to start coding. The researcher should place all scripts in the work_area directory. In this area, the researcher is free to organize the code as it pleases. However, you may have noticed that this folder already contains two files: “template.do” and “profile.do”. As the name suggests, template.do is a template that you should use to prepare your scripts. But before you can start coding you must edit profile.do. This do-file sets all the configurations (globals, paths, etc.) and must be placed in the same directory as the script file that you are executing. Stata will run this file first automatically. For example, suppose that you decide to place your project directory under the folder C:/Users/Jane/. In that case you edit profile.do and adjust the global root_path, to “C:/Users/Jane/pxxx_BPLIM”, the path to the project directory.\n*********************************************************\n*            Initialization                              \n*********************************************************\nversion 18\nclear all\nprogram drop _all\nset more off\nset rmsg on\nset matsize 10000\nset linesize 255\ncapture log close\n*********************************************************\n*               Define globals                          *\n********************************************************* \n**** Path for replication ****\n* Root path\nglobal root_path \"C:/Users/Jane/pxxx_BPLIM\"    /* changed here  */\n* Base path for replications\nglobal path_rep \"${root_path}/work_area\"\n\n**** Paths for data ****\n* Set the path for non perturbed data source\nglobal path_source \"${root_path}/initial_dataset\"\n\n**** Globals for type of modified dataset\n* Dummy \nglobal M1 \"D\"\n/*********************************************************************\nExample: pseudo data \n\nuse \"${path_source}/SLB_${M1}_YBNK_20102018_OCT20_QA1_V01.dta\"\n*********************************************************************/\n\n**** Path for project specific ado files ****\nadopath ++ \"${root_path}/tools\"\n\n* Change ados path\nsysdir set PLUS \"${root_path}/tools\"\nlocal places \"SITE PERSONAL OLDPLACE\"\nforeach p of local places {\n  capture adopath - `p'\n}\nNote that the globals path_rep and path_source are built based on root_path. Global M1 is used to reference the type of dataset being used. This means that when writing your scripts you must always use the global M1 when referring to the datasets. For example, when reading the data do not write:\nuse \"${path_source}/CB_D_2006.dta\"\nbut instead use the global in the name of the file:\nuse \"${path_source}/CB_${M1}_2006.dta\"\n\n\nInstalling tools for your project\nAll the external user-written ados must be installed in the tools directory of your project. We recommend using adoinstall for this purpose. Fo example, to install reghdfe in the tools directory, you need to type the following in Stata:\nssc install adoinstall\nadoinstall reghdfe, to(\"C:/Users/Jane/package/pxxx_BPLIM/tools\")\nThe scripts that you prepare should only use user-written commands that are placed in the tools directory of your project.",
    "crumbs": [
      "Home",
      "Guides",
      "Work with Pseudo-Data"
    ]
  },
  {
    "objectID": "Guides/07_Working_with_Pseudo-Data/pseudo_data_guide.html#preparing-your-code",
    "href": "Guides/07_Working_with_Pseudo-Data/pseudo_data_guide.html#preparing-your-code",
    "title": "How to Work with Pseudo-Data",
    "section": "Preparing your code",
    "text": "Preparing your code\nIt is really important to follow the above guidelines to ensure that results can be safely replicated on the original data. If the code is well organized and follows the guidelines then BPLIM staff (or an internal co-author) can easily change the configuration file profile.do to rerun the analysis on the original data. The template file that we provide shows examples on how the researcher should code, based on the settings defined in profile.do. Below we show the contents of template.do:\n* Project      :  pxxx_BPLIM\n* Author(s)    :\n* Date         :\n* Description  :\n* Dependencies :\n* Modifications: (add date, author and change)\n\n* Run profile (usually not needed, but just to be sure)\ncapture run \"profile.do\"\n\n* Change to work path - global `path_rep` defined in profile.do\ncd \"${path_rep}\"\n\n/* You may create a `results` folder inside `path_rep` to save outputs (this \nis optional since there is already a `results` folder outside `path_rep`)\nAlways use capture when creating directories in scripts*/\ncapture mkdir results\n* You may create the structure that you want, adding sub-diectories to `results`\ncapture mkdir results/tables\ncapture mkdir results/figures\n\n/*\nWhen defining globals for paths (if you do not want to use relative paths), remember to\ninclude the global `path_rep`. This is the path where the analysis should run. See the\ntwo examples below, where we define two globals for separate results folders\n*/\nglobal results_tables \"${path_rep}/results/tables\"\nglobal results_figures \"${path_rep}/results/figures\"\n\n* Creating a log file in the work area, where \"logexample\" is the log requested for extraction\nlog using \"logexample.log\", replace\n\n*********************************************************\n*                  Open data files                      *\n*********************************************************\n/*\nPlease note the VERY IMPORTANT use of global `M1`, ${M1}, \nin the file names of the modified data. Failing to use the \nglobals when working with modified data will cause the \nREPLICATION TO FAIL.\n*/\n\n* Example on how to read a dummy data file provided by BPLIM:\nuse \"${path_source}/CB_${M1}_2006\", clear\n\n*********************************************************\n*            Start data analysis                        *\n*********************************************************\n*\n* ...\n* YOUR STATA CODE GOES HERE\n* ...\n* You may call other do-files, just be sure to use the\n* globals defined in the profile and eventually in this\n* file\n\n*********************************************************\n*            Saving the results                             *\n*********************************************************\n\n* Saving an intermediate dataset. You may save it in your\n* work area, if you wish to preserve it in order to analyze\n* it later. However, in a replication, if these datasets are\n* not needed, you can delete them. As an example, see below:\n\n* Create intermediate data directory\ncap mkdir intermediate_data\n\n* Add global for directory\nglobal path_data \"${path_rep}/intermediate_data\"\n\n* Save datasets\ncompress\nsave \"${path_data}/filename1.dta\", replace\nsave \"${path_data}/filename2.dta\", replace\n\n* Remove intermediate data after analysis is finished\nlocal files: dir \"${path_data}\" files \"*.dta\", respectcase\nforeach file of local files {\n  rm \"${path_data}/`file'\"\n}\n\n****** Results Examples ******\n* Creating a graph and saving to the results area (figures)\ngraph export \"${results_figures}/mygraph.png\", replace\n* Creating a table and saving to the results area (tables)\nesttab using \"${results_tables}/myfile.tex\", cells(\"cell1 cell2 ...\")\n\n*********************************************************\n*                  Close the log file                   *\n*********************************************************\nlog close\n\n***************** IMPORTANT *****************\n\n* BPLIM reserves the right to decline to send log files that are \n* exceptionally large. For really long scripts, remember that not\n* every line of code and its output needs to be in the log file.\n* Take the following example:\n/*\nlog using \"mylog.log\", replace\n\nquietly {\n  sysuse auto, clear\n  summarize price\n  noisily display \"Mean Price: `r(mean)'\"\n  drop if foreign == 1\n  keep price mpg rep78\n  noisily reg price mpg i.rep78\n}\n\nlog close\n*/\n\n* Only the outputs of the third and sixth lines will appear in the\n* log file. This is a good strategy to follow if your log files are \n* too cluttered with code and output, which are only intermediate steps\n* to final outputs\nNotice the first lines of the template, where we run the configuration file - capture run \"profile.do\" - and change directory to the work area folder - cd \"${path_rep}\". Note that global path_rep is defined in the configuration file. We also create new folders in our working directory and set globals in order to reference them later:\ncapture mkdir results\n* You may create the structure that you want, adding sub-diectories to `results`\ncapture mkdir results/tables\ncapture mkdir results/figures\n...\nglobal results_tables \"${path_rep}/results/tables\"\nglobal results_figures \"${path_rep}/results/figures\"\nThe remaining lines of the template contain standard examples of how to generate outputs and organize your code.\n\nCreate a master script\nResearchers should create a master script that runs their analysis from top to bottom. This file should be based on template.do and must create a log file that proves that the code ran without errors. Still, researchers are free to organize code in the work_area as they please. For example, the following structure would be acceptable:\n.../package/pxxx_BPLIM/work_area/\n│\n├── profile.do\n├── master.do\n│\n├── 01_data_management/\n│   ├── 01_data_manipulation.do\n│   └── ...\n│\n├── 02_exploratory/\n│   ├── 01_tables.do\n│   ├── 02_figures.do\n│   └── ...\n│\n└── 03_regressions/\n    ├── 01_regressions.do\n    └── ...\nThen, within master.do, the researcher only has to run the dependencies:\n...\ncd ${path_rep}\n...\n\ndo 01_data_management/01_data_manipulation.do\n\ndo 02_exploratory/01_tables.do\ndo 02_exploratory/02_figures.do\n\ndo 03_regressions/01_regressions.do\n\nlog close",
    "crumbs": [
      "Home",
      "Guides",
      "Work with Pseudo-Data"
    ]
  },
  {
    "objectID": "Guides/07_Working_with_Pseudo-Data/pseudo_data_guide.html#creating-the-replicability-package",
    "href": "Guides/07_Working_with_Pseudo-Data/pseudo_data_guide.html#creating-the-replicability-package",
    "title": "How to Work with Pseudo-Data",
    "section": "Creating the replicability package",
    "text": "Creating the replicability package\nAfter completing your analysis, you must prepare your replication package to send to BPLIM. With that package, BPLIM wil be able to replicate your results using the original data.\nPreparing the package is a simple procedure. All you need to do is run ado archive_rep, which will zip all the necessary files for the replication. It also generates a list of all the files used and creates a requirements file with all the dependencies (ados). Please note that only script files (ados, dos, etc) are copied to the archive. In this case, we would run the following in Stata:\nadopath + \"C:/Users/Jane/pxxx_BPLIM/tools\"\narchive_rep, rep(1) path(\"C:/Users/Jane/pxxx_BPLIM\")\nThe output of the command is a folder named Rep001 with the folders and files (scripts) needed for the replication. This folder, and the zip file Rep001.zip (zip file of the folder) are created in the work_area directory. The researcher only has to send the zip file to BPLIM.",
    "crumbs": [
      "Home",
      "Guides",
      "Work with Pseudo-Data"
    ]
  },
  {
    "objectID": "Guides/07_Working_with_Pseudo-Data/pseudo_data_guide.html#recap",
    "href": "Guides/07_Working_with_Pseudo-Data/pseudo_data_guide.html#recap",
    "title": "How to Work with Pseudo-Data",
    "section": "Recap",
    "text": "Recap\nIt is important that the researcher follows the guidelines and the proper workflow:\n\nResearcher identifies the datasets and tools for the project\nBPLIM staff prepares the package and sends it to the researcher in a zip file\nResearcher unpacks the file and creates the project structure\nResearcher runs the do-file to create the pseudo data in the project initial dataset folder\nResearcher adapts profile.do, namely the global root_path, so that it points to the correct directory in her computer\nResearcher creates a master.do file based on template.do (it can be copied) and organizes the code as she desires\nAfter the analysis is finished, the researcher runs the ado archive_rep and sends the output zip file to BPLIM. Researchers should clearly indicate the output files needed for the analysis.\nBPLIM checks that the analysis runs from top to bottom. In case that it does, the staff will run the analysis on the original data\nBPLIM checks the output files requested by the researchers and emails the results if they respect the output control rules.\n\nImportant note:\n\nIntermediate datasets must be created during the analysis. Remember that the researcher is only allowed to send scripts and logs for replications, so BPLIM only has access to datasets in the initial_dataset folder. Trying to reference intermediate data that is not created during the analysis will cause the replication to fail.",
    "crumbs": [
      "Home",
      "Guides",
      "Work with Pseudo-Data"
    ]
  },
  {
    "objectID": "Guides/05_How_to_use_Containers/Guide_Containers.html#alternative-a-use-a-pre-defined-setup",
    "href": "Guides/05_How_to_use_Containers/Guide_Containers.html#alternative-a-use-a-pre-defined-setup",
    "title": "Using Containers",
    "section": "Alternative A: use a pre-defined setup",
    "text": "Alternative A: use a pre-defined setup\nOnce you have logged in to the server, you will see an icon that opens the container for your project. To launch the container and start the predefined application that you specified in your request, simply double-click on the icon. Once you have completed this step, it will function similarly to any other application.",
    "crumbs": [
      "Home",
      "Guides",
      "How to use Containers"
    ]
  },
  {
    "objectID": "Guides/05_How_to_use_Containers/Guide_Containers.html#alternative-b-use-the-command-line",
    "href": "Guides/05_How_to_use_Containers/Guide_Containers.html#alternative-b-use-the-command-line",
    "title": "Using Containers",
    "section": "Alternative B: use the command line",
    "text": "Alternative B: use the command line\nThis solution makes use of the Terminal, giving you more freedom to interact with the container. Follow the steps below:\n\nOpen a Terminal in your project’s work_area \n(e.g., /bplimext/projects/PROJECT_ID/work_area).\nThe container is located in your project’s tools folder \n(/bplimext/projects/PROJECT_ID/tools/_container).\nRun the container using the following command:\n\nsingularity shell ../tools/iPROJECT_ID.BPLIM_Python_R_Jupyter.sif\n\nInside the container, the prompt will change to Singularity &gt;\n\n\n\n\n\nFor example, to open a Jupyter Notebook, type:\n\njupyter notebook\nIf you want to use Stata or RStudio, type xstata-mp or rstudio, respectively.\n\nWhen you’re finished using the container, type exit to leave the Singularity image.",
    "crumbs": [
      "Home",
      "Guides",
      "How to use Containers"
    ]
  },
  {
    "objectID": "Guides/05_How_to_use_Containers/Guide_Containers.html#starting-point-the-container",
    "href": "Guides/05_How_to_use_Containers/Guide_Containers.html#starting-point-the-container",
    "title": "Using Containers",
    "section": "Starting point: the container",
    "text": "Starting point: the container\nA Singularity container is a way to package and distribute software and its dependencies in a portable and isolated environment. To build a Singularity container, you’ll need to have Singularity installed on your system.\nHere are the basic steps to build a container:\n\nCreate a recipe/definition file: This is a text file that contains the instructions for building the container. The recipe file should specify the base image to use, any additional software to install, and any environment variables to set.\nBuild the container: Use the singularity build command to build the container from the recipe file. For example:\n\nsingularity build mycontainer.sif recipe.def\n\nTest the container: Use the singularity shell command to enter the container and test that it runs correctly. For example:\n\nsingularity shell mycontainer.sif\n\n(optional) Publish your container to a public or private registry, like Singularity Hub or Singularity Container Library.\n\nIn the Appendix of this manual, you will find an example of a definition file for building a container with Stata. For detailed instructions and troubleshooting, you can refer to the official documentation at https://sylabs.io/guides/3.6/user-guide/index.html. Additionally, here is a simple recipe file that creates a container based on the Ubuntu 20.04 base image and installs the nano text editor:\n        Bootstrap: library\n        From: ubuntu:20.04\n\n        %post\n            apt-get update\n            apt-get install -y nano",
    "crumbs": [
      "Home",
      "Guides",
      "How to use Containers"
    ]
  },
  {
    "objectID": "Guides/05_How_to_use_Containers/Guide_Containers.html#the-definition-file-step-by-step",
    "href": "Guides/05_How_to_use_Containers/Guide_Containers.html#the-definition-file-step-by-step",
    "title": "Using Containers",
    "section": "The definition file step by step",
    "text": "The definition file step by step\nIn a Singularity definition file, you can write a variety of commands and instructions. Some common things you might include in the file are:\n\nPackage installation: You can use package managers like apt or yum to install software and dependencies that your container needs to run. For example, you might install a specific version of Python or a library that your application depends on.\nEnvironment setup: You can use commands like ENV or export to set environment variables that your container needs to run. For example, you might set the PATH variable to include the location of a specific binary or library.\nFile copy: You can use commands like COPY or ADD to copy files from the host system into the container. For example, you might copy a script or a configuration file that your application needs to run.\nRunscript: As previously stated, the Runscript is a script that is executed when the container is run. It typically contains commands to set up the environment and launch the application or process that the container is designed to run.\nLabels: You can include information about the container, such as the container’s name, version, and author.\nHelp: You can include a brief description of the container, which is useful for users who are trying to understand what the container does.\n\nIt’s worth noting that depending on the complexity of your container and the requirements of your application, you may need to include additional commands and instructions in your definition file.\n\nheader\nIn a Singularity definition file, the “header” refers to the first section of the file that contains metadata and instructions for building the container. The header typically includes information such as the container’s name, version, and author, as well as instructions for obtaining and installing the software that the container is designed to run. The header also can include instructions for configuring the build environment, such as setting environment variables or installing dependencies. The header is usually written in a specific format (e.g. #!Singularity) and starts at the first line of the file.\n        #!Singularity\n        Bootstrap: library\n        From: ubuntu:20.04\n\n        %help\n        This container runs Stata.\n\n        %labels\n        AUTHOR BPLIM\n        VERSION v1.0\nThis header uses the Bootstrap: library to indicate that the container should be built using the Singularity Library, and uses the From: ubuntu:20.04 to indicate that the container is based on the Ubuntu 20.04 image. This header also includes a %help section that provides a brief description of the container, and a %labels section that includes information about the container’s author and version.\nIt’s worth noting that the way headers are written is not fixed and different instructions could be included.\n\n\nrunscript\nThe “runscript” in a Singularity definition file is a script that is executed when the container is run. It typically contains commands to set up the environment and launch the application or process that the container is designed to run. The runscript is executed by the Singularity runtime after the container is started, and it can be used to configure the container’s environment, set up the application, and launch the application or process.\nAn example:\n    %runscript\n        Rscript myscript.R\nIn case you want the container to execute a Stata .do file you can write something along the following lines:\n    %runscript\n        if [ $# -ne 1 ]; then\n            echo \"Please provide the main script\"\n            exit 1\n        fi\n        stata-mp -e do \"$1\"\n        if tail -1 \"$log\" | egrep \"^r\\([0-9]+\\);\"\n        then\n            exit 1\n        else\n            exit 0\n        fi\nTo launch Stata in graphical mode, your run script should look something like this:\n%runscript\n    xstata-mp\nIf your container is named container_name.sif, you can launch Stata inside the container using the Terminal by typing the following command:\n./container_name.sif\nIf you want to execute a particular file, you should type the following command in the Terminal:\n./container_name.sif MyDoFile.do\n\n\nfiles\nThe %files section in a Singularity definition file is used to specify files or directories that should be included in the container image when it is built. Here is an example of what you might include in the %files section:\n    %files\n    /path/to/myfile1.txt\n    /path/to/mydir1\n    /path/to/myfile2.sh\n    /path/to/mydir2\nThis would include the files myfile1.txt and myfile2.sh and the directories mydir1 and mydir2 in the container image, with the same paths.\nYou can also use wildcard to copy multiple files or directories, for example :\n    %files\n    /path/to/mydir/*\nThis would include all files and directories in the folder mydir in the container image.\nIt’s worth noting that the %files section is optional, you don’t need to include it in your definition file if you don’t need to add any additional files to your container. Also, the %files section is only useful when creating a new container, if you want to add files to an existing container you can use the singularity copy command.\n\n\nenvironment\nThe %environment section in a Singularity definition file is used to specify environment variables that should be set when the container is run. Here is an example of what you might include in the %environment section to build a container running R version 4.1.1:\n    %environment\n        export R_VERSION=4.1.1\n        export R_HOME=/usr/lib/R/$R_VERSION\n        export PATH=$PATH:$R_HOME/bin\nThis will set the environment variable R_VERSION to 4.1.1, the environment variable R_HOME to /usr/lib/R/4.1.1 and the environment variable PATH to include the R’s binary path \\$R_HOME/bin\nYou can use these environment variables in your %post or %runscript sections to install R 4.1.1 and run commands with the R version 4.1.1.\nThis will set the environment variable R_VERSION to 4.1.1, the environment variable R_HOME to /usr/lib/R/4.1.1 and the environment variable PATH to include the R’s binary path \\$R_HOME/bin\n    %post\n        apt-get update && apt-get install -y r-base=$R_VERSION\nIt’s worth noting that you can also use ENV instruction in place of export and you can use %environment section to set any environment variable you want, not only R.\n\n\npost\nThe %post section in a Singularity definition file is used to specify commands that should be run during the container build process, after the base image has been imported. Here is an example of what you might include in the %post section to build a container running R version 4.1.1 and the most recent version of RStudio Server:\n    %post\n        # update package lists and install R 4.1.1\n        apt-get update && apt-get install -y r-base=$R_VERSION\n        \n        # add RStudio repository and key\n        echo \"deb https://cran.rstudio.com/bin/linux/ubuntu \n        bionic-cran35/\" | tee -a /etc/apt/sources.list\n        apt-key adv --keyserver keyserver.ubuntu.com --recv-keys \n        E298A3A825C0D65DFD57CBB651716619E084DAB9\n        \n        # install RStudio Server\n        apt-get update && apt-get install -y rstudio-server\nThis will update the package lists, install R version 4.1.1 using the environment variable set before and add the RStudio repository and key to the container, then install RStudio Server on the container.\nIt’s worth noting that this example assumes that the container is based on Ubuntu 20.04 (codenamed ‘focal’), if you are using a different version or distribution you should adjust the package manager commands and repository URLs accordingly.\nAlso, the %post section is optional and you don’t need to include it in your definition file if you don’t need to run any additional command during the container building process.\nIf you also want to include Jupyter Notebook, the R kernel and R nbextensions in your container, you can add the following commands to the %post section of your Singularity definition file:\n    %post\n        # update package lists and install R 4.1.1\n        apt-get update && apt-get install -y r-base=$R_VERSION\n        \n        # add RStudio repository and key\n        echo \"deb https://cran.rstudio.com/bin/linux/ubuntu \n        bionic-cran35/\" | tee -a /etc/apt/sources.list\n        apt-key adv --keyserver keyserver.ubuntu.com --recv-keys\n        E298A3A825C0D65DFD57CBB651716619E084DAB9\n        \n        # install RStudio Server\n        apt-get update && apt-get install -y rstudio-server\n        \n        # install Jupyter\n        apt-get install -y jupyter-core\n        \n        # install R kernel for Jupyter\n        R -e \"install.packages(c('repr', 'IRdisplay', 'evaluate', \n        'crayon', 'pbdZMQ', 'devtools', 'uuid', 'digest'), \n        repos='https://cloud.r-project.org/')\"\n        R -e \"devtools::install_github('IRkernel/IRkernel')\"\n        R -e \"IRkernel::installspec(user = FALSE)\"\n        \n        \n        # install R nbextensions\n        R -e \"install.packages('devtools', repos = \n        'https://cloud.r-project.org/')\"\n        R -e \"devtools::install_github('randy3k/r-notebook')\"\n        \nThis will install Jupyter, the R kernel for Jupyter, and the R nbextensions for Jupyter Notebook.\nIt’s worth noting that these commands are installing R packages from CRAN and GitHub repositories, so you may want to check that these repositories are available on your system and that you have internet connection during the container build process.\nNote: you can install packages from binaries.\n\n\nIncluding additional packages\nTo include TinyTeX and machine learning packages in your container, you can add the following commands to the %post section of your Singularity definition file:\n    %post\n        # update package lists and install R 4.1.1\n        apt-get update && apt-get install -y r-base=$R_VERSION\n        \n        # add RStudio repository and key\n        echo \"deb https://cran.rstudio.com/bin/linux/ubuntu\n        bionic-cran35/\" | tee -a /etc/apt/sources.list\n        apt-key adv --keyserver keyserver.ubuntu.com --recv-keys\n        E298A3A825C0D65DFD57CBB651716619E084DAB9\n        \n        # install RStudio Server\n        apt-get update && apt-get install -y rstudio-server\n        \n        # install Jupyter\n        apt-get install -y jupyter-core\n        \n        # install R kernel for Jupyter\n        R -e \"install.packages(c('repr', 'IRdisplay', 'evaluate',\n        'crayon', 'pbdZMQ', 'devtools', 'uuid', 'digest'),\n        repos='https://cloud.r-project.org/')\"\n        R -e \"devtools::install_github('IRkernel/IRkernel')\"\n        R -e \"IRkernel::installspec(user = FALSE)\"\n        \n        # install R nbextensions\n        R -e \"install.packages('devtools', repos = \n        'https://cloud.r-project.org/')\"\n        R -e \"devtools::install_github('randy3k/r-notebook')\"\n        \n        # install TinyTeX\n        wget -qO- \"https://github.com/yihui/tinytex/raw/main/tools/inst\n        all-unx.sh\" | sh\n        \n        # install additional R packages\n        R -e \"install.packages(c('caret','randomForest','e1071','gbm','\n        xgboost', 'lightgbm','catboost','mlr','tidymodels','h2o','caret\n        Ensemble','pROC','ROCR',  \n        'pROC.plot','ROCR.plot','kernlab','pls','neuralnet','nnet'), \n        repos='https://cloud.r-project.org/')\"\n        \n        \nThis will install TinyTeX, which is a lightweight, cross-platform, and easy-to-maintain LaTeX distribution and also some popular machine learning packages such as caret, randomForest, e1071, gbm, xgboost, lightgbm, catboost, mlr, tidymodels, h2o, caretEnsemble, pROC, ROCR, pROC.plot, ROCR.plot, kernlab, pls, neuralnet, and nnet.\n\n\nCleaning temporary files\nTo clean up temporary files during the build process of your container, you can add the following command to the %post section of your Singularity definition file:\n    %post\n        # ... other commands ...\n        # clean up temporary files\n        apt-get clean && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*\nThis command will remove all temporary files created during the installation of packages, including package lists and downloaded package files. By doing this, it will help to keep your container’s size as small as possible, and can also help to avoid potential issues with the container.\nYou can also include this set of commands at the end of your %runscript, so that when the container runs it will clean temporary files after the completion of the container’s job.\n    %runscript\n       # commands to run your container\n       ...\n       apt-get clean && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*\nThis will keep your container clean and ready for the next run.\nIn the example provided in the Appendix we also added:\n        apt-get update\n        apt-get autoremove\n        apt-get autoclean",
    "crumbs": [
      "Home",
      "Guides",
      "How to use Containers"
    ]
  },
  {
    "objectID": "Guides/05_How_to_use_Containers/Guide_Containers.html#building-the-container",
    "href": "Guides/05_How_to_use_Containers/Guide_Containers.html#building-the-container",
    "title": "Using Containers",
    "section": "Building the container",
    "text": "Building the container\ntime singularity build --fakeroot iPROJECT_ID_BPLIM.sif PROJECT_ID_BPLIM.def\nWe usually name the image with the prefix i followed by the name of the definition file. For example, iBPLIM-Stata_V1.sif, where the definition file is named BPLIM-Stata_V1.def.",
    "crumbs": [
      "Home",
      "Guides",
      "How to use Containers"
    ]
  },
  {
    "objectID": "Guides/05_How_to_use_Containers/Guide_Containers.html#an-example-of-a-singularity-definition-file",
    "href": "Guides/05_How_to_use_Containers/Guide_Containers.html#an-example-of-a-singularity-definition-file",
    "title": "Using Containers",
    "section": "An example of a Singularity definition file",
    "text": "An example of a Singularity definition file\n        Bootstrap: docker\n        From: ubuntu:20.04\n        IncludeCmd: yes\n\n\n        %runscript\n            if [ $# -ne 1 ]; then\n                echo \"Please provide the main script\"\n                exit 1\n            fi\n            stata-mp -e do \"$1\"\n            if tail -1 \"$log\" | egrep \"^r\\([0-9]+\\);\"\n            then\n                exit 1\n            else\n                exit 0\n            fi\n\n\n        %files\n          /mnt/cephfs/home/exu0o9@bdp.pt/containers/\n          _SOURCES/libpng12-0_1.2.54-1ubuntu1.11ppa0eoan_amd64.deb\n          \n          /mnt/cephfs/home/exu0o9@bdp.pt/containers/\n          _SOURCES/stata17.tar.gz\n          \n          /mnt/cephfs/home/exu0o9@bdp.pt/containers/\n          _SOURCES/stata_container_just_Stata.do\n\n\n        %environment\n\n          R_VERSION=4.1.0\n          export R_VERSION\n          R_CONFIG_DIR=/etc/R/\n          export R_CONFIG_DIR\n          export LC_ALL=C\n          export PATH=$PATH\n\n           TZ=Europe/Lisbon\n           export PATH=\"/opt/stata17:$PATH\"\n\n\n        %labels\n\n          Author         :: Gustavo Iglesias and Miguel Portela - \n                            BPLIM\n                            \n          Version        :: (just) Stata -- V1.0.1\n          \n          Build_date     :: January 12, 2023\n\n\n        %post\n\n        apt update && apt-get update &&\n        DEBIAN_FRONTEND=\"noninteractive\" TZ=\"Europe/London\" apt-get\n        -y install apt-transport-https apt-utils\n        software-properties-common dirmngr curl wget xkb-data x11-apps\n        bzip2 qt5-default mesa-utils libgl1-mesa-dev libgl1-mesa-glx\n        libegl1-mesa libxrandr2 libxss1 libxcursor1 libxcomposite1\n        libasound2 libxi6 libxtst6 iproute2 swig build-essential\n        libnss3 net-tools unixodbc-dev git vim krb5-user libncurses5\n        libxml2-dev libsasl2-dev libldap2-dev libssl-dev libnlopt-dev\n        gnupg gnupg2 unixodbc gfortran nano cmake libblas3 libblas-dev\n        liblapack-dev liblapack3 aptitude xorg-dev libreadline-dev\n        libpcre3-dev liblzma-dev libbz2-dev libcurl4-openssl-dev\n        libmagick++-dev libhdf5-dev hdf5-helpers gsl-bin libgsl-dev\n        libgsl23 libgslcblas0 libgdal-dev libproj-dev libnss3\n        libzmq3-dev libgtk2.0-0\n        \n\n\n        #  Stata\n\n            tar -xvzf/mnt/cephfs/home/exu0o9@bdp.pt/containers/_SOURCES\n            /stata17.tar.gz --no-same-owner\n            \n            mv stata17 /opt/\n            \n            dpkg -i/mnt/cephfs/home/exu0o9@bdp.pt/containers/_SOURCES\n            /libpng12-0_1.2.54-1ubuntu1.11ppa0eoan_amd64.deb\n            \n            export PATH=\"/opt/stata17:$PATH\"\n\n            \n            # install ado files\n            mkdir /opt/stata17/ado/plus\n\n            stata-mp -b do\n            /mnt/cephfs/home/exu0o9@bdp.pt/containers/_SOURCES/\n            stata_container_just_Stata.do\n            \n                chmod -R ugo=rx /opt/stata17/\n\n\n        # CLEAN temporary files\n\n            apt-get update\n            apt-get autoremove\n            apt-get autoclean",
    "crumbs": [
      "Home",
      "Guides",
      "How to use Containers"
    ]
  },
  {
    "objectID": "Guides/05_How_to_use_Containers/Guide_Containers.html#footnotes",
    "href": "Guides/05_How_to_use_Containers/Guide_Containers.html#footnotes",
    "title": "Using Containers",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee the installation notes on Sylabs.↩︎\nFor more information, please visit BPLIM’s GitHub resources.↩︎",
    "crumbs": [
      "Home",
      "Guides",
      "How to use Containers"
    ]
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#overview",
    "href": "Guides/03_External_Server/External_Server_Guide.html#overview",
    "title": "External Server Guide",
    "section": "Overview",
    "text": "Overview\nThis guide will help you connect to BPLIM’s external server, navigate your project environment, and use statistical software (Stata, R, Python, Julia) for your research. The server runs on Linux and uses containerized environments to ensure reproducibility and consistency across projects.\nKey features:\n\nSecure remote access via NoMachine client\nIsolated project environments with defined folder structures\nStatistical software running in containers\nSupport for interactive and batch processing modes",
    "crumbs": [
      "Home",
      "Guides",
      "External Server"
    ]
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#upon-access-approval",
    "href": "Guides/03_External_Server/External_Server_Guide.html#upon-access-approval",
    "title": "External Server Guide",
    "section": "Upon Access Approval",
    "text": "Upon Access Approval\nOnce access is approved, you can connect to the external server using the NoMachine client. See Download, install and configure NoMachine client for detailed instructions.",
    "crumbs": [
      "Home",
      "Guides",
      "External Server"
    ]
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#password-policy",
    "href": "Guides/03_External_Server/External_Server_Guide.html#password-policy",
    "title": "External Server Guide",
    "section": "Password Policy",
    "text": "Password Policy\nPasswords are a critical security component. Your initial password must be changed at first login, and all passwords must comply with the requirements below.\nPassword requirements:\n\nMinimum length: 8 characters\nCharacter classes: At least 4 different types (uppercase, lowercase, numbers, punctuation)\nHistory: Cannot reuse any of your last 7 passwords\nExpiration: Passwords expire after 60 days and must be changed at next login\nFailed attempts: After 6 consecutive failed login attempts, your account will be locked for 10 minutes\n\nFor complete password policy details, see Appendix: Password Requirements",
    "crumbs": [
      "Home",
      "Guides",
      "External Server"
    ]
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#first-steps",
    "href": "Guides/03_External_Server/External_Server_Guide.html#first-steps",
    "title": "External Server Guide",
    "section": "First Steps",
    "text": "First Steps\nThis section guides you through your initial login and introduces the basic interface and folder structure.\n\nLogging In\n\nWhen you start NoMachine, you will see the following connection screens:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAccessing Your Project\n\nOnce logged in, select the Kickoff Application Launcher menu:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNavigate to your project:\n\nClick on Applications.\nSelect BPLIM and click on your project (e.g., P999_research_project).\nThis opens the Dolphin file manager1, showing your project folder:\n\n\n\n\n\n\n\n\n\n\n\nYou can display the Terminal (command line) alongside Dolphin by pressing F4.\n\n\nUnderstanding Your Project Structure\n\nLauncher scripts: Files with the .sh extension are scripts that launch applications or enter containerized environments. For example, stata_container.sh starts Stata.2\nYou can run launcher scripts in two ways:\n\nGUI method: Click the .sh file in Dolphin\nTerminal method: Type ./stata_container.sh in the Terminal\n\nProject folders: Your project folder contains the following directories:\n\n\n\n\n\n\n\n\n\nDirectory\nPurpose\nAccess\n\n\n\n\ninitial_dataset\nData sources provided by BPLIM\nRead-only\n\n\n— external\nData provided by the researcher\nRead-only\n\n\n— intermediate\nIntermediate files\nRead-only\n\n\n— modified\nModified data provided by BPLIM\nRead-only\n\n\nresults\nOutput files generated by researchers\nRead-write\n\n\ntools\nProject-specific analysis tools\nRead-only\n\n\nwork_area\nTemporary working directory\nRead-write\n\n\n\nNote: Your work_area folder also contains templates for Stata, R and/or Python, depending on your project requirements. By default, these template files are read-only.\n\n\nLogging Out\n\nTo properly disconnect, log out as shown below, then close the NoMachine window:3\n\n\n\n\n\n\n\n\n\n\nConfirm by clicking Logout:4\n\n\n\n\n\n\n\n\n\nImportant notes about session management:\n\nPersistent sessions: If you do not log out, your session remains open until your next login. While this keeps programs running, it consumes server resources.\nBest practice for long-running tasks: Use batch mode (see Running Programs in Batch Mode) instead of leaving sessions open.\nServer maintenance: During server reboots, open sessions are terminated and unsaved work is lost. Save your work regularly.",
    "crumbs": [
      "Home",
      "Guides",
      "External Server"
    ]
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#managing-disk-space",
    "href": "Guides/03_External_Server/External_Server_Guide.html#managing-disk-space",
    "title": "External Server Guide",
    "section": "Managing Disk Space",
    "text": "Managing Disk Space\nProper disk space management is essential for maintaining access to the server.\n\nCritical Rule: Do Not Save Files in Your Home Folder\nNever save files in your home folder (/home/USER_LOGIN). If you exceed its size limit, you will not be able to log in. Always save files in your project’s work_area folder.\n\n\nMonitoring and Cleaning Your Project Folder\nCheck your project size regularly to avoid exceeding storage limits. Follow these steps in the Terminal:\n\nNavigate to your project folder:\ncd /bplimext/projects/P999_research_project/\nList the total project size:\ndu -h\nCheck folder sizes and list those larger than or equal to 1 GB:\ndu --max-depth=1 -h | sort -h | grep G\nMove to the work_area folder:\ncd work_area\nRepeat the size check in this folder:\ndu --max-depth=1 -h | sort -h | grep G\nIdentify duplicate or temporary files and remove them:\nrm FILE_TO_DELETE\nCompress large files or folders you are not currently using:\n\nCompress a folder:\ntar -zcvf YOUR_FOLDER.tar.gz YOUR_FOLDER\nCompress an individual file:\ngzip YOUR_FILE",
    "crumbs": [
      "Home",
      "Guides",
      "External Server"
    ]
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#using-the-terminal",
    "href": "Guides/03_External_Server/External_Server_Guide.html#using-the-terminal",
    "title": "External Server Guide",
    "section": "Using the Terminal",
    "text": "Using the Terminal\nThe Terminal is a command-line interface for interacting with the Linux system. It is essential for running programs in batch mode, managing files, and monitoring processes.\n\nAccessing the Terminal\nYou can access the Terminal in two ways:\n\nFrom the menu: Red Hat → Applications → System → Terminal\n\n\n\n\n\n\n\n\n\n\n\nFrom Dolphin: Press F4 to open an integrated Terminal at the current location\n\n\n\nEssential Terminal Tips\nQuick reference:\n\nCase sensitivity: Linux commands are case-sensitive (ls is not the same as LS)\nCommand history: Use arrow keys (up/down) to scroll through previous commands\nAuto-completion: Press Tab to auto-complete file names and commands\nKeyboard layouts: Non-English keyboards may have different symbol mappings. For example, on a Portuguese keyboard, + is on the ? key\nCommon commands: See Appendix: Shell Commands for a comprehensive list\n\nExample command:\nls -lArth\nLists files in human-readable format (h), long format (l), reverse order (r), sorted by modification time (t), including hidden files (A)",
    "crumbs": [
      "Home",
      "Guides",
      "External Server"
    ]
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#what-are-containers",
    "href": "Guides/03_External_Server/External_Server_Guide.html#what-are-containers",
    "title": "External Server Guide",
    "section": "What Are Containers?",
    "text": "What Are Containers?\nA container is a self-contained environment that includes a program along with all its dependencies, libraries, and configurations. This ensures:\n\nConsistency: Programs behave identically across different sessions\nReproducibility: Your analysis can be replicated exactly\nIsolation: Project-specific packages don’t conflict with other projects",
    "crumbs": [
      "Home",
      "Guides",
      "External Server"
    ]
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#starting-containers",
    "href": "Guides/03_External_Server/External_Server_Guide.html#starting-containers",
    "title": "External Server Guide",
    "section": "Starting Containers",
    "text": "Starting Containers\nThere are three ways to start a container:\n\nMethod 1: Using Launcher Scripts (Recommended)\nEach project includes launcher scripts (.sh files) for different software:\n\nstata_container.sh - Launches Stata\nr_container.sh - Launches R/RStudio\npython_container.sh - Launches Python/Jupyter\njulia_container.sh - Launches Julia\n\nTo use: Click the script in Dolphin or run ./script_name.sh in the Terminal.\n\n\nMethod 2: From the Terminal\ncd /bplimext/projects/P999_research_project\nsingularity shell tools/_container/CONTAINER_ID.sif\nAfter entering the container, the Terminal prompt changes to show Singularity&gt;. You can then launch applications manually.\n\n\nMethod 3: Direct Execution\nRun commands directly inside the container without entering an interactive shell:\nsingularity exec tools/_container/CONTAINER_ID.sif &lt;command&gt;\nWhen to use each method:\n\nMethod 1: Best for interactive work with graphical interfaces\nMethod 2: Good for running multiple commands or programs within the same container session\nMethod 3: Ideal for batch processing and automated scripts",
    "crumbs": [
      "Home",
      "Guides",
      "External Server"
    ]
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#stata",
    "href": "Guides/03_External_Server/External_Server_Guide.html#stata",
    "title": "External Server Guide",
    "section": "Stata",
    "text": "Stata\n\nStarting Stata\nUse the stata_container.sh launcher script in your project folder:\n\nGUI method: Click stata_container.sh in Dolphin\nTerminal method: Run ./stata_container.sh from your project folder\n\nManual container access:\nIf you need to manually access the Stata container:\ncd /bplimext/projects/P999_research_project\nsingularity shell tools/_container/CONTAINER_ID.sif\nThen launch Stata:\n\nGraphical version: xstata-mp\nCommand-line version: stata-mp\n\n\n\nAdo-files\nAdo-files are text files containing Stata programs. It is advisable to create and save your ado-files so results can be replicated later when running them on BPLIM datasets.\nStata looks for ado-files in several locations, typically organized as:\n\nSITE – system-wide ado-files\nPLUS – user-installed ado-files\nPERSONAL – user-created ado-files\nOLDPLACE – legacy location for ado-files\n\nStata always searches the current directory (.) and a set of predefined folders for ado-files. For BPLIM projects, ado-files provided by BPLIM are either built into the container or stored under /bplimext/projects/P999_research_project/tools. Ado-files that you create yourself should be saved in your project’s work_area (for example, in a dedicated ado/ subfolder).” Keep the adopath + \"/bplimext/projects/P999_research_project/tools\" code snippet immediately after this, as you already have.\nTo make sure Stata recognizes this directory, add the following line at the beginning of your .do file:\nadopath + \"/bplimext/projects/P999_research_project/tools\"\nThe sysdir command within Stata will list all directories currently in use:\n\n\n\n\n\n\nIn addition, the adopath command lists all directories where Stata searches for ado-files, including any paths you add yourself (such as the project tools directory). Whereas sysdir shows the base system directories (SITE, PLUS, PERSONAL, OLDPLACE), adopath displays the full search path.\n\n\nTemporary Files\nTo manage Stata’s temporary files:\n\nCheck the current temporary folder:\ntempfile junk\ndisplay \"`junk'\"\nIt should display something like /tmp/St98278.000001\nIf the temporary file is not in the path /tmp, exit Stata and edit your .bashrc in your home directory (cd ~):\n\n\nUsing Dolphin (GUI): In Dolphin, enable Show Hidden Files (or press Ctrl + H), then locate .bashrc and open it with KWrite.\n\n\nUsing the Terminal: Run kwrite ~/.bashrc or vi ~/.bashrc. If you use vi, press ESC, type :wq and press Enter to save and exit (see Using the vi File Editor for more details).”\n\nexport STATATMP=\"/tmp\"\n\nApply the changes:\nsource .bashrc\nStart a new Stata session (inside the container).\n\n\n\nRunning Stata in Batch Mode\nBatch mode is the recommended method for long-running or computationally intensive programs.\nFor the full workflow, see Running Programs in Batch Mode. In most cases your batch command will look like:\nstata-mp do /bplimext/projects/P999_research_project/work_area/prog1.do\n\n\nRunning Stata with Screen\nTo run Stata interactively (not in batch mode) while preserving your session in the Terminal if the connection drops, use screen. For long-running interactive work in the Terminal, using screen is strongly recommended. See Using Screen for Persistent Sessions for detailed instructions.",
    "crumbs": [
      "Home",
      "Guides",
      "External Server"
    ]
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#r",
    "href": "Guides/03_External_Server/External_Server_Guide.html#r",
    "title": "External Server Guide",
    "section": "R",
    "text": "R\n\nStarting R/RStudio\nUse the r_container.sh launcher script in your project folder:\n\nGUI method: Click r_container.sh in Dolphin\nTerminal method: Run ./r_container.sh from your project folder\n\nThis launches RStudio inside the container environment.\nManual container access:\nIf you need to manually access the R container:\ncd /bplimext/projects/P999_research_project\nsingularity shell tools/_container/CONTAINER_ID.sif\nrstudio\nIMPORTANT: When exiting R, do not save your workspace image to your home folder. If you need to preserve your workspace, save it in your project’s work_area folder.\n\n\nRunning R in Batch Mode\nBatch mode is recommended for long-running R scripts. For the full workflow, see Running Programs in Batch Mode. In most cases your batch command will look like:\nRscript /bplimext/projects/P999_research_project/work_area/analysis.R &gt; analysis.log 2&gt;&1\n\n\nRunning R with Screen\nTo run R interactively (not in batch mode) while preserving your session in the Terminal if the connection drops, use screen. For long-running interactive work in the Terminal, using screen is strongly recommended. See Using Screen for Persistent Sessions for detailed instructions.",
    "crumbs": [
      "Home",
      "Guides",
      "External Server"
    ]
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#python",
    "href": "Guides/03_External_Server/External_Server_Guide.html#python",
    "title": "External Server Guide",
    "section": "Python",
    "text": "Python\nStarting Python/Jupyter\nUse the python_container.sh launcher script in your project folder:\n\nGUI method: Click python_container.sh in Dolphin\nTerminal method: Run ./python_container.sh from your project folder\n\nManual container access:\ncd /bplimext/projects/P999_research_project\nsingularity shell tools/_container/CONTAINER_ID.sif\nOnce inside the container, launch Jupyter Notebook:\njupyter notebook\nThis opens Jupyter in Firefox. Click New and select the Python kernel to create a notebook.\nYou can also use VSCode:\nvscode",
    "crumbs": [
      "Home",
      "Guides",
      "External Server"
    ]
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#julia",
    "href": "Guides/03_External_Server/External_Server_Guide.html#julia",
    "title": "External Server Guide",
    "section": "Julia",
    "text": "Julia\nStarting Julia\nUse the julia_container.sh launcher script in your project folder:\n\nGUI method: Click julia_container.sh in Dolphin\nTerminal method: Run ./julia_container.sh from your project folder\n\nManual container access:\ncd /bplimext/projects/P999_research_project\nsingularity shell tools/_container/CONTAINER_ID.sif\nOnce inside the container, you can launch:\n\nJulia REPL: julia\nJupyter Notebook: jupyter notebook (opens in Firefox; select the Julia kernel)\nVSCode: vscode (opens the BPLIM-configured VSCode environment)",
    "crumbs": [
      "Home",
      "Guides",
      "External Server"
    ]
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#structuring-and-writing-code",
    "href": "Guides/03_External_Server/External_Server_Guide.html#structuring-and-writing-code",
    "title": "External Server Guide",
    "section": "Structuring and Writing Code",
    "text": "Structuring and Writing Code\nGoal: keep code readable, reproducible, and easy to hand over.\n\nOrganize your project\n\nWork in work_area; folders initial_dataset, external, intermediate, toolsand modified are read-only.\nUse the macros/variables defined in configuration files (profile.do, config.R, config.py) - path_source, path_source_p, path_rep, etc.\nKeep scripts in a dedicated folder (e.g., work_area/scripts), and store outputs in a results folder under the work_area (check the template).\nDo not save anything in your home folder.\n\n\n\nNaming and layout\n\nUse short, descriptive names with underscores (e.g., 01_import.do, 02_clean.R, 03_analysis.py).\nUse YYYYMMDD for dated files (e.g., analysis_20250312.log).\nAdd a brief README in work_area explaining the script order and entry points.\n\n\n\nPaths and reproducibility\n\nUse paths defined in the configuration files or prefer relative paths from the work_area instead of hard-coded absolute paths or home-relative paths.\nSet seeds for random routines (Stata: set seed, R: set.seed(), Python: random.seed()/numpy.random.seed()).\nUse the provided launcher scripts (stata_container.sh, r_container.sh, python_container.sh, julia_container.sh) to ensure you are inside the correct container.\n\n\n\nAdo-files and packages\n\nSave your own ado-files in work_area/ado/ (or similar) and add it to the search path, for example:\nrun profile.do\n...\nadopath + “${path_rep}/ado”\nPackages or commands not already available must be requested from the BPLIM Team.\n\n\n\nBatch and logging\n\nFor long runs, use batch mode (see Running Programs in Batch Mode); redirect output to a dated log in results/logs (e.g., stata-mp do ... &gt; results/logs/run_20250312.log).\nKeep batch scripts (e.g., batch_run1) alongside the code they execute, and reference them from the batch section.\n\n\n\nEditors and tools\n\nUse the provided wrappers (vscode for VSCode) or the software-specific editors inside each container.\nIf editing via GUI, remember to show hidden files when needed (e.g., .bashrc); if using vi, see Using the vi File Editor.\nTrack code with GitLab (see Version Control with GitLab) and commit regularly with clear messages.\n\n\n\nData handling\n\nNever copy data to your home folder.\nKeep only essential intermediate files; clean temporary artifacts in work_area to manage space.\nPlace final, non-sensitive outputs in results in line with the output extraction rules.",
    "crumbs": [
      "Home",
      "Guides",
      "External Server"
    ]
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#updates-to-commands-and-packages",
    "href": "Guides/03_External_Server/External_Server_Guide.html#updates-to-commands-and-packages",
    "title": "External Server Guide",
    "section": "Updates to Commands and Packages",
    "text": "Updates to Commands and Packages\nRequests for additional commands or packages, as well as updates to existing ones, must be submitted to the BPLIM Team.",
    "crumbs": [
      "Home",
      "Guides",
      "External Server"
    ]
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#build-a-container-to-fine-tune-your-statistical-packages",
    "href": "Guides/03_External_Server/External_Server_Guide.html#build-a-container-to-fine-tune-your-statistical-packages",
    "title": "External Server Guide",
    "section": "Build a Container to Fine-Tune Your Statistical Packages",
    "text": "Build a Container to Fine-Tune Your Statistical Packages\nThe server uses Apptainer (formerly Singularity) containers. To request one, please send the BPLIM Team the definition file. We will build the image and place it in your project’s tools. Detailed information about Apptainer/Singularity containers is available at https://sylabs.io/.6 Additional notes are provided in the Appendix.",
    "crumbs": [
      "Home",
      "Guides",
      "External Server"
    ]
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#why-use-batch-mode",
    "href": "Guides/03_External_Server/External_Server_Guide.html#why-use-batch-mode",
    "title": "External Server Guide",
    "section": "Why Use Batch Mode?",
    "text": "Why Use Batch Mode?\n\nEfficiency: Frees up your interactive session\nReliability: Programs continue running even if you disconnect\nResource management: Better server performance for all users\nRecommended for: Any program that runs longer than 30 minutes",
    "crumbs": [
      "Home",
      "Guides",
      "External Server"
    ]
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#basic-batch-workflow",
    "href": "Guides/03_External_Server/External_Server_Guide.html#basic-batch-workflow",
    "title": "External Server Guide",
    "section": "Basic Batch Workflow",
    "text": "Basic Batch Workflow\n\nNavigate to your working folder:\ncd /bplimext/projects/P999_research_project/work_area/\nPrepare your analysis script and batch file:\n\nFirst, create or reuse a Stata do-file in your work_area, for example prog1.do.\nThen create a batch script file (plain text), for example batch_prog1, containing the command to execute your do-file.\n\nExample for Stata (batch_prog1):\nstata-mp do /bplimext/projects/P999_research_project/work_area/prog1.do\nExample for R (batch_r_analysis):\nRscript /bplimext/projects/P999_research_project/work_area/analysis.R /\n  analysis.log 2&gt;&1\nEnter the container environment:\nsingularity shell ../tools/_container/CONTAINER_ID.sif\nSubmit the batch job:\nat now -f batch_prog1",
    "crumbs": [
      "Home",
      "Guides",
      "External Server"
    ]
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#scheduling-jobs",
    "href": "Guides/03_External_Server/External_Server_Guide.html#scheduling-jobs",
    "title": "External Server Guide",
    "section": "Scheduling Jobs",
    "text": "Scheduling Jobs\nThe at command allows you to schedule jobs:\n\nRun immediately: at now -f batch_script\nRun in 5 hours: at now + 5 hours -f batch_script\nRun in 30 minutes: at now + 30 minutes -f batch_script\n\nFor more options, type man at in the Terminal.",
    "crumbs": [
      "Home",
      "Guides",
      "External Server"
    ]
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#managing-batch-jobs",
    "href": "Guides/03_External_Server/External_Server_Guide.html#managing-batch-jobs",
    "title": "External Server Guide",
    "section": "Managing Batch Jobs",
    "text": "Managing Batch Jobs\nView queued/running jobs:\natq\n\n= indicates the job is currently running\na indicates the job is queued with its scheduled time\n\nRemove a job from the queue:\natrm &lt;job_number&gt;",
    "crumbs": [
      "Home",
      "Guides",
      "External Server"
    ]
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#monitoring-running-programs",
    "href": "Guides/03_External_Server/External_Server_Guide.html#monitoring-running-programs",
    "title": "External Server Guide",
    "section": "Monitoring Running Programs",
    "text": "Monitoring Running Programs\n\nUsing top\ntop\n\nPress i to hide background processes\nPress k to kill a process (enter PID, then type 9 to force termination)\nPress q to exit\n\n\n\nUsing tail to monitor log files\ntail -f logfile.log\nThis continuously displays new lines as they are written. Press CTRL + C to stop monitoring.",
    "crumbs": [
      "Home",
      "Guides",
      "External Server"
    ]
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#basic-screen-usage",
    "href": "Guides/03_External_Server/External_Server_Guide.html#basic-screen-usage",
    "title": "External Server Guide",
    "section": "Basic Screen Usage",
    "text": "Basic Screen Usage\nStart a new screen session:\nscreen -S my_session_name\nDetach from a screen session (keeps it running):\nPress CTRL + A, then D\nList all running screen sessions:\nscreen -ls\nReattach to a screen session:\nscreen -r my_session_name\nOr, if only one session exists:\nscreen -r\nReattach to a specific session by PID:\nscreen -r &lt;pid&gt;",
    "crumbs": [
      "Home",
      "Guides",
      "External Server"
    ]
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#example-running-stata-with-screen",
    "href": "Guides/03_External_Server/External_Server_Guide.html#example-running-stata-with-screen",
    "title": "External Server Guide",
    "section": "Example: Running Stata with Screen",
    "text": "Example: Running Stata with Screen\nscreen -S stata_session\nsingularity shell tools/_container/CONTAINER_ID.sif\nstata-mp",
    "crumbs": [
      "Home",
      "Guides",
      "External Server"
    ]
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#example-running-r-with-screen",
    "href": "Guides/03_External_Server/External_Server_Guide.html#example-running-r-with-screen",
    "title": "External Server Guide",
    "section": "Example: Running R with Screen",
    "text": "Example: Running R with Screen\nscreen -S r_session\nsingularity shell tools/_container/CONTAINER_ID.sif\nTo detach from the session while keeping R running, press CTRL + A, then D. To return later, run:screen -r stata_session`",
    "crumbs": [
      "Home",
      "Guides",
      "External Server"
    ]
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#when-to-use-screen-vs-batch-mode",
    "href": "Guides/03_External_Server/External_Server_Guide.html#when-to-use-screen-vs-batch-mode",
    "title": "External Server Guide",
    "section": "When to Use Screen vs Batch Mode",
    "text": "When to Use Screen vs Batch Mode\n\nUse screen: For interactive work where you need to see results immediately and may want to modify your approach\nUse batch mode: For fully scripted analyses that don’t require interaction",
    "crumbs": [
      "Home",
      "Guides",
      "External Server"
    ]
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#projects-using-modified-data",
    "href": "Guides/03_External_Server/External_Server_Guide.html#projects-using-modified-data",
    "title": "External Server Guide",
    "section": "Projects Using Modified Data",
    "text": "Projects Using Modified Data\nIf your project uses modified data provided by BPLIM:\n\nRun the replication app successfully before requesting outputs. See the Replication App manual for instructions.\nSend an email to bplim@bportugal.pt with the subject line:\nSubject: P999_research_project: request replication\nReplace P999_research_project with your actual project ID.",
    "crumbs": [
      "Home",
      "Guides",
      "External Server"
    ]
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#projects-not-using-modified-data",
    "href": "Guides/03_External_Server/External_Server_Guide.html#projects-not-using-modified-data",
    "title": "External Server Guide",
    "section": "Projects NOT Using Modified Data",
    "text": "Projects NOT Using Modified Data\nIf your project does not use modified data:\n\nPlace all outputs in the results folder within your project.7\nSend an email to bplim@bportugal.pt with the subject line:\nSubject: P999_research_project: request for result extraction\nReplace P999_research_project with your actual project ID.",
    "crumbs": [
      "Home",
      "Guides",
      "External Server"
    ]
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#shell_commands",
    "href": "Guides/03_External_Server/External_Server_Guide.html#shell_commands",
    "title": "External Server Guide",
    "section": "Basic Shell Commands on Linux",
    "text": "Basic Shell Commands on Linux\n\ntop: List processes currently running on the server\n\nPress i to hide background processes.\n\nPress h to display the help menu for available options.\n\npwd: Show the current working directory\ncd: Change directory\n  cd /bplimext/projects/PXXX_name/work_area/\ncd ~: Move to your home folder\ncp: Copy file(s) to a given path\n  cp prog1.do /bplimext/projects/PXXX_name/results\nmv: Move file(s) or rename file(s)\n  mv prog1.do /bplimext/projects/PXXX_name/results\nrm: Delete a file\n  rm /bplimext/projects/PXXX_name/results/prog1.do\nmkdir: Create a directory\n  mkdir programs\nrmdir: Delete an empty directory\n  rmdir programs\nscreen: Start a session manager that allows running programs in the background and resuming them later\n  screen top\nman: Show the manual page for a given command\n  man ls\ndu -h: Display disk usage of files and directories in human-readable format\n  du /bplimext/projects/PXXX_name/work_area/\ndf -h: Show disk space utilization in human-readable format\nvi: View or edit ASCII text files (e.g., .do files, logs)\nghostscript: Preview files with .eps or .pdf extensions\n  ghostscript /bplimext/projects/PXXX_name/results/file_name.pdf\nokular: View PDF files\nfind: Search for files\n\nBasic structure: find /path options pattern\n\n  find . -name \"*.do\"\n\nSave search results to a file:\n\n  find . -name \"*.do\" &gt; find_results.txt\n\nSearch for a string within filenames:\n\n  find . -name \"*.do\" | grep \"analysis\"\n\nIdentify .do files containing the word graph export:\n\n  find . -name \"*.do\" -exec grep \"graph export\" '{}' \\; -print\npasswd: Change your password\nExit a program: Press CTRL + C to terminate the current process in the shell",
    "crumbs": [
      "Home",
      "Guides",
      "External Server"
    ]
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#using-the-vi-file-editor",
    "href": "Guides/03_External_Server/External_Server_Guide.html#using-the-vi-file-editor",
    "title": "External Server Guide",
    "section": "Using the vi File Editor",
    "text": "Using the vi File Editor\n\nOpen a file in vi from the shell, for example:\n vi batch1.txt\nCommon shortcut keys in vi:\n\ni: Insert text\nESC: Exit insert mode\nx: Delete the character under the cursor\ndd: Delete the current line\n10 dd: Delete 10 lines\nyy: Copy (yank) the current line\np: Paste the copied (yanked) text\nSHIFT + G: Go to the last line\ngg: Go to the first line\nESC + :q!: Quit without saving changes\nESC + :w!: Write (save) and overwrite the file\nESC + :q: Quit if no changes have been made\n\nFor a more complete guide, see: https://www.cs.colostate.edu/helpdocs/vi.html\nEasier alternative: use the gedit text editor for a graphical interface:\n gedit batch1.txt",
    "crumbs": [
      "Home",
      "Guides",
      "External Server"
    ]
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#password",
    "href": "Guides/03_External_Server/External_Server_Guide.html#password",
    "title": "External Server Guide",
    "section": "Password Requirements",
    "text": "Password Requirements",
    "crumbs": [
      "Home",
      "Guides",
      "External Server"
    ]
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#nomachine-frequently-asked-questions",
    "href": "Guides/03_External_Server/External_Server_Guide.html#nomachine-frequently-asked-questions",
    "title": "External Server Guide",
    "section": "NoMachine: Frequently Asked Questions",
    "text": "NoMachine: Frequently Asked Questions\n\nMac users cannot install NoMachine and receive the error below:\n\n\n\n\n\n\n\n\n\n\n\nEnsure your macOS is up to date.\n\nAs a temporary solution, download the NoMachine Enterprise Client from the official website and run the installation file.\n\n\nNoMachine Client\n\n\n\n\n\nNoMachine authentication failure\n\n\n\n\n\n\n\n\n\n\n\nThis may happen due to a mismatched keyboard layout.\nFor example, if you use a Portuguese keyboard but the system assumes a US keyboard, a password containing ç may be rejected as “wrong password.”\nVerify your keyboard layout or change your password after the first login using:\n passwd\nIf login fails with the error:\n“Could not connect to the server. Error is 138: Connection is timed out”\ncheck whether your network firewall is blocking the connection.\nSome university networks block external connections to BPLIM’s server.\nTest from another location (e.g., your home network).\n\n\nUser pressed ‘Lock’ instead of ‘Log out’ and cannot unlock\n\n\nCheck that the keyboard layout is correct (e.g., PT or UK).\n\nClose the NoMachine session and start a new one. Before the final Login step, right-click and choose Logout, then click to reconnect.\n\n\n\n\n\n“Cannot see the screen in NoMachine”\n\n\n\n\n\n\n\n\n\n\n\nOption A: Move your mouse to the top-right corner of NoMachine.\nA folded-sheet icon will appear. Left-click → Display → Change settings → enable Disable client-side hardware decoding.\n\n\n\n\n\n\n\n\n\n\n\nOption B: Close the NoMachine connection and start a new one. Before the final Login step, right-click and choose Logout, then click to reconnect.\n\n\n\n\n\n\n“Error: Parameter ‘agentm_display’ has bad value”\n\n\n\n\n\n\n\n\n\n\n\nThis usually means your home folder is full (/home/USER_LOGIN).\nDo not save files in your home folder.\n\nAsk the BPLIM Team to free up space in your home directory.\n\n\n\n\n\nSession is frozen\n\n\nFrom the first NoMachine screen, click the following icon:\n\n\n\n\n\n\n\n\n\n\n\nThen right-click the icon below and choose Terminar sessão:",
    "crumbs": [
      "Home",
      "Guides",
      "External Server"
    ]
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#install_nomachine",
    "href": "Guides/03_External_Server/External_Server_Guide.html#install_nomachine",
    "title": "External Server Guide",
    "section": "Download, Install and Configure NoMachine Client",
    "text": "Download, Install and Configure NoMachine Client\nStep 1: Go to the following link and use the credentials provided by BPLIM to access the site:\n\nBanco de Portugal Webdrive\n\n\nNote: sometimes the internet provider, e.g., a University, may block access to this particular website. Please check with your provider in case you get an error while trying to use the link.\n\n\n\n\n\n\n\n\n\n\n\nStep 2: Download the file with an extension compatible with your OS (Operating System).\n\n\n\n\n\n\n\n\n\n\nStep 3: Install ‘NoMachine’.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 4: Reboot your computer\n\n\n\n\n\n\n\n\n\nStep 5: NoMachine client access configuration.\n\nStep 5.1: Start ‘NoMachine’ and create a new connection.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 5.2: Define the ‘Host’ as bplimexterno.bportugal.pt, ‘Port’ 4000, ‘Protocol’ NX and set a ‘Friendly Name’ for ‘Name’.\n\n\n\n\n\n\n\n\n\n\n\nStep 5.3: Use password authentication – with or without a proxy – according to the instructions provided by your network administrator or IT support. The proxy settings can be customized under Proxy in the bottom-right corner. After completing the configurations, click Add to create the connection.\n\n\n\n\n\n\n\n\n\n\n\nStep 5.4: Once the entry for bplimexterno.bportugal.pt has been created, connect:\n\n\n\n\n\n\n\n\nStep 5.5: Before the first effective connection, it may be necessary to accept the certificate from bplimexterno.bportugal.pt. You should verify that the \"fingerprint\" (verification code) is:\n\n\n\n\n\nSHA256 ED 1B D9 E2 C2 F8 C6 08 1A 53 5F 97 DA 71 77 D9 D2 EE 7A 5F 9C 35 87 B3 19 F4 7E A1 CB 2C 68 0B\n\n\n\n\n\n\n\n\n\n\n\n\nStep 5.6: Connect with the UserID (case sensitive) and password provided by Banco de Portugal:\n\n\n\n\n\n\n\n\n\n\n\nStep 5.7: After the first successful login, it is necessary to change the password, which must comply with the Password Policy defined Section 1.2.\n\n\n\n\n\n\n\n\n\n\n\nIf the new password does not comply with the Password Policy, the original password provided by the Banco de Portugal will be re-requested. You get the message “Authentication failed, please try again.” See Appendix 3 for details.\n\n\n\n\n\n\n\n\n\n\n\nThe NoMachine client does not tell you why the new password was not accepted – it is the responsibility of the user to verify that the new password is in compliance.\n\n\n\n\n\nStep 5.8: Upon login success, the following screens should appear.\n\n\n\n\n\n\n\n\n\n\n\nCreate a new desktop.\n\n\n\n\n\n\n\n\n\n\n\n\nStep 5.9: In the following screen define the settings of your monitor.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 5.10: Upon login success, the following screens should appear.\n\n\n\n\n\nOnce logged in and with access to a KDE session, click on the upper right corner of the KDE desktop, as shown below, to access the menu and then expand the screen as exemplified for greater ease of use.\n\n\n\n\n\n\n\n\n\n\n\n\nStep 5.11: You should see the following screen.\n\n\n\n\n\n\n\n\n\n\n\n\nStep 5.12: Click ‘Display’.\n\n\n\n\n\n\n\n\n\n\n\nStep 5.13: Choose the option that best fits your monitor.\n\n\n\n\n\n\n\n\n\n\n\nTip: At any time during a NoMachine session you can press Ctrl + Alt + 0 to open the NoMachine menu. From there, select Display → Change settings to adjust the resolution or scaling mode (for example, Fit to window).",
    "crumbs": [
      "Home",
      "Guides",
      "External Server"
    ]
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#version-control-with-gitlab",
    "href": "Guides/03_External_Server/External_Server_Guide.html#version-control-with-gitlab",
    "title": "External Server Guide",
    "section": "Version Control with GitLab",
    "text": "Version Control with GitLab\nThe server provides GitLab for version control. Git is a distributed version-control system for tracking changes in files, ideal for managing code and scripts across your research project.\nTo request Git access: Contact the BPLIM Team at bplim@bportugal.pt.\n\nBenefits of Using Version Control\n\nTrack all changes to your code and scripts\nRevert to previous versions if needed\nCollaborate with team members\nMaintain a complete history of your research workflow\n\n\n\nGetting Started with Git\n\nGenerate an SSH Key\nOpen a Terminal in your home folder and generate an SSH key:\ncd ~\nssh-keygen -t rsa -C \"BPLIM git\"\ncat ~/.ssh/id_rsa.pub\nHighlight the generated key, right-click, and select Copy to copy it to your clipboard.\n\n\nAccess GitLab\nOpen Firefox (Red Hat → Search → Firefox) and navigate to:\nhttps://vxpp-bplimgit.bplim.local/\n\n\n\n\n\n\n\n\n\nLog in with your external server credentials.\n\n\nAdd Your SSH Key in GitLab\n\nNavigate to your profile by clicking Settings in the top-right corner\n\n\n\n\n\n\n\n\n\n\n\nIn the left sidebar, click SSH Keys\n\n\n\n\n\n\n\n\n\n\n\nPaste your SSH key in the Key text box\n\n\n\n\n\n\n\n\n\n\n\nEnter a title (e.g., “BPLIM git”) and click Add key\n\n\n\nCreate a New GitLab Project\nGo to Projects → New project and create a repository (e.g., scripts_P999).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConfigure Git\nCreate or edit the .gitconfig file in your home folder. Use KWrite (Red Hat → Search → KWrite):\n[cola]\n        spellcheck = false\n[user]\n        name = Your Name\n        email = your_username@sxpe-bplim01.bplim.local\n[gui]\n        editor = kwrite\n\n\nClone Your Project\nIn the Terminal, navigate to your work_area and clone the repository:\ncd /bplimext/projects/P999_research_project/work_area/\ngit clone git@vxpp-bplimgit.bplim.local:username/scripts_P999.git\n\n\nAdd .gitignore File\nCopy the .gitignore template from your project’s tools folder:\ncd scripts_P999\ncp /bplimext/projects/P999_research_project/tools/.gitignore .\n\n\nMake Your First Commit\ngit add *\ngit commit -a -m \"Initial commit\"\ngit push\n\n\nBest Practices\n\nStore all your scripts and code in the Git repository folder (e.g., scripts_P999)\nCommit changes regularly with descriptive messages\nPull before you push to avoid conflicts\nUse branches for experimental work",
    "crumbs": [
      "Home",
      "Guides",
      "External Server"
    ]
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#building-custom-containers",
    "href": "Guides/03_External_Server/External_Server_Guide.html#building-custom-containers",
    "title": "External Server Guide",
    "section": "Building Custom Containers",
    "text": "Building Custom Containers\nIf you need custom software packages or specific versions, you can request a custom container.\n\nSteps to Build a Custom Container\n\nCreate a Container Definition\nUse the template files available in the BPLIM Containers GitHub repository.\n\n\nTest and Build Using Sylabs Cloud\n\nSign in to Sylabs Cloud (use your GitHub account)\nClick CREATE:\n\n\n\n\n\n\n\n\n\n\n\nUpload your .def file or paste its contents:\n\n\n\n\n\n\n\n\n\n\n\nSylabs validates your script. Once successful, click Build\nMonitor the build process for any errors\nAfter successful build, send the definition file to the BPLIM Team\n\n\n\nUsing Your Custom Container\nOnce the BPLIM Team builds your container, it will be placed in your project’s tools/_container folder.\nTo use it:\ncd /bplimext/projects/P999_research_project/tools/_container\nsingularity shell YOUR_CONTAINER_ID.sif\nThe Terminal prompt changes to Singularity&gt;, indicating you’re inside the container. You now have access to your custom software environment.\n\n\n\n\n\n\n\n\n\nLaunch applications as needed (e.g., rstudio for RStudio).",
    "crumbs": [
      "Home",
      "Guides",
      "External Server"
    ]
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#jupyter-lab",
    "href": "Guides/03_External_Server/External_Server_Guide.html#jupyter-lab",
    "title": "External Server Guide",
    "section": "Jupyter Lab",
    "text": "Jupyter Lab\nJupyterLab is a web-based interactive development environment for notebooks, code, and data. It provides a flexible interface for data science, scientific computing, and machine learning workflows.\n\nStarting JupyterLab\nFrom within a container (Python or Julia), run:\njupyter lab --browser=firefox\nThis opens JupyterLab in Firefox, providing an integrated environment for:\n\nInteractive notebooks (Python, Julia, R)\nCode editing and execution\nData visualization\nTerminal access\n\n\n\nExample JupyterLab Session\n\n\n\n\n\n\n\n\n\nTip: JupyterLab is ideal for exploratory data analysis and prototyping. For production scripts, consider using dedicated .py, .R, or .jl files.",
    "crumbs": [
      "Home",
      "Guides",
      "External Server"
    ]
  },
  {
    "objectID": "Guides/03_External_Server/External_Server_Guide.html#footnotes",
    "href": "Guides/03_External_Server/External_Server_Guide.html#footnotes",
    "title": "External Server Guide",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDolphin is the file manager included with the KDE desktop environment. You can browse folders, create/delete files and folders (right-click for context menu), and manage your project files. More information: https://userbase.kde.org/Dolphin↩︎\nContainers are self-contained environments that include software and all its dependencies (libraries, configurations, packages). This ensures consistent behavior across sessions and enables reproducible research. Each container is isolated, preventing conflicts between different projects’ software requirements.↩︎\nClick the cross button in the upper-right corner of the NoMachine window to close the connection.↩︎\nBefore logging out, ensure all active programs are closed (unless running in batch mode). Batch mode is recommended for computationally intensive or long-running tasks.↩︎\nContainers are self-contained environments that include software and all its dependencies (libraries, configurations, packages). This ensures consistent behavior across sessions and enables reproducible research. Each container is isolated, preventing conflicts between different projects’ software requirements.↩︎\nSingularity is now called Apptainer. Both names refer to the same container technology. Documentation: https://apptainer.org↩︎\nOutput extraction rules: Only non-sensitive text files without identifiable information can be extracted. For each graph requested, you must provide the corresponding data table for replication. Graphs must be in PNG format; vector graphics are not permitted.↩︎",
    "crumbs": [
      "Home",
      "Guides",
      "External Server"
    ]
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#what-are-the-characteristics-of-bplims-datasets",
    "href": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#what-are-the-characteristics-of-bplims-datasets",
    "title": "Guide for Researchers",
    "section": "2.1 What are the characteristics of BPLIM’s datasets?",
    "text": "2.1 What are the characteristics of BPLIM’s datasets?\nAll BPLIM datasets created for research purposes are stripped of elements that allow for direct identification of companies, banks, or individuals. Whenever possible, the datasets contain unique unit identifiers common across datasets: examples of these are tina – the tax identification number anonymized for companies – and bina – the bank identification number anonymized. \nBy default BPLIM datasets are made available in Stata format. Larger datasets may be made available in parquet format. Data is stored in an efficient way that minimizes file size and follows BPLIM’s naming convention. Labels are applied to all variables and value labels to all categorical variables. Whenever possible, labels can be displayed in Portuguese and English.\nAll datasets are accompanied by a Manual that contains all relevant information regarding the data. The data manuals, the metafiles1 and citation information for the different data extractions are available on GitHub. A metafile that contains additional descriptive statistics for each dataset can be obtained once researchers are given access to the server. Please refer to the External Server Guide on how to access this information. \nBPLIM datasets may also have companion script files that calculate additional variables or harmonized variables to guarantee comparability over time and across datasets. Datasets are updated regularly based on a data extraction (“data freeze”) at a specific point in time, and a versioning system is applied to reflect any changes to the data set. Most datasets have an associated Digital Object Identifier (DOI).",
    "crumbs": [
      "Home",
      "Guides",
      "Guide for Researchers"
    ]
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#what-data-are-available",
    "href": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#what-data-are-available",
    "title": "Guide for Researchers",
    "section": "2.2 What data are available?",
    "text": "2.2 What data are available?\nThe complete list of datasets, including a short description and the access conditions, is available in the BPLIM Datasets Guide. On BPLIM´s website you will find a list of the latest version of the datasets available for external researchers, along with a link to the respective documentation.",
    "crumbs": [
      "Home",
      "Guides",
      "Guide for Researchers"
    ]
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#who-can-gain-access-to-the-data",
    "href": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#who-can-gain-access-to-the-data",
    "title": "Guide for Researchers",
    "section": "3.1 Who can gain access to the data?",
    "text": "3.1 Who can gain access to the data?\nAccess is restricted to BPLIM accredited researchers who intend to utilize the data for scientific purposes. Individuals affiliated with BdP are classified as Internal Researchers and have unrestricted access to all datasets maintained by BPLIM. Those not affiliated with BdP are considered External Researchers, and their access is subject to several restrictions. BPLIM Datasets Guide summarizes the access restrictions for each dataset.",
    "crumbs": [
      "Home",
      "Guides",
      "Guide for Researchers"
    ]
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#how-can-researchers-request-access-to-the-data",
    "href": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#how-can-researchers-request-access-to-the-data",
    "title": "Guide for Researchers",
    "section": "3.2 How can researchers request access to the data?",
    "text": "3.2 How can researchers request access to the data?\nInternal researchers are provided access to a data repository maintained by BPLIM. The repository makes available data, which can be used unreservedly for research and policy activities without any formality. However, data made available strictly for policy should not be used for research because as it may have lower quality, are not documented, and may not be reproducible. If Internal Researchers are working with external co-authors or if they need to use or link other micro datasets not available in the data repository they will need to submit a project to BPLIM.\nExternal Researchers must always submit a project. The project proposal must: (1) contain a short description of the research project; (2) identify all participants involved in the project along with their affiliations, and include a curriculum vitae (CV) for each; and (3) specify the datasets, timeframe, and variables required. All external researchers with access to the data are required to sign a confidentiality agreement. If the project consists of a master or doctoral dissertation then the supervisor(s) has to be identified and must also sign the confidentiality agreement. BPLIM staff can collaborate with the researcher(s) to identify the required datasets and, if necessary, construct a customized dataset. A copy of the required documents can be found in BPLIM’s website.",
    "crumbs": [
      "Home",
      "Guides",
      "Guide for Researchers"
    ]
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#approval-process",
    "href": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#approval-process",
    "title": "Guide for Researchers",
    "section": "3.3 Approval process",
    "text": "3.3 Approval process\nUpon submission of all required documentation and verification that it conforms to BPLIM rules, the project will be evaluated to ensure that it addresses a legitimate research question. Compliance with the guidelines is crucial for recurring external researchers (those who have already participated in BPLIM projects). Once the project is approved, the researcher will receive an email notification with the user credentials and instructions for accessing the data. Summary information about the project and researchers will be posted on BPLIM’s website.",
    "crumbs": [
      "Home",
      "Guides",
      "Guide for Researchers"
    ]
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#how-can-the-data-be-accessed",
    "href": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#how-can-the-data-be-accessed",
    "title": "Guide for Researchers",
    "section": "3.4 How can the data be accessed?",
    "text": "3.4 How can the data be accessed?\nWhen applying for a project, researchers must specify if they plan to access an internal account, in Pitagoras, or an external account in the External Server.\nAccounts open at Pitagoras can only be accessed at the installations of BdP (“on-site access”) either at Lisbon or Porto. Internal researchers can log into Pitagoras from their terminal using their network login credentials. External researchers will be provided with a login and password for Pitagoras and granted access to a terminal where it is technically restricted to transfer, download, copy, paste, or print any data. BPLIM projects at Pitagoras are placed in a specific folder containing all projects, with users having access only to their designated project folder.\nFor more details on accessing BPLIM projects in Pitagoras, please refer to the BPLIM Pitagoras Manual. Due to a limited number of terminals available, external researchers must book their visits well in advance.\nIf the account is on the External Server, then the data must be accessed remotely (“remote access”) using a secure connection. BPLIM uses the NoMachine software for this purpose. With this connection, it is not possible to exchange files between the external server and the local computer. For more details on using the External Server, please refer to the External Server Guide.\nIn special circumstances explained below, the external researcher may be granted indirect access to the data (“surrogate access”, also known as, “remote execution”). With surrogate access, there is no need for the external researcher to have an account, as BPLIM staff (or an internal researcher) will act as a proxy for data access. This means that BPLIM staff (or an internal researcher) will execute the scripts written by the external researcher and share the outputs after disclosure control.",
    "crumbs": [
      "Home",
      "Guides",
      "Guide for Researchers"
    ]
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#what-type-of-anonymization-is-applied-to-bplims-datasets",
    "href": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#what-type-of-anonymization-is-applied-to-bplims-datasets",
    "title": "Guide for Researchers",
    "section": "3.5 What type of anonymization is applied to BPLIM’s datasets?",
    "text": "3.5 What type of anonymization is applied to BPLIM’s datasets?\nWhen BPLIM makes its datasets accessible to researchers, it uses several different strategies to anonymize the data. The type of anonymization depends on the specific data and the user. BPLIM uses four levels of anonymization:\n\nLevel 1 - All information that could lead to the direct identification of statistical units (firms/banks/individuals) is omitted, and unique identifiers (e.g., NIF, bank ID) undergo a 1-to-1 transformation to new identifiers that are specific to the project. Level 1 datasets will contain “_A_” in the name.\nLevel 2 - in addition to Level 1, the values of variables containing sensitive information will be replace by modified values, which are random values that exhibit some correlation with the original values. The file name of a Level 2 dataset will contain “_P_”, and the labels of the modified variables will reflect this information.\nLevel 3 - in addition to Level 2, variables may be sorted randomly and independently to break the link between the observations. Level 3 datasets will be identified with “_R_”.\nLevel 4 - a subset of the data is generated randomly (pseudo-data), respecting only the metadata and the time structure of the original data. Level 4 datasets will be identified with “_D_” in their name.\n\nDatasets of Level 2, 3, or 4 are generically designated as modified datasets.\nLevel 4 datasets are the only ones that researchers are allowed to use outside of the bank computing environment, because the values generated for this level are fictitious.",
    "crumbs": [
      "Home",
      "Guides",
      "Guide for Researchers"
    ]
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#what-determines-the-type-of-anonymization-applied-to-the-data-set",
    "href": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#what-determines-the-type-of-anonymization-applied-to-the-data-set",
    "title": "Guide for Researchers",
    "section": "3.6 What determines the type of anonymization applied to the data set?",
    "text": "3.6 What determines the type of anonymization applied to the data set?\nBPLIM data meant to be used by Internal researchers are always anonymized at Level 1. The exception is if the Internal Researcher is accessing the data through the External Server. In that case, Internals Researchers have the same access conditions as External Researchers.\nDatasets made available to External Researchers are subject to a confidentiality classification as follows: low, medium, or high. If the data are classified as low, then the data is anonymized at Level 1. Datasets classified as medium may be anonymized at Level 2 or 3, depending on the risk of identification. For datasets classified with a high level of confidentiality, External Researchers may only have access to Level 4 data.",
    "crumbs": [
      "Home",
      "Guides",
      "Guide for Researchers"
    ]
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#how-can-researchers-work-with-modified-data",
    "href": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#how-can-researchers-work-with-modified-data",
    "title": "Guide for Researchers",
    "section": "3.7 How can researchers work with modified data?",
    "text": "3.7 How can researchers work with modified data?\nModified datasets serve only the purpose of facilitating the creation of scripts that manipulate/analyze the data. Results of analysis performed on “modified” datasets are not valid for research purposes. However, external researchers can always request to have their scripts run on the original datasets. This rule applies whether the access is “on-site” or “remote”. Researchers working with modified data should use BPLIM’s Replication App. For instructions on how to use the Replication App, please refer to the Replication App User Guide. While not strictly enforced, use of the BPLIM’s Replication App, will ensure that the replication on the original data is implemented correctly and in a much more timely manner. We strongly encourage use of the BPLIM’s Replication App.\nResearchers working with Level 4 data (pseudo-data) in their personal computers, will receive a package along with instructions to create the pseudo-data. For instructions on how to work with pseudo-data, please refer to the BPLIM Guide to Working with Pseudo-Data.",
    "crumbs": [
      "Home",
      "Guides",
      "Guide for Researchers"
    ]
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#how-are-projects-involving-co-authorship-between-internal-researchers-and-external-researchers-handled",
    "href": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#how-are-projects-involving-co-authorship-between-internal-researchers-and-external-researchers-handled",
    "title": "Guide for Researchers",
    "section": "3.8 How are projects involving co-authorship between Internal Researchers and External Researchers handled?",
    "text": "3.8 How are projects involving co-authorship between Internal Researchers and External Researchers handled?\nProjects where internal and external researchers have access to the data are designated “mixed projects”.\nIf the mixed project is implemented in the External Server and only data with low level of confidentiality is used, then the distinction is irrelevant as all researchers are treated as external and the data is anonymized (Level 1).\nHowever, if the data needed for the project is classified at a higher confidentiality level, external researchers can only access modified data or, in the most restrictive cases, pseudo-data, but never the original data. In such mixed projects, where the external researcher does not have access to the original, the internal researcher is responsible for ensuring that the information shared with their external co-authors complies with the confidentiality requirements associated with the data.\nIn these cases, BPLIM will open a second “parallel” account with access only for the internal researcher(s) and place all the original (anonymized) data there. It will be the responsibility of the internal researcher to execute all scripts on the original data stored in this “parallel” account.\nFurthermore, it will be their responsibility to ensure that external researchers do not have any access to confidential data. Specifically, external co-authors must not access the project account containing the original data, the internal co-author’s desktop computer, or any logs that may contain confidential information.\nIn a mixed project all interaction with BPLIM should be done via the internal researcher.",
    "crumbs": [
      "Home",
      "Guides",
      "Guide for Researchers"
    ]
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#how-can-an-external-researcher-gain-surrogate-access-to-a-dataset",
    "href": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#how-can-an-external-researcher-gain-surrogate-access-to-a-dataset",
    "title": "Guide for Researchers",
    "section": "3.9 How can an External Researcher gain surrogate access to a dataset?",
    "text": "3.9 How can an External Researcher gain surrogate access to a dataset?\nThe BPLIM Dataset Guide lists the datasets that can be accessed in surrogate mode by an external researcher that does not have an internal co-author. In that case the external researcher needs to submit a detailed project to BPLIM. The project will be evaluated according to the relevance of the topic to the research agenda of BdP. Only projects deemed relevant will be granted surrogate access. If BPLIM decides to support the project, external researchers will be assigned a data expert at BPLIM, who will collaborate with them to prepare and run scripts on the original data. However, the coding itself remains the ultimate responsibility of the external researcher, and BPLIM will not validate or certify the scripts written by the researcher. External researchers are encouraged to work closely with BPLIM staff to ensure they achieve the intended results and are also encouraged to discuss their findings with BPLIM staff. It is highly recommended that external researchers initiate their project with a short-stay visit in BPLIM, during which time they can discuss their research with BPLIM staff and gain a thorough understanding of the data complexities. Additional visits throughout the project are also encouraged. All outputs shared with the external researcher are subject to the usual disclosure restrictions.",
    "crumbs": [
      "Home",
      "Guides",
      "Guide for Researchers"
    ]
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#transfer-of-external-files",
    "href": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#transfer-of-external-files",
    "title": "Guide for Researchers",
    "section": "3.10 Transfer of external files",
    "text": "3.10 Transfer of external files\nInternal researchers working in Pitagoras can freely copy files to and from their accounts. Thus, they are free to place external files in their Pitagoras accounts. However, if the external data needs to be merged with BPLIM datasets using an anonymized key, then the internal researcher must be working in a BPLIM project account at Pitagoras. They will also need to fill in an application for using an external dataset (see below). BPLIM will anonymize the external datasets using the same linking key as the one used for the BPLIM datasets. Note that BPLIM identifiers (eg: tina and bina) are specific to a project and are not valid to link files exchanged between accounts.\nIf the account is shared with external researchers - a “mixed project” - the internal researcher must ensure that external researchers are allowed access to the external dataset and that they do not gain undue access to confidential data. At the request of the internal researcher, BPLIM will anonymize/modify the external datasets intended for use in “mixed projects”.\nExternal researchers may also request that external data files be placed in their accounts. BPLIM staff will assist if there is a need to merge external datasets with BPLIM datasets. External datasets typically contain aggregated data, but it may be possible to add external datasets with finer granularity. BPLIM staff will assess if the addition of the external datasets increases the risk of identification of individual observations. In such cases, additional measures will be undertaken to ensure that the confidentiality of the data is preserved once external files are merged with existing BPLIM datasets. These situations will be analyzed case by case and discussed with BPLIM staff.\nAll external datasets provided to BPLIM should be in a Stata or CSV format and an External Datasets Form must be filled in. In the form, researchers are required to explain the data provenance, provide a justification for its use, and identify the key variables that enable linking the external dataset with BPLIM’s datasets. The researcher must also certify that all researchers with access to the account are authorized to use the data. It is the responsibility of the researcher to ensure the external files can be legitimately used for that purpose.",
    "crumbs": [
      "Home",
      "Guides",
      "Guide for Researchers"
    ]
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#statistical-software-1",
    "href": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#statistical-software-1",
    "title": "Guide for Researchers",
    "section": "4.1 Statistical Software",
    "text": "4.1 Statistical Software\nWhen researchers apply for a new project, they will need to specify the software to be used. Available options are Stata, R, Julia, and Python. BPLIM provides the researcher with a default list of external packages/ados for each software. If researchers require additional packages, they must specify the package, its source, and version.\nFor each project, BPLIM creates a container with the software and packages required for the project. If researchers require additional external packages during the project, they should send a request via email to BPLIM.\nOnce the project account is set up, researchers will have access to a Linux environment where they can use the container. Templates for writing code are also provided in the account, and researchers should strive to adhere to the conventions outlined in these templates as much as possible.",
    "crumbs": [
      "Home",
      "Guides",
      "Guide for Researchers"
    ]
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#bplim-tools",
    "href": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#bplim-tools",
    "title": "Guide for Researchers",
    "section": "4.2 BPLIM Tools",
    "text": "4.2 BPLIM Tools\nBPLIM staff has developed several Stata packages to assist researchers in their tasks. Some of these tools are tailored for use with BPLIM datasets, while others have broader utility. To promote transparency in coding and versioning, BPLIM makes all tools available on Github. Users are welcome to suggest improvements or add their own contributions. Tools with general applicability can be installed directly from Github on any internet-connected machine.",
    "crumbs": [
      "Home",
      "Guides",
      "Guide for Researchers"
    ]
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#can-researchers-transfer-files-from-the-server",
    "href": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#can-researchers-transfer-files-from-the-server",
    "title": "Guide for Researchers",
    "section": "5.1 Can researchers transfer files from the server?",
    "text": "5.1 Can researchers transfer files from the server?\nExternal researchers are never allowed to transfer files to/from BPLIM’s accounts. This policy also applies to internal researchers accessing the external server. However, internal researchers have the flexibility to transfer files to and from their accounts in Pitagoras, including data and output logs.\nWhen internal researchers collaborate with external co-authors, it becomes their responsibility to ensure that all files shared with external co-authors comply with BPLIM’s data security and confidentiality policies. This ensures that no sensitive or restricted information is improperly disseminated.\nFor further details, please refer to the Output Control Guide.",
    "crumbs": [
      "Home",
      "Guides",
      "Guide for Researchers"
    ]
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#what-restrictions-apply-to-the-release-of-output-logs",
    "href": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#what-restrictions-apply-to-the-release-of-output-logs",
    "title": "Guide for Researchers",
    "section": "5.2 What restrictions apply to the release of output logs?",
    "text": "5.2 What restrictions apply to the release of output logs?\nAs a general principle, BPLIM will not verify the “correctness” of scripts used by researchers to generate logs and expressly disclaims responsibility for any errors or inaccuracies in researchers’ code. This responsibility solely rests with the researcher.\nOutput logs should never contain information that discloses individual dataset records; they should only include aggregate-level information. Therefore, listings of individual records, tables with cells derived from manipulation of three or fewer observations, statistical measures with standard errors of zero, minimum and maximum values, etc., are prohibited.\nPlain text files (including Latex and comma or tab separated values) are the preferable formats, although other formats may be accepted provided that the data content can be easily verified. Graphical outputs should be generated in “.png” format.\nIn mixed projects, the internal researcher is responsible for ensuring adherence to these principles. BPLIM will assist in this process upon request from the internal researcher. For projects involving only external researchers, BPLIM staff will verify the conformity of all outputs.\nOutput disclosure control depends on staff availability and may take longer in periods of high workload. BPLIM staff will only answer requests for output extraction sent by email and will take the necessary time to ensure that all confidentiality requirements are safeguarded. Researchers should keep their output requests to a minimum and, whenever feasible, these requests should be of final outputs.\nFor further details, please refer to the Output Control Guide.",
    "crumbs": [
      "Home",
      "Guides",
      "Guide for Researchers"
    ]
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#what-happens-if-the-researcher-violates-the-rules",
    "href": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#what-happens-if-the-researcher-violates-the-rules",
    "title": "Guide for Researchers",
    "section": "5.3 What happens if the researcher violates the rules?",
    "text": "5.3 What happens if the researcher violates the rules?\nBPLIM assumes that all researchers operate in good faith and should endeavor to adhere to the provisions outlined in the signed “Declaration on Confidentiality and Use of Data”. In cases where a researcher engages in behavior deemed inappropriate, BPLIM reserves the right to terminate or suspend all projects involving the researcher and the institution to which he/she is affiliated.",
    "crumbs": [
      "Home",
      "Guides",
      "Guide for Researchers"
    ]
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#replicability-of-work",
    "href": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#replicability-of-work",
    "title": "Guide for Researchers",
    "section": "6.1 Replicability of work",
    "text": "6.1 Replicability of work\nResearchers working at BPLIM have all the necessary conditions to ensure that their work is reproducible. All BPLIM datasets are versioned and can be exactly recreated based on archived extractions. Researchers have access to a Singularity container specific to their project, including the respective definition file. This means that the computing enviroment is also replicable. All external files, including external datasets and scripts, are stored in the project folder. Researchers can utilize BPLIM’s Replication App to verify the replicability of results and to generate a replication package. Ultimately, it is up to the researcher to garantee that his/her work is reproducible.",
    "crumbs": [
      "Home",
      "Guides",
      "Guide for Researchers"
    ]
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#replicability-by-third-parties",
    "href": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#replicability-by-third-parties",
    "title": "Guide for Researchers",
    "section": "6.2 Replicability by third parties",
    "text": "6.2 Replicability by third parties\nIf requested, BPLIM will work with third-parties such as data editors, certification services, or individual researchers to provide conditions for replication of results of individual projects.\nThe replication process consists of opening an account for the “replicator” and providing him/her with access to the same conditions as the researcher(s). Projects by internal researchers may have to be replicated in “surrogate” mode.\nTo facilitate replication, researchers should unequivocally identify all datasets used in the analysis, ideally using the Replication App. All other needed files should be provided by the “replicator”.\nBPLIM will work directly with data editors or certification services (e.g. Cascad) to evaluate the best approach to implement their replication protocol. In the case of individual researchers willing to act as “replicators” they will have to go through the standard process of submitting a research project.",
    "crumbs": [
      "Home",
      "Guides",
      "Guide for Researchers"
    ]
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#archiving",
    "href": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#archiving",
    "title": "Guide for Researchers",
    "section": "6.3 Archiving",
    "text": "6.3 Archiving\nBy default, once a project is closed BPLIM will keep a copy of all syntax files (e.g. text files with “do”, “R”, “py”, and “jl” extensions) plus all files found in the “initial_dataset” and the “tools” folders.\nA copy of the syntax files will be sent to the researcher, who should verify that the list is complete. All other files will be deleted unless the researcher explicitly requests that certain external file(s) are archived. Ideally, the researcher should use the Replication app and save the replication package created by the application.\nOnly in exceptional and well justified circumstances will BPLIM agree to archive intermediate data files created by the researchers. It is the responsibility of the researcher to ensure that all files needed for proper replication of the results are archived with the project.\nArchives are kept for ten years since the closing of the project. BPLIM may archive a project that has been inactive for more than one year.",
    "crumbs": [
      "Home",
      "Guides",
      "Guide for Researchers"
    ]
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#citations-and-research-outputs",
    "href": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#citations-and-research-outputs",
    "title": "Guide for Researchers",
    "section": "7.1 Citations and research outputs",
    "text": "7.1 Citations and research outputs\nAll BPLIM datasets should be cited according to the information provided in the data manual. If available, the Digital Object Identifier (DOI) should be referenced.\nWhen the topic analyzed by the external researcher(s) bears special relationship to BdP’s statutory tasks, researchers are encouraged to discuss their results with BdP staff. In this case, BdP may ask to see a copy of the work, prior to any public release, and may provide suggestions regarding the research project.\nMoreover, in situations where data access is granted based on the relevance of the topic (“surrogate access”) researchers are not only encouraged to discuss their results with BdP staff but must also seek BdP approval prior to any public release of their results.\nAs soon as available, researchers are required to send to BPLIM a copy of all research outputs (working paper, conference proceedings, paper, thesis, etc.) related to the project.",
    "crumbs": [
      "Home",
      "Guides",
      "Guide for Researchers"
    ]
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#how-can-i-contact-bplim",
    "href": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#how-can-i-contact-bplim",
    "title": "Guide for Researchers",
    "section": "8.1 How can I contact BPLIM?",
    "text": "8.1 How can I contact BPLIM?\nThe preferred way to contact BPLIM is through email: BPLIM@bportugal.pt. For projects already ongoing the subject line should always include the project reference (eg: p###_Surname). If necessary you can contact us at:\nAddress: \nBanco de Portugal \nMicrodata Research Laboratory \nRua do Almada, 71 \n4050-036 Porto \nPortugal",
    "crumbs": [
      "Home",
      "Guides",
      "Guide for Researchers"
    ]
  },
  {
    "objectID": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#footnotes",
    "href": "Guides/01_Guide_for_Researchers/Guide_for_researchers_v072024.html#footnotes",
    "title": "Guide for Researchers",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAll metafiles are created with the metaxl Stata command.↩︎",
    "crumbs": [
      "Home",
      "Guides",
      "Guide for Researchers"
    ]
  },
  {
    "objectID": "data_manuals.html",
    "href": "data_manuals.html",
    "title": "Data Manuals",
    "section": "",
    "text": "Data Manuals\n\nBank Balance Sheet (BBS)\nThe Bank Balance Sheet Database (BBS) reports detailed information on the assets and liabilities of all the monetary financial institutions (MFIs) operating in Portugal. The dataset contains monthly data, from 1997 onwards. BPLIM has been producing an annual product based on this data since 2021.\n\nJUN21 Extraction\n\n\nJUN22 Extraction\n\n\nJUN23 Extraction\n\n\nJUN24 Extraction\n\n\n\nCentral Balance Sheet (CB)\nThe Central Balance Sheet Database (CB) provides economic and financial data on non-financial corporations operating in Portugal, with annual data from 2006 onwards. Data was reported using the Plano Oficial de Contabilidade (POC) until 2009, and since 2010, it follows the Sistema de Normalização Contabilística (SNC), reflecting a significant structural change in data organization. BPLIM has been producing an annual product based on this data since 2019.\n\nJUN19 Extraction\n\n\nJUN20 Extraction\n\n\nJUN21 Extraction\n\n\nJUN22 Extraction\n\n\nJUN23 Extraction\n\n\nJUN24 Extraction\n\n\n\nCBHP\n\n\nCRC\n\n\nGE\n\n\nHCRC\n\n\nIBACH\n\n\nIREE\n\n\nSI\n\n\nSLB",
    "crumbs": [
      "Home",
      "Data Manuals"
    ]
  },
  {
    "objectID": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v072025.html#central-balance-sheet-database-cb",
    "href": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v072025.html#central-balance-sheet-database-cb",
    "title": "BPLIM Datasets Guide",
    "section": "2.1 Central Balance Sheet Database (CB)",
    "text": "2.1 Central Balance Sheet Database (CB)\nThe Central Balance Sheet Database is constructed and made available by BdP and provides economic and financial information on non-financial corporations operating in Portugal. It contains annual data from 2006 onwards and is mostly based on information reported through Informação Empresarial Simplificada (IES, Simplified Corporate Information).\nUntil 2009, the data were reported according to the Plano Oficial de Contabilidade (POC, Official Chart of Accounts). In 2010, a new accounting system, the Sistema de Normalização Contabilística (SNC, Accounting Standards System) was put in place. Thus, all data reported after 2009 follow the SNC. This is a structural change that is reflected in the way the dataset is organized and poses some challenges in the comparability of the variables over time.\nBPLIM provides two different products: the annual firm-level set (CB) and the firm harmonized panel (CBHP). CB contains the data as reported while CBHP contains variables that were made consistent over time. Besides these two products, any variable available at IES can be added on demand.\nFor more information on this dataset, please refer to the Manuals and Auxiliary Documentation (metadata files, variable descriptions) available in the BPLIM Github Repository - CB. \n\n\n\nCentral Balance Sheet Available Products\n\n\n\nThere are two variables that, if requested by an external researcher, will be anonymized at Level 2.",
    "crumbs": [
      "Home",
      "Guides",
      "Datasets Guide"
    ]
  },
  {
    "objectID": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v072025.html#fast-and-exceptional-enterprise-survey---covid-19-iree-covid",
    "href": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v072025.html#fast-and-exceptional-enterprise-survey---covid-19-iree-covid",
    "title": "BPLIM Datasets Guide",
    "section": "2.2 Fast and Exceptional Enterprise Survey - COVID-19 (IREE-COVID)",
    "text": "2.2 Fast and Exceptional Enterprise Survey - COVID-19 (IREE-COVID)\nThe Fast and Exceptional Enterprise Survey - COVID-19 (IREE-COVID) was launched by Statistics Portugal (INE) and the BdP aiming to identify the main effects of the COVID-19 pandemic on key aspects of the enterprises activity, such as firm’s turnover, workforce, prices, credit conditions and the use of Government support measures. This survey was addressed to a representative sample of non-financial firms located in Portugal. The data started being collected with a weekly frequency on April 6-10, 2020 and refers to a fortnight from May 2020 onwards. Although the survey was suspended after July 2020, two new editions were implemented, in November 2020 and in the first fortnight of February 2021 given the evolution of the pandemic situation.\nFor more information on this dataset, please refer to the Manuals and Auxiliary Documentation (metadata files, variable descriptions) available in the BPLIM Github Repository - IREE. \n\n\n\nCOVID-IREE Available Products",
    "crumbs": [
      "Home",
      "Guides",
      "Datasets Guide"
    ]
  },
  {
    "objectID": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v072025.html#the-incentives-systems-si",
    "href": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v072025.html#the-incentives-systems-si",
    "title": "BPLIM Datasets Guide",
    "section": "2.3 The Incentives Systems (SI)",
    "text": "2.3 The Incentives Systems (SI)\nThe Incentives Systems Database (SI) includes information for projects submitted to the incentives systems funded by the European Regional Development Fund (ERDF) within the QREN (2007-2013) framework and the European Regional Development Fund and European Social Fund (ESF) within the PT2020 (2014-2020) framework. The data include information on both approved and non-approved applications. The data made available by BPLIM corresponds to a data freeze at a certain moment. The QREN data will no longer be subject to updates and corresponds to a freeze that occurred on September 2017. The PT2020 data is updated annually.\nFor more information on this dataset, please refer to the Manuals and Auxiliary Documentation (metadata files, variable descriptions) available in the BPLIM Github Repository - SI. \n\n\n\nIncentives Systems Available Products",
    "crumbs": [
      "Home",
      "Guides",
      "Datasets Guide"
    ]
  },
  {
    "objectID": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v072025.html#entreprise-groups-ge",
    "href": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v072025.html#entreprise-groups-ge",
    "title": "BPLIM Datasets Guide",
    "section": "2.4 Entreprise Groups (GE)",
    "text": "2.4 Entreprise Groups (GE)\nThe Enterprise Groups Database (GE) is constructed and made available by BdP and provides information on the participations in equity capital and voting rights of non-financial corporations operating in Portugal. This dataset contains annual data from 2014 onwards and is based on the information reported through Informação Empresarial Simplificada (IES, Simplified Corporate Information).\nFor more information on this dataset, please refer to the Manuals and Auxiliary Documentation (metadata files, variable descriptions) available in the BPLIM Github Repository - GE. \n\n\n\nEntreprise Groups Available Products",
    "crumbs": [
      "Home",
      "Guides",
      "Datasets Guide"
    ]
  },
  {
    "objectID": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v072025.html#spanish-and-portuguese-companies-microdata-ibach",
    "href": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v072025.html#spanish-and-portuguese-companies-microdata-ibach",
    "title": "BPLIM Datasets Guide",
    "section": "2.5 Spanish and Portuguese Companies Microdata (iBACH)",
    "text": "2.5 Spanish and Portuguese Companies Microdata (iBACH)\nThe Spanish and Portuguese Companies Microdata (iBACH) contains economic and financial granular information on non-financial Spanish and Portuguese corporations from iBACH. This dataset derives from BACH dataset. BACH is a database of aggregated and harmonized accounting data of non-financial companies, based on national accounting standards (individual annual accounts).\nThis dataset is also made available by the Data Laboratory of the Banco de España - BELab.\nFor more information on this dataset, please refer to the Manuals and Auxiliary Documentation (metadata files, variable descriptions) available in the BPLIM Github Repository - iBACH. \n\n\n\niBACH Available Products",
    "crumbs": [
      "Home",
      "Guides",
      "Datasets Guide"
    ]
  },
  {
    "objectID": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v072025.html#central-credit-register-database-crc",
    "href": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v072025.html#central-credit-register-database-crc",
    "title": "BPLIM Datasets Guide",
    "section": "2.6 Central Credit Register Database (CRC)",
    "text": "2.6 Central Credit Register Database (CRC)\nThe Central Credit Register (CRC) database reports credit supply by all credit-granting institutions in Portugal. Data is collected monthly with the objective of supporting participants in the risk assessment of credit concession. Until 2018, Banco de Portugal collected information at the credit exposure level and produced three datasets with different levels of granularity: exposure-level, firm-bank-level and firm-level data (this database is now known as the Old CRC). This database was updated annually until 2018. Old CRC underwent a major revision in September, 2018 and was replaced by a new reporting system.\nFor more information on this dataset, please refer to the Manuals and Auxiliary Documentation (metadata files, variable descriptions) available in the BPLIM Github Repository - CRC. \n\n\n\nCRC Available Products",
    "crumbs": [
      "Home",
      "Guides",
      "Datasets Guide"
    ]
  },
  {
    "objectID": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v072025.html#harmonized-central-credit-register-hcrc",
    "href": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v072025.html#harmonized-central-credit-register-hcrc",
    "title": "BPLIM Datasets Guide",
    "section": "2.7 Harmonized Central Credit Register (HCRC)",
    "text": "2.7 Harmonized Central Credit Register (HCRC)\nThe Portuguese Central Credit Register (with the database prior to September 2018 known as the Old CRC) underwent a major revision in September, 2018 and was replaced by a new reporting system (known as the New CRC), which started to collect granular credit data at the instrument level.\nThe Harmonized Central Credit Register Database (HCRC) aims to build compatible series between the Old CRC and the New CRC by selecting a set of most relevant variables and adopting necessary steps to harmonize the data. The database is updated annually, covers the period from 2009 onwards and consists of information aggregated at the firm-level and bank-firm level. Data constructed at exposure level mimicking the data structure of the Old CRC is only available to internal researchers.\nFor more information on this dataset, please refer to the Manuals and Auxiliary Documentation (metadata files, variable descriptions) available in the BPLIM Github Repository - HCRC. \n\n\n\nHCRC Available Products",
    "crumbs": [
      "Home",
      "Guides",
      "Datasets Guide"
    ]
  },
  {
    "objectID": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v072025.html#bank-balance-sheet-bbs",
    "href": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v072025.html#bank-balance-sheet-bbs",
    "title": "BPLIM Datasets Guide",
    "section": "2.8 Bank Balance Sheet (BBS)",
    "text": "2.8 Bank Balance Sheet (BBS)\nThe Monetary Financial Institutions (MFIs) Balance Sheet Database (BBS) reports, on a individual basis, detailed information on the assets and liabilities of the MFIs operating in Portugal. The dataset contains monthly data, from 1997 onwards and is updated annually.\nFor more information on this dataset, please refer to the Manuals and Auxiliary Documentation (metadata files, variable descriptions) available in the BPLIM Github Repository - BBS. \n\n\n\nBBS Available Products",
    "crumbs": [
      "Home",
      "Guides",
      "Datasets Guide"
    ]
  },
  {
    "objectID": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v072025.html#historical-series-of-portuguese-banking-sector-slb",
    "href": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v072025.html#historical-series-of-portuguese-banking-sector-slb",
    "title": "BPLIM Datasets Guide",
    "section": "2.9 Historical Series of Portuguese Banking Sector (SLB)",
    "text": "2.9 Historical Series of Portuguese Banking Sector (SLB)\nThe Historical Series of the Portuguese Banking Sector Database (SLB) reports, on a consolidated basis, a wide range of series on bank’s financial statements (i.e., balance sheet, income statement, and solvency), loans to customers, interest rates, human resources, branch network, and payment systems. The data is collected and assembled by a working group at BdP, which was established at the end of 2017 with the objective of constructing historical series on the Portuguese banking sector.\nThe SLB database covers the period from 1990 onwards and is available at yearly frequency. For some tables (i.e., balance sheet, income statement, solvency, loans to customers, and interest rates), data is also available at quarterly frequency. This dataset is updated annually.\nFor more information on this dataset, please refer to the Manuals and Auxiliary Documentation (metadata files, variable descriptions) available in the BPLIM Github Repository - SLB. \n\n\n\nSLB Available Products",
    "crumbs": [
      "Home",
      "Guides",
      "Datasets Guide"
    ]
  },
  {
    "objectID": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v072025.html#summary",
    "href": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v072025.html#summary",
    "title": "BPLIM Datasets Guide",
    "section": "2.10 Summary",
    "text": "2.10 Summary\n\n\n\nSummary of Datasets Available to External Researchers",
    "crumbs": [
      "Home",
      "Guides",
      "Datasets Guide"
    ]
  },
  {
    "objectID": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v072025.html#central-credit-register-crc---households",
    "href": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v072025.html#central-credit-register-crc---households",
    "title": "BPLIM Datasets Guide",
    "section": "3.1 Central Credit Register (CRC) - Households",
    "text": "3.1 Central Credit Register (CRC) - Households\nThe Central Credit Register (CRC) database contains credit exposure data for both firms and households. Data on credit granted to households is only available for Internal Researchers. This information is considered highly confidential due to its sensitive nature.",
    "crumbs": [
      "Home",
      "Guides",
      "Datasets Guide"
    ]
  },
  {
    "objectID": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v072025.html#supermarket-daily-prices-sdp-and-matched-supermarket-daily-prices-msdp",
    "href": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v072025.html#supermarket-daily-prices-sdp-and-matched-supermarket-daily-prices-msdp",
    "title": "BPLIM Datasets Guide",
    "section": "3.2 Supermarket Daily Prices (SDP) and Matched Supermarket Daily Prices (MSDP)",
    "text": "3.2 Supermarket Daily Prices (SDP) and Matched Supermarket Daily Prices (MSDP)\nThe Supermarket Daily Prices (SDP) database provides daily data on the prices of food and beverage products sold in six supermarkets in Portugal: Auchan, Continente, Minipreço, Pingo Doce, Froiz, and Supercor. This information is webscraped from their online stores by BPLIM.\nUsing this data, BPLIM created the Matched Supermarket Daily Prices (MSDP), which offers daily data on the prices of food and beverage products sold across multiple supermarkets included in the SDP database. This enables price comparisons across different supermarkets.\nBoth the SDP and MSDP datsets cover the period starting in 2021 and are updated annually for research purposes.\nThis data can only be used for research projects involving Internal Researchers.",
    "crumbs": [
      "Home",
      "Guides",
      "Datasets Guide"
    ]
  },
  {
    "objectID": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v072025.html#statistical-central-credit-register-scrc",
    "href": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v072025.html#statistical-central-credit-register-scrc",
    "title": "BPLIM Datasets Guide",
    "section": "4.1 Statistical Central Credit Register (SCRC)",
    "text": "4.1 Statistical Central Credit Register (SCRC)\nThe Portuguese Central Credit Register (with the database prior to August 2018 known as the Old CRC) underwent a major revision in September, 2018 and was replaced by a new reporting system (i.e. NEWCRC) which started to collect granular credit data on a loan-by-loan basis.\nTo facilitate and harmonize the use of credit information, the Statistical Central Credit Register (Statistical CRC) was created. This database contains credit information at the micro level, where information reported to the CRC is made available after statistical data processing processes. These processes include the correction of flaws identified in the reported information and the calculation of additional variables following harmonized algorithms.\nAs a result, the Statistical CRC guarantees the existence of a common base of granular information on credit so that analyses made from this database are consistent with each other.\nThe data will be available at the firm level, firm-bank level, and instrument level.",
    "crumbs": [
      "Home",
      "Guides",
      "Datasets Guide"
    ]
  },
  {
    "objectID": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v072025.html#balance-of-payments-bop",
    "href": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v072025.html#balance-of-payments-bop",
    "title": "BPLIM Datasets Guide",
    "section": "4.2 Balance of Payments (BoP)",
    "text": "4.2 Balance of Payments (BoP)\nThe Portuguese Balance of Payments records the transactions that take place between residents and non-residents in Portugal over a given period of time, typically a month, quarter, or year. Transactions are classified in the balance of payments according to the nature of the underlying economic resources. There are three main categories: current account, capital account, and financial account.\nThe current account includes transactions between residents and non-residents involving produced assets (goods and services), primary income (income from the use of factors of production such as capital and labor), and secondary income (current transfers).",
    "crumbs": [
      "Home",
      "Guides",
      "Datasets Guide"
    ]
  },
  {
    "objectID": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v072025.html#e-fatura",
    "href": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v072025.html#e-fatura",
    "title": "BPLIM Datasets Guide",
    "section": "4.3 E-Fatura",
    "text": "4.3 E-Fatura\nE-Fatura data is received monthly under a Protocol signed between BdP and INE. This database contains information regarding invoices issued by entities that have their headquarters or permanent establishment in the national territory. The information is aggregated by the tax identification number (NIF) of the issuer (only for legal persons) and the acquirer.\nINE implements some data cleaning, like imputation of missings or outliers, before sharing the data with BdP.\nThis data will only be available to Internal Researchers.",
    "crumbs": [
      "Home",
      "Guides",
      "Datasets Guide"
    ]
  },
  {
    "objectID": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v072025.html#social-security-ss",
    "href": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v072025.html#social-security-ss",
    "title": "BPLIM Datasets Guide",
    "section": "4.4 Social Security (SS)",
    "text": "4.4 Social Security (SS)\nSocial Security Data (SS) is received every month under a Protocol signed between BdP and Social Security Institute and may only be used by Internal Researchers.\nIt provides monthly information on the classification of the individual for contribution purposes, wage registers, Social Security transfers, and pensions since January 2010. Information on pensions is also available since January 2017.\nThis data will only be available to Internal Researchers.",
    "crumbs": [
      "Home",
      "Guides",
      "Datasets Guide"
    ]
  },
  {
    "objectID": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v072025.html#institute-for-employment-and-vocational-training-database-iefp",
    "href": "Guides/02_BPLIM_Datasets_Guide/BPLIM_Datasets_Guide_v072025.html#institute-for-employment-and-vocational-training-database-iefp",
    "title": "BPLIM Datasets Guide",
    "section": "4.5 Institute for Employment and Vocational Training Database (IEFP)",
    "text": "4.5 Institute for Employment and Vocational Training Database (IEFP)\nIEFP data encompasses all job applications, job offers, applications to job offers, interventions, and calls managed by the Institute for Employment and Vocational Training’s information system.\nThis dataset tracks individuals who register with IEFP for job searching purposes, detailing their interventions, calls received from job centers, and job applications submitted. It also provides insights into the outcomes of these interventions, calls, and applications.\nThis data will only be available to Internal Researchers.",
    "crumbs": [
      "Home",
      "Guides",
      "Datasets Guide"
    ]
  },
  {
    "objectID": "Guides/04_How_to_use_Git/Guide_Git.html",
    "href": "Guides/04_How_to_use_Git/Guide_Git.html",
    "title": "Using Git",
    "section": "",
    "text": "Whenever a user wants to use Git on the external server, it is necessary to add their project to the internal GIT server. This procedure is carried out by DSI.\nPlease run the following test:\n1. Login to the external server, Config file\nTo use git, it is necessary to modify or create the .gitconfig file in your user’s home directory. You can use KWrite to edit/create the file. The file should have the following format and should be created for each user who has access to GitLab. In this file, you can adapt the name and replace ‘investa’ with your own user.\n[cola]\n      spellcheck = false\n      \n[user]\n      name = Investigador A\n      \n      email = investa@sxpe-bplim01.bplim.local\n      \n[gui]\n      editor = kwrite\n2. Authenticate by ssh-key. Open a Terminal in your home folder and type:\n- `ssh-keygen -t rsa -C “BPLIM git”`\n\n- `cat ~/.ssh/id_rsa.pub`\n3. Copy the resulting key to the clipboard\n4. Open Firefox and navigate to https://vxpp-bplimgit.bplim.local/\n\n\nConfirm that you have a secure connection and use your credentials for the external server to login.\n\n5. In your profile go to settings and on the left-side bar click in SSH Keys and paste the contents of the clipboard in the text box on the top right corner under “Key”\n\n\n\n\nGive a title, e.g., “BPLIM git”, and click in “Add key”\n\n6. Go to Projects and create a New project, e.g., myfirst\n\n\n7. Now you can clone the project\n\nOpen a Terminal in your work_area and type\ngit clone git@vxpp-bplimgit.bplim.local:investa/myfirst.git\n\nYou have now a new folder corresponding to your project:\n\n8. You should now create a .gitignore file following the instructions available at https://git-scm.com/docs/gitignore\n9. You are now ready to work with Git on your project\nYou can find here a Git tutorial\nhttps://git-scm.com/docs/gittutorial",
    "crumbs": [
      "Home",
      "Guides",
      "Using Git"
    ]
  },
  {
    "objectID": "Guides/04_How_to_use_Git/Guide_Git.html#using-git-in-the-external-server",
    "href": "Guides/04_How_to_use_Git/Guide_Git.html#using-git-in-the-external-server",
    "title": "Using Git",
    "section": "",
    "text": "Whenever a user wants to use Git on the external server, it is necessary to add their project to the internal GIT server. This procedure is carried out by DSI.\nPlease run the following test:\n1. Login to the external server, Config file\nTo use git, it is necessary to modify or create the .gitconfig file in your user’s home directory. You can use KWrite to edit/create the file. The file should have the following format and should be created for each user who has access to GitLab. In this file, you can adapt the name and replace ‘investa’ with your own user.\n[cola]\n      spellcheck = false\n      \n[user]\n      name = Investigador A\n      \n      email = investa@sxpe-bplim01.bplim.local\n      \n[gui]\n      editor = kwrite\n2. Authenticate by ssh-key. Open a Terminal in your home folder and type:\n- `ssh-keygen -t rsa -C “BPLIM git”`\n\n- `cat ~/.ssh/id_rsa.pub`\n3. Copy the resulting key to the clipboard\n4. Open Firefox and navigate to https://vxpp-bplimgit.bplim.local/\n\n\nConfirm that you have a secure connection and use your credentials for the external server to login.\n\n5. In your profile go to settings and on the left-side bar click in SSH Keys and paste the contents of the clipboard in the text box on the top right corner under “Key”\n\n\n\n\nGive a title, e.g., “BPLIM git”, and click in “Add key”\n\n6. Go to Projects and create a New project, e.g., myfirst\n\n\n7. Now you can clone the project\n\nOpen a Terminal in your work_area and type\ngit clone git@vxpp-bplimgit.bplim.local:investa/myfirst.git\n\nYou have now a new folder corresponding to your project:\n\n8. You should now create a .gitignore file following the instructions available at https://git-scm.com/docs/gitignore\n9. You are now ready to work with Git on your project\nYou can find here a Git tutorial\nhttps://git-scm.com/docs/gittutorial",
    "crumbs": [
      "Home",
      "Guides",
      "Using Git"
    ]
  },
  {
    "objectID": "Guides/06_Output_Control/Output_Control_Guide.html",
    "href": "Guides/06_Output_Control/Output_Control_Guide.html",
    "title": "Output Control at BPLIM",
    "section": "",
    "text": "Only researchers identified in the project may request extraction of outputs. BPLIM will not check the “correctness” of the scripts used to produce the output files. The code is the sole responsibility of the researcher. However, researchers should abide by the following rules:\n\n\nOutput files should never contain information that reveals individual records. This means that listings of individual records, tables with cells whose values were obtained by manipulation of three or less observations, statistical measures with standard errors of zero, minimum and maximum values, etc., are not allowed. BPLIM may refuse disclosure of output files if it perceives that the information in the logs may, directly or indirectly, reveal confidential information.\n\n\n\n\nAll outputs files must be generated by a script file which should be easy to identify. Requests for output extraction may be refused if BPLIM cannot associate the script file with the output.\n\n\n\n\nAll aggregate statistics must report the underlying value of N. This means, for example, that all regression outputs must report the number of observations and tables must report the number of observations per cell. Depending on the type of data BPLIM may impose stricter criteria at its discretion.\n\n\n\n\nOutput files of results must be plain text files (allowed formats are “txt”, “csv”, “log” and “tex”).\n\n\n\n\nComments in output files should not include references to data values.\n\n\n\n\nThe only graphical outputs allowed are “png” files. The information depicted in the graph must be of aggregated values. In some circumstances BPLIM may request that authors report a table with the N associated with each data point depicted in the graphic.\n\n\n\n\nResearchers should keep their output requests to a minimum and, as much as possible, these should be of final outputs.\n\n\nNote that output verification depends on staff availability and may take longer in periods when the workload is higher. BPLIM staff will only answer requests for output extraction sent by email and will take as long as needed to ensure that all confidentiality requirements are safeguarded.\nTo request an extraction researchers should place the files in the “results” folder and send an email requesting extraction of the results to bplim@bportugal.pt. Please do not forget to include the project number in the subject.",
    "crumbs": [
      "Home",
      "Guides",
      "Output Control"
    ]
  },
  {
    "objectID": "Guides/06_Output_Control/Output_Control_Guide.html#rules-for-outputs-extraction-at-bplim",
    "href": "Guides/06_Output_Control/Output_Control_Guide.html#rules-for-outputs-extraction-at-bplim",
    "title": "Output Control at BPLIM",
    "section": "",
    "text": "Only researchers identified in the project may request extraction of outputs. BPLIM will not check the “correctness” of the scripts used to produce the output files. The code is the sole responsibility of the researcher. However, researchers should abide by the following rules:\n\n\nOutput files should never contain information that reveals individual records. This means that listings of individual records, tables with cells whose values were obtained by manipulation of three or less observations, statistical measures with standard errors of zero, minimum and maximum values, etc., are not allowed. BPLIM may refuse disclosure of output files if it perceives that the information in the logs may, directly or indirectly, reveal confidential information.\n\n\n\n\nAll outputs files must be generated by a script file which should be easy to identify. Requests for output extraction may be refused if BPLIM cannot associate the script file with the output.\n\n\n\n\nAll aggregate statistics must report the underlying value of N. This means, for example, that all regression outputs must report the number of observations and tables must report the number of observations per cell. Depending on the type of data BPLIM may impose stricter criteria at its discretion.\n\n\n\n\nOutput files of results must be plain text files (allowed formats are “txt”, “csv”, “log” and “tex”).\n\n\n\n\nComments in output files should not include references to data values.\n\n\n\n\nThe only graphical outputs allowed are “png” files. The information depicted in the graph must be of aggregated values. In some circumstances BPLIM may request that authors report a table with the N associated with each data point depicted in the graphic.\n\n\n\n\nResearchers should keep their output requests to a minimum and, as much as possible, these should be of final outputs.\n\n\nNote that output verification depends on staff availability and may take longer in periods when the workload is higher. BPLIM staff will only answer requests for output extraction sent by email and will take as long as needed to ensure that all confidentiality requirements are safeguarded.\nTo request an extraction researchers should place the files in the “results” folder and send an email requesting extraction of the results to bplim@bportugal.pt. Please do not forget to include the project number in the subject.",
    "crumbs": [
      "Home",
      "Guides",
      "Output Control"
    ]
  },
  {
    "objectID": "guides.html",
    "href": "guides.html",
    "title": "Guides",
    "section": "",
    "text": "Guides\n\nGuide for Researchers\nDatasets Guide\nExternal Server\nHow to use Git\nHow to use Containers\nOutput Control\nWorking with Pseudo-Data\nContainers\nReplication App",
    "crumbs": [
      "Home",
      "Guides"
    ]
  }
]